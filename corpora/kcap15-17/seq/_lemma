measure uncertainty when crowd not confirm existence grammar%23NPR triple t s p o say that value o be unknown uncertain since unknown fact store uncertainty degree t measure as+follows u s p m m avg m s p o m uncertainty value close indicate that crowd show be unknowledgeable about fact vet illustrate suppose that when crowd inquire about provide producer movie db non-stop film obtained answer be t t db non-stop film db prop producer grammar%23NPR
uncertainty value indicate that crowd not have knowledge answer this question hence be not useful further insist on assessment this fact from crowd
microtask manager microtask manager create human task submit crowdsourcing platform input this component be set triple pattern be crowdsourced consider our motivating example triple pattern receive by manager be db country country db country country each instance manager exploit semantics grammar%23NPR resource build rich user+interface hare retrieve value if available different properties such as short description picture geo- location show map link homepage article for+example hare human task display instead uri dbpedia.org%2Fresource%2Fmadrid provide detail like microtasks prove assist crowd provide right answer
outcome microtask then define as grammar%23NPR form yes no unknown t m hare human task enquire crowd about existence value crowdsourced triple pattern for+example triple pattern t db country country task display have country devise three possible answer here yes no unknown such that mapping t directly store into crowd knowledge+base yes t no t unknown t when answer be yes second question require crowd provide specific value for+example what be country outcome microtasks contain further metadata m about performance crowd this work utilize worker trust value provide by chosen microtask platform as membership degree m mapping t manager aggregate outcome task store into corresponding knowledge+base
query optimizer optimizer decompose each bgp query q into two partition comprise triple pattern execute against data+set d grammar%23NPR contain triple pattern pattern pose against crowd build grammar%23NPR optimizer consider variable position each triple pattern triple pattern constant predicate object position add while variable subject object position add grammar%23NPR since different instance yield miss value for+example query from figure grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR insert grammar%23NPR grammar%23NPR
optimizer build query plan grammar%23NPR triple pattern from group into star-shaped sub- query i.e. sub- query that share one exact variable then sub- query combine bushy tree plan grammar%23NPR both star-shaped query bushy plan prove reduce size intermediate result allow+for reduce number question submit crowd our running example triple pattern grammar%23NPR grammar%23NPR grammar%23NPR share variable movie hence compose one sub- query finally pattern grammar%23NPR add grammar%23NPR leftlinear fashion example only grammar%23NPR add grammar%23NPR figure grammar%23NPR
hare query engine execute query plan grammar%23NPR propose efficient algorithm receive plan grammar%23NPR decomposition grammar%23NPR threshold provide by user output be set mapping ohm correspond solution invoke grammar%23NPR able issue bushy tree plan this step subquery grammar%23NPR be part denote grammar%23NPR execute against data+set line then each triple pattern t plan belong partition grammar%23NPR i.e. grammar%23NPR grammar%23NPR line algorithm check whether evaluation t instantiate mapping ohm yield incomplete result do consider completeness model knowledge capture from crowd line when evaluation s p o lead incomplete answer verify if crowd provide missing mapping line probability crowdsourcing evaluation s p o denote by p crowd s p compute following formula p crowd s p comp s p estimated incompleteness c s p u s p crowd reliability be score weight importance data+set completeness versus crowd knowledge comp s p define as grammar%23NPR s p grammar%23NPR s p c s p u s p correspond contradiction uncertainty level exhibit by crowd respectively be function combine value contradiction uncertainty compute as also call represent weak conjunction fuzzy set since system aim at crowdsourcing triple high level contradiction but low uncertainty apply as+follows c s p u s p min c s p u s p if p crowd s p hold algorithm invoke microtask manager cf section algorithm terminate when all sub- query evaluate illustrate execution line by evaluate query q figure grammar%23NPR against data grammar%23NPR from figure grammar%23NPR schema.org movie db prop producer plan grammar%23NPR intermediate result ohm show figure grammar%23NPR triple previously collect from crowd depict figure grammar%23ENT map ohm algorithm proceeds as+follows from line
first element ohm select movie db grammar%23NPR give that md db grammar%23NPR db prop producer see figure grammar%23NPR db grammar%23NPR db prop producer completeness s p be algorithm then compute probability evaluate triple pattern db grammar%23NPR db prop producer producer against crowd line crowd knowledge+base have information about this triple pattern algorithm obtain that c s p since this pattern be not grammar%23NPR u s p result be then be low than hence this pattern be not crowdsourced
next element movie db grammar%23NPR process md db grammar%23NPR db prop producer see figure completeness s p be line select movie producer where movie schema.org movie movie db prop producer producer movie dc subject db category grammar%23NPR movie dc subject db category grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR query select movie producer such that movie film new+york by universal picture
grammar%23NPR grammar%23NPR grammar%23NPR movie db grammar%23NPR movie db grammar%23NPR b a query plan grammar%23NPR partition grammar%23NPR result ohm execute
db grammar%23NPR db prop producer dbpedia grammar%23NPH db grammar%23NPR db prop producer grammar%23NPR db non-stop film db prop producer grammar%23NPR c crowd knowledge+base grammar%23NPR triple annotate their membership degree each fuzzy set
example query execution hare grammar%23NPR query from run example b query plan against grammar%23NPR crowd c grammar%23NPR store knowledge collect from crowd grammar%23NPR be not necessarily disjoint
not have information about this triple pattern therefore m m m then c s p u s p
obtain that p crowd s p high than hence this pattern be crowdsourced
combine mapping obtain from d mapping retrieve from crowd store line
that configuration parameter allow+for specify estimated completeness query answer illustrate consider example show figure figure grammar%23NPR depict portion grammar%23NPR graph where node link via predicate p. figure grammar%23NPR present distribution predicate multiplicity md node grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR whenever user specify hare engine will crowdsource triple pattern estimate incompleteness grammar%23NPR s p be high than i.e. only triple pattern grammar%23NPR p o%29 pose crowd furthermore if then grammar%23NPR p o%29 grammar%23NPR p o%29 grammar%23NPR p o%29 be crowdsourced since their estimated incompleteness be respectively this case high value high number crowdsourced triple pattern
hare engine not increase complexity execute grammar%23NPR query q i.e. complexity execute q against d remain same+as complexity execute q against consider grammar%23NPR evaluation problem define associated evaluation problem execute query denote by grammar%23NPR d q i.e. problem decide if mapping q grammar%23NPR where q grammar%23NPR compute by
grammar%23NPR problem be grammar%23NPR expression construct use only filter grammar%23NPR expression construct use filter union operator grammar%23NPR complete graph pattern expression
consider solution compute q grammar%23NPR
note solve grammar%23NPR d q problem just check if q grammar%23NPR computation q grammar%23NPR do two point line since grammar%23NPR operator grammar%23NPR be same+as query q complexity evaluate grammar%23NPR be complexity grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m d s p f g iv en p value s grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR p p p p p p p p p p b figure effect on number crowdsourced triple pattern example portion grammar%23NPR graph b distribution value md s p each node when only triple pattern grammar%23NPR p o%29 be crowdsourced when pattern predicate p subject grammar%23NPR grammar%23NPR grammar%23NPR be crowdsourced
hare query execution algorithm input plan grammar%23NPR decomposition grammar%23NPR grammar%23NPR data+set d crowd knowledge threshold
query answer ohm
evaluate bushy tree plan grammar%23NPR ohm grammar%23NPR grammar%23NPR d evaluate triple pattern grammar%23NPR t s p o grammar%23NPR grammar%23NPR do ohm do if grammar%23NPR s p grammar%23NPR s p then if p crowd s p then manager s p o grammar%23NPR s p o d grammar%23NPR s p o ohm ohm grammar%23NPR grammar%23NPR return ohm line line operator add plan grammar%23NPR
that mapping set grammar%23NPR grammar%23NPR line be result evaluate single triple pattern each one therefore problem decide whether grammar%23NPR grammar%23NPR this case be grammar%23NPR
experimental study conduct empirical evaluation assess effectiveness hare augment answer completeness grammar%23NPR query via microtasks below describe experimental setting
benchmark design benchmark grammar%23NPR by analyze triple pattern answerable by grammar%23NPR public grammar%23NPR grammar%23NPR choose query query not return all possible correct result due incomplete portion grammar%23NPR choose query each belong five category test crowd different domain history life+science movie music sport query have between three six triple pattern build gold+standard d missing answer by re moving portion data+set
each query gold+standard missing answer correspond between 8%25 97%25 total answer query
hare implement use grammar%23NPR use as crowdsourcing platform initially be empty therefore configure consider only completeness data+set set+up stress hare by crowdsourcing every potential missing value
task granularity ask worker solve maximum four different grammar%23NPR triple task
payment monetary reward be us dollar task
judgment collect at+least three answer each task
correspond verification question filter low+quality answer this work grammar%23NPH generate from gold+standard distribution set
execute benchmark query hare crowdsourced total grammar%23NPR triple collect 1%2C619 crowd answer see table average standard+deviation crowd confidence obtain from grammar%23NPR be life+science grammar%23NPR sites.google.com%2Fsite%2Fhareengine grammar%23NPR dbpedia.org%2Fsparql0 ns w er s query crowd answer data+set answer 90+100 grammar%23NPR dg em en ts c pl time min sports pc %3A%28
12th minute 77%25 80+100+120+140+160+180 ns w er s query crowd answer data+set answer 90+100 90+100 grammar%23NPR dg em en ts c pl time min b music pc %3A%28
12th minute 82%25 ns w er s query crowd answer data+set answer 90+100 grammar%23NPR dg em en ts c pl time min c life+science pc %3A%28
12th minute 97%25 ns w er s query crowd answer data+set answer 90+100 grammar%23NPR dg em en ts c pl time min d movie pc %3A%28
12th minute 98%25 0+100+200+300+400+500 ns w er s query crowd answer data+set answer 90+100 grammar%23NPR dg em en ts c pl time min e history pc %3A%28
12th minute 75%25 figure size query answer crowd response+time domain minimum maximum value percentage judgement finish by 12th minute display first row number answer obtain query each domain second row percentage judgement complete function time judgement receive highlight
history music sport movie crowd confidence on+average very high indicate that majority answer be reliable
size compute number answer when query execute against grammar%23NPR q d grammar%23NPR crowd q grammar%23NPR
first row report on result clearly show that crowd produce answer all benchmark query
measure effectiveness hare compute query pc correspond ratio answer produce by hare answer when same query execute only against data+set i.e. pc q crowd d q d
maximum value pc report domain figure all query pc be greater than indicating that crowd enhance number answer furthermore from movie hare be able produce time more answer than other engine evaluate this query against grammar%23NPR
be important highlight that pc value affect by estimated completeness data+set when data+set completeness be high chance enhance answer size reduce for+instance portion grammar%23NPR life+science history domain exhibit high level completeness therefore average pc value achieve be not as high as other domain
result experiment confirm that hare be able correctly identify sub- query query produce incomplete result problem that crowd be feasible solution resolve missing value when execute grammar%23NPR query problem
result when execute benchmark hare knowledge grammar%23NPR total crowd domain grammar%23NPR triple answer w o w o answer sports 69+199 music 71+204 life+science 82+250 movie 120+490 history 160+476 total 1%2C619 crowd response+time analyze efficiency term time crowd contact by hare when execute query crowd response+time query correspond elapsed+time since first task pose grammar%23NPR until last answer retrieve figure second row report on fraction judgement crowd answer complete as function time all domain observe similar behavior small portion judgement finish much later than majority judgement highlight response+time plot figure second row
analyze percentage judgement complete until certain point+in+time at 12th minute first task submit at+least 75%25 judgement finish all domain be movie domain best observed scenario where 98%25 judgement finalize by this+time acknowledge that crowd response+time be not same order+of+magnitude as when query execute against data+set usually order second however be good tradeoff between answer completeness versus total execution+time whenever achieve by hare consider
quality crowd answer measure quality crowd answer compute precision recall mapping retrieve from crowd with+respect+to gold+standard d each query q crowdsourced triple pattern t q true positive correspond mapping t provide by crowd where t q d analogously false+positive be mapping t from crowd where t q d
report on result precision recall use heat map see figure altogether crowd be able respond out query both precision recall equal crowd achieve accuracy value range from
grammar%23NPR report on value recall observe that out query crowd be able answer all missing value furthermore query achieved recall be greater than only two case where crowd achieve low recall be query music movie question query be what be name american blues musician what be gross movie respectively although crowd show be skilled two domain general be predicate within domain where crowd do not ex avg recall avg b precision figure recall precision value dark color represent high value recall precision column correspond knowledge+domain sport mu music l life+science mo movie hi history row represent benchmark query
hibit high level expertise this observation provide evidence importance hare triple based approach on identification portion domain where crowd be knowledgeable
subsequent request hare exploit this knowledge avoid crowdsourcing two question again
value precision report on figure grammar%23NPR crowd be able provide correct answer out query furthermore crowd achieve precision great than query heat map clearly show heterogeneity level crowd expertise similarly recall precision values support importance hare triple based approach
related work database community propose several human computer query processing architecture relational data approach such as grammar%23NPR deco grammar%23NPR target scenario existing microtask platform directly embed query processing system grammar%23NPR provide sql like data definition query+language specify table column operator operator be subject crowdsourcing similarly deco be declarative approach allow specification fetch rule indicate how data obtain from humans resolution rule specify how conflict crowdsourced data solve grammar%23NPR define specification language describe micrtoasks term type question input output furthermore grammar%23NPR extend generate query plan plan combine both relational table crowd task reduce number task submit crowd relational engine rely on database+administrator crowd based workflow designer specify crowdsource when execute query contrary hare not rely on user knowledge but on data knowledge collect from crowd decide whether sub- query need pose against crowd
also apply other context data+management grammar%23NPR be recommendation system mine frequent pattern from personal+data collect via crowdsourcing
mine are specify oassis-ql sparql-like language grammar%23NPR exploit general+knowledge from ontological concept reason over data from crowd in+order+to reduce number subsequent crowdsourced question mine pattern grammar%23NPR be system cleanse tabular data exploit information from rdf knowledge+base kb crowd
discover pattern pattern align table column ontological definition kb identify type relationship column pattern then validate via crowdsourcing correct pattern use generate possible repair data+entry although both grammar%23NPR grammar%23NPR rely on sparql-like query rdf data system tackle problem enhance answer grammar%23NPR query evaluation that hare solve
grammar%23NPH et+al. present model estimate cardinality crowd answer this solution tailor decide when stop execution microtasks question potential large+number answer technique also use enrich process crowdsourcing triple pattern large potential missing value
conclusion outlook this paper define hare first hybrid query engine over linked data be able execute grammar%23NPR query as combination machine human driven functionality hare support microtask crowdsourcing feature as first-class computational component aim enhance existing query answer at execution+time definition completeness model enable hare decide on-the-fly part grammar%23NPR query resort human computation formally demonstrate that hare query engine not impact on time complexity evaluate grammar%23NPR query furthermore our experiment confirm that hare be able increase answer size up time
empirically observe that large portion up 98%25 human task submit by hare grammar%23NPR platform finish less+than grammar%23ENT accuracy crowd answer range from summary hare implement feasible solution effectively tackle studied problem
concentrate on study further approach capture crowd answer reliability as+well+as consider other quality dimension knowledge collect from crowd
reference grammar%23NPH grammar%23NPH crowdsourcing link data quality assessment grammar%23NPR page 260+276
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR query drive crowd mining grammar%23NPR page 589+600
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR data clean system power by knowledge+base crowdsourcing grammar%23NPR page grammar%23ENT grammar%23ENT
grammar%23NPH
answer query crowdsourcing grammar%23NPR page
grammar%23NPH s. oh count crowd grammar%23NPR 109+120

powered sort join grammar%23NPR
grammar%23NPR database query processing people cidr page 211+214
grammar%23NPH grammar%23NPH deco system declarative crowdsourcing grammar%23NPR grammar%23ENT grammar%23ENT
query optimization over crowdsourced data
781+792
grammar%23NPR collect structured data from crowd grammar%23NPR page 577+588
grammar%23NPH semantics complexity grammar%23NPR association+for+computing+machinery grammar%23NPR database syst.
adoption linked data good practice different topical domain grammar%23NPR page 245+260
foundation grammar%23NPR query optimization grammar%23NPR page
grammar%23NPH
enumeration query grammar%23NPR page 673+684
grammar%23NPH grammar%23NPH grammar%23NPH efficiently join group pattern grammar%23NPR query grammar%23NPR page 228+242 grammar%23ENT visual source grammar%23NPR grammar%23NPH e reggio+nell%27emilia enzo+ferrari via grammar%23NPH fabio.benedetti%40unimore.it grammar%23NPH e reggio+nell%27emilia enzo+ferrari via grammar%23NPH sonia.bergamaschi%40unimore.it grammar%23NPH e reggio+nell%27emilia enzo+ferrari via grammar%23NPH laura.po%40unimore.it data cloud triple source just three years from source grammar%23ENT grammar%23ENT
data produce at increasing rate tool lack produce high level representation dataset support user exploration query source overcome above problem significantly increase number consumer data devise new method tool call grammar%23NPR promote understanding navigation query source both expert beginner also provide standardized homogeneous summary source support user creation visual query on previously unknown dataset
extensively evaluate portability usability tool grammar%23NPR test on entire set dataset available at data grammar%23NPR i.e. source this paper showcase usability evaluation different feature tool schema summary representation visual query building obtain on user comprise both semantic+web expert beginner
schema extraction schema summarization visual query generation grammar%23NPR query generation introduction be eight years since grammar%23NPH design linked data principle now data consist more than thousand dataset collect several billion grammar%23NPR dataset generation also encourage by open+access trend importance highlight by grammar%23NPR produce by open data barometer grammar%23ENT grammar%23ENT g20+developing+nations largest industrial economy follow+up by pledge advance open data as tool against corruption united+nations recognize need grammar%23NPR linkeddatacatalog.dws.informatik.uni-mannheim.de%2Fstate grammar%23NPR barometer.opendataresearch.org%2Freport%2Fsummary permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york grammar%23ENT grammar%23NPR grammar%23ENT dx.doi.org%2F10.1145%2F2815833.28158492015 association+for+computing+machinery
revolution achieve global development goal although grow more more navigation visualization linked data be still at+the+beginning
portal such as well+known data hub catalog dataset dataset be available as on provide keyword search method identify dataset interest usually user have manually explore new dataset use grammar%23NPR query understand if dataset really contain information look+for follow that user no grammar%23NPR knowledge not become consumer data contain even skilled user be not easy task because be no fixed modeling rule design structure dataset usually external class properties use within dataset without formally define how relate class define locally
great number dataset publish without real documentation help on reveal their structure
tool grammar%23NPR aim solve above issue in+order+to empower user without technical skill explore understanding extract knowledge from dataset without any priori knowledge on source in+particular aim provide high level schema summary able capture structural information source enable class property browse powerful intuitive visual query builder empower user in- dept exploration instance source eventually generate grammar%23NPR query able extract piece knowledge user be concerned tool take+advantage query refinement panel grammar%23NPR compiler capture each change visual query refresh corresponding grammar%23NPR query result
this paper describe grammar%23NPR test portability tool on more than dataset demonstrate that our tool use great part dataset belong
conduct usability evaluation in+order+to show effectiveness grammar%23NPR represent structure dataset support user building query on unknown source result demonstrate effectiveness tool further highlight future line development
remainder paper be structured as+follows discuss related work section draw architecture motivation example section section illustrate use case scenario while section report evaluation on portability usability tool finally section sketch conclusion future line extension grammar%23NPR
work accomplish framework phd program organize by global grammar%23NPH grammar%23ENT fund by emilia+romagna et+al.
top most similar animal found by different algorithm inappropriate highlight bold
israel+defense+forces grammar%23NPR tf israel+defense+forces lethal+dose serval lion lion blackbuck stoat snow+leopard serval cougar lion cougar tiger leopard+cat tiger jaguar cougar serval cougar aardvark spotted+hyena asian golden cat cougar wildcat coyote leopard+cat grant gazelle gray+wolf spotted+hyena red+fox tiger stoat bongo antelope blue human eurasian lynx be indeed line human perception if so how well correlation be
task description human evaluation investigate second research question propose task task create formal set assess how human perceive knowledge difficulty level call quiz game task basically task involve play quiz game subject present quiz produce use three selected similarity measure namely grammar%23NPR tf israel+defense+forces tf israel+defense+forces lethal+dose quiz generate each difficulty level measure therefore be altogether quiz generate base on wildlife dataset use three different similarity measure rationale use subset baseline mainly base on following two consideration four baseline perform very similar pilot study more importantly four baseline plus proposed algorithm involve test quiz require grammar%23ENT subject complete report by grammar%23NPH et+al. that human evaluation test take grammar%23ENT result participant less focus more likely interrupt
above describe task offer on successfully use in+the+past develop gold+standard data various task such as natural+language+processing image labelling
present each subject job contain quiz task
job i.e. quiz perform by separate subject
each job record answer pick by subject also reduce randomness human evaluation subject instruct choose additional option not know if be not sure about answer quiz
selection automatically categorise as incorrect answer
model accuracy quantify difficulty level perceive by user human evaluation task introduce concept model www.mturk.com accuracy accuracy indicate percentage time user choose correct answer quiz generate by model here model refer particular similarity measure e.g. grammar%23NPR tf israel+defense+forces let qsk be answer select by sth subject kth quiz ck be correct answer kth quiz s denote number subject accuracy kth quiz calculate as+follows grammar%23NPR s qsk ck s. finally interest calculate model accuracy grammar%23NPR encode percentage time user choose correct answer test quiz difficulty level l generate by model m. derivation grammar%23NPR formalise equation+group grammar%23NPR k grammar%23NPR d where d be total number quiz difficulty level l generate by model m
correlation between model accuracy another set experiment investigate correlation between difficulty level suggest by similarity measure perceive by human as encode model accuracy our hypothesis be that if difficulty level suggest by similarity measure be line human perception pairwise similarity quiz have correlation model accuracy certain degree here pairwise similarity each quiz calculate by average+out similarity score between correct answer distractor i.e. incorrect answer that quiz
human evaluation task subject present test quiz generate by sherlock i.e. quiz difficulty level each similarity measure i.e. grammar%23NPR grammar%23NPR tf israel+defense+forces lethal+dose complete whole test take approximately grammar%23ENT each subject on+average next figure top animal similar found by different algorithm inappropriate highlight bold











easy
medium
grammar%23NPR
m y









easy
medium
grammar%23NPR
m y
grammar%23NPR









easy
medium
grammar%23NPR
m y
b tf israel+defense+forces








easy
medium
grammar%23NPR
m y
c tf israel+defense+forces lethal+dose figure quiz similarity base on different similarity measure on wildlife domain dataset
over 47%25 improvement overall accuracy demonstrate effectiveness proposed algorithm one reason why grammar%23NPR lethal+dose significantly outperform baseline be probably due fact that our algorithm not only consider direct indirect connection between rdf resource but also model information predicate by treat rdf graph as document this way our proposed algorithm can e rich semantic inform tion from data even condition where link between rdf resource be sparse
better compare illustrate clustering performance figure list top+ten animal animal be most similar group by different similarity algorithm figure animal list descending order base on their similarity one one be not same family as highlight bold observe from figure among four baseline grammar%23NPR perform best three animal cluster not belong same family as contrast grammar%23NPR be little accurate include six outlier cluster tf israel+defense+forces lethal+dose algorithm again give good performance only one outlier include
using human judgement examine previous pilot+experiment show that similarity measure especially proposed tf israel+defense+forces lethal+dose algorithm be potentially appropriate means measure quiz difficulty level this study still base on synthetic gold+standard without human evaluation therefore be necessary verify whether difficulty level capture by similarity measure be indeed line human judgement if so how strong correlation be
task description human evaluation investigate second research question propose task task create formal set assess how human perceive knowledge difficulty level call quiz game task basically task involve play quiz game subject present quiz produce use three selected similarity measure including grammar%23NPR tf israel+defense+forces tf israel+defense+forces ld quiz generate each difficulty level measure therefore be altogether quiz generate base on wildlife dataset use three different similarity measure rationale use subset baseline mainly base on concern that four baseline perform quite similar pilot study more importantly four baseline plus proposed algorithm involve testing quiz require grammar%23ENT complete test be eported by that human evaluation test take grammar%23ENT result participant less focus more likely interrupt
above describe task offer on mechanical turk successfully use in+the+past develop goldstandard data various task e.g. natural+language+processing labelling image p esented each subject job contain quiz task each job i.e. quiz perform by separate subject record answer pick by subject also reduce rando n s hum n evaluation subject instruct choose additional option not know if be not sure about answer quiz such selection automatically categorise as incorrect answer grammar%23NPR r similarity cc ac y grammar%23NPR r p tf israel+defense+forces r similarity cc ac y b tf israel+defense+forces r p tf israel+defense+forces r similarity cc ac y c tf israel+defense+forces lethal+dose r p figure grammar%23NPR correlation between model accuracy pair wise similarity quiz
whole test take min subject on+average
model accuracy quantify difficulty level perceive by user human evaluation task introduce concept model accuracy indicate percentage time user choose correct answer quiz generate by model here model refer particular similarity measure e.g. grammar%23NPR tf israel+defense+forces let qsk be answer select by sth subject kth quiz ck be correct answer kth quiz s denote number subject accuracy kth quiz calculate as+follows grammar%23NPR s qsk ck grammar%23NPH finally interest calculate model accuracy grammar%23NPR encode percentage time user choose correct answer test quiz difficulty level l generate by model m. derivation grammar%23NPR formalise equation+group grammar%23NPR k grammar%23NPR d where d be total number quiz difficulty level l generate by model m
correlation between model accuracy similarity distribution this experiment investigate correlation between difficulty level suggest by similarity measure perceive by human as encode model accuracy our hypothesis be that if difficulty level suggest by similarity measure be line human perception pairwise similarity quiz have some correlation model accuracy here pairwise similarity quiz calculate by average+out similarity score between correct answer distractor that quiz
show pair wise quiz similarity testing quiz where each histogram correspond quiz observe that similarity base on grammar%23NPR quite concentrate less+than difference between quiz high low similarity value be likely due fact that grammar%23NPR rely on direct indirect link between resource be relatively sparse wildlife rdf dataset as result grammar%23NPR produce similarity value very small difference quiz
classic tf israel+defense+forces scheme produce quite skewed similarity distribution easy medium cluster have very small similarity value difficult class have much high similar score contrast quiz similarity distribution each difficulty level obtain use proposed tf israel+defense+forces lethal+dose algorithm be much+more balanced well spread
show grammar%23NPR correlation between model accuracy pair wise similarity quiz generate from same model all data+point average over quiz difficulty level see that all three tested model model accuracy derive from human evaluation indeed show negative+correlation pair wise quiz similarity in+addition proposed tf israel+defense+forces lethal+dose show good correlation than both grammar%23NPR tf israel+defense+forces terms r value furthermore significance test tf israel+defense+forces lethal+dose be only measure p while other approach i.e. grammar%23NPR p grammar%23NPR p human evaluation result be line observation pilot study use gold+standard derive base on biological classification+system therefore conclude that similarity measure be appropriate means measure quiz difficulty level that proposed tf israel+defense+forces lethal+dose algorithm be superior baseline predict quiz difficulty level
conclusion this paper present generic framework sherlock generate educational quiz using linked data compare existing system sherlock offer distinctive capability control difficulty level generated quiz base on novel hybrid semantic similarity measure tf israel+defense+forces lethal+dose tf israel+defense+forces lethal+dose algorithm be simple but very effective effective outperform four strong baseline both pilot evaluation use synthetic gold+standard as+well+as human evaluation give 47%25 gain overall clustering accuracy over baseline
future work first plan carry+out more comprehensive user test evaluation further explore relation between quiz difficulty semantic similarity make sherlock system context aware be also interesting direction as semantic similarity be context dependent
acknowledgment this work support by british+broadcasting+corporation connect studio grammar%23NPR award make by grammar%23NPR digital+economy theme grammar%23NPR www.bbc.co.uk%2Fpartnersandsuppliers connectedstudio digital+economy hub award reference ep grammar%23NPR award make by united+kingdom economic+and+social+research+council award reference grammar%23NPR
reference linked data movie quiz answer be out there so be question blog+post

grammar%23NPH grammar%23NPH latent dirichlet journal machine+learning research
grammar%23NPH grammar%23NPH grammar%23NPH l. boyd-graber grammar%23NPH
reading tea leave how human interpret topic model advance neural information+processing+system page 288+296
learning from quiz use intelligent learning companion companion volume page 435+438
grammar%23NPH grammar%23NPH grammar%23NPH l. j li
large-scale hierarchical image database computer+vision pattern+recognition grammar%23NPR on page 248+255
editor wordnet electronic lexical+database

topic semantic representation psychological review
grammar%23NPH grammar%23NPR collecting association strength linked data triple game search computing volume grammar%23NPR page 223+239
similarity measure text document cluster
proceeding computer+science grammar%23NPR new+zealand page
introduction latent semantic analysis discourse process 259+284
combine local context similarity word+sense identification wordnet electronic lexical+database 265+283
y. feature lda supervised topic model automatic detection api documentation from page 328+343
measure semantic distance on link data use resource recommendation grammar%23NPR spring symposium link data meet artificial+intelligence
grammar%23NPH grammar%23NPH grammar%23NPH semi-automatic generation quiz learning artifact from linked data
linked learning 2nd international workshop on learning education data at grammar%23NPR
game purpose semantic+web intelligent+systems
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
effect sensory modality task duration on performance workload stress sustained attention human factor 219+233
mine web synonym grammar%23NPR ir versus lsa on proceeding 12th ecml page 491+502 united+kingdom

evaluate link data heuristics quiz quiz clean+up dbpedia grammar%23NPR
grammar%23NPR renowned individual semantic quiz jeopardy like quiz game ranking fact proceedings 7th on semantics page
grammar%23NPH grammar%23NPH verb semantics lexical selection
proceedings 32nd annual meeting on page 133+138 grammar%23ENT track growth semantic wiki community grammar%23NPH grammar%23NPH reed admiralty way grammar%23NPH conscientious+arsonists+%28ca%29 gil%40isi.edu dipsy%40isi.edu rmarkham%40usc.edu varunr%40isi.edu abstract contributor hundred semantic wiki site create structured information rwanda+defense+force every+day thus grow semantic content in+spades although wiki analyze extensively there be little analysis use semantic wiki create gather aggregate data from site show how this content grow over time make this detailed data readily available research community also present high-level analysis almost wiki index have less+than 5%2C000 page author keyword semantic wiki semantic+web rdf social knowledge collection association+for+computing+machinery classification keyword grammar%23NPR information interface presentation user+interface interaction style introduction semantic wiki allow community user structure information by create concept property property link wiki topic page et al semantic wiki be very successful platform social knowledge collection grammar%23NPH be hundred community use semantic wiki social knowledge collection collaboratively create structured content some semantic wiki have serious use such as scientific+knowledge organization have practical use for+example gardening restaurant finding some wiki have user user be on young side such as teens organize information about different character card trade perhaps best known semantic wiki be grammar%23NPR grammar%23NPR grammar%23ENT structured version although there be significant amount work study wiki community notably be very+little work on analyze community community generate structured content such as semantic wiki prior work analyze semantic wiki community terms organization contributor evolution structured content over time grammar%23NPH grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPH et al grammar%23NPR use data from semantic wiki about their creation edit as+well+as wiki user user edit study find that concepts not use very often not use at+all many wiki properties use all wiki although very small number user edit very large+number property assertion use almost every wiki however study be limited because data data collect be subset all semantic wiki focus on particular time+period focus on particular set question require data about provenance contribution be very useful if this kind study be more common repeatable our goal be make data about semantic wiki community readily available so that become very easy do study about their organization dynamic evolution wiki content this paper present our work on community resource study semantic wiki data be central repository detailed provenance data edit semantic wiki site include aggregated statistics statistics highlight most salient wiki as+well+as total amount semantic content all wiki also present high-level analysis use data from semantic wiki small size 5%2C000 page less focus on how user edit wiki add structure content analyze concepts property create amount editor involve create amount edit each category permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright hold by owner author s publication right licensed association+for+computing+machinery association+for+computing+machinery grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.related work give detailed overview semantic wiki thorough comparison alternative implementation including semantic grammar%23NPR grammar%23NPR while be many published analysis wiki e.g. grammar%23ENT grammar%23NPR et al grammar%23ENT semantic wiki community not study in+depth some prior work concern analyze semantic wiki community some work focus on success failure community terms reach critical+mass walk grammar%23ENT but look+at general user data wiki without focus on semantic aspect contribution our prior work address how user edit wiki add structure their content grammar%23NPH grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPH et al grammar%23NPR as mention introduction although be first comprehensive study semantic wiki community use limited number semantic wiki specific time+period our work seek provide more comprehensive data structured knowledge collection from volunteer study prior work including grammar%23NPR grammar%23ENT grammar%23NPR et al grammar%23ENT learner grammar%23NPR grammar%23NPH contrast semantic wiki contributor site not involve use content do interact as community collaborative ontology editor allow community user create class property property describe domain popular collaborative ontology editor be collaborative prot%C3%A9g%C3%A9 grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPR grammar%23ENT study collaborative ontology editor find that type contribution be diverse shape role contributor unlike semantic wiki not require much training user train use editor overview collect information about structure content existing semantic wiki together+with their provenance terms overview add content when be publicly available data accessible community focus on data from semantic grammar%23NPR grammar%23ENT since their content be easily accessible through grammar%23NPR architecture figure show overview architecture rest this section describe each aspect system in+detail get basic information from grammar%23NPR site site already exist be repository https%3A%2F%2Fsemantic-mediawiki.org https%3A%2F%2Fwikiapiary.com data about grammar%23NPR site also access semantic wiki directly collect more detailed data together+with provenance then compute aggregate statistics base on this detailed data implement as extension detailed data store database while aggregate statistics each wiki store as semantic properties that wiki as any wiki have user+interface user+interface allow user browse data total statistics all wiki show graph each wiki detailed information about growth structured content user+base show data collect by be available programmatically through api as be data from grammar%23NPR availability data software all data analyze this paper find at software be also available at access data semantic wiki be accessible through api all grammar%23NPR have api api allow access data about user content edit together+with time+stamp semantic grammar%23NPR have extended api access edit semantic properties concept provenance bee bot provenance bee be extension grammar%23NPR centralized repository summary information therefore semantic site grammar%23NPR be semantic site grammar%23NPR have list wiki site analyze each site page type website create each website some semantic properties include grammar%23NPR c media wiki grammar%23NPR get baseline wiki data get usage data get general sta s c compute aggregate sta s c wiki apiary get c data filter bot block user provenance bee compute aggregate sta s c store data grammar%23NPR db raw data db aggregate data grammar%23NPR provenance bee api grammar%23NPR api user interface figure overview architecture provenance bee grammar%23NPR age distribution
semantic+web skill distribution
schema summary so obtain high accuracy student student complete survey our laboratory be able complete task less+than grammar%23ENT average
generation functionality group task obtain overall accuracy 90%25 be very good result because last query be quite complex in+fact involve more class filter order condition student student complete survey our laboratory be able complete task less+than grammar%23ENT average be very promising result in+fact all student enrol have very low knowledge grammar%23NPR
qualitative evaluation evaluate score obtain answer qualitative question propose
summary browsing functionality figure have overview score obtain by user result cluster accord knowledge user semantic+web technology sus overall median value be according+to classify this functionality grammar%23NPR median value obtain distinguish among skilled unskilled user be rather similar versus so assume that this functionality appreciate by both kind user
also request rank level agreement three sentence see figure distribution result practically most user think that be easy browse task number n correct correct 89%25 89%25 89%25 27+100 total 162+148 91%25 table result task list table task number n correct correct grammar%23NPR 27+100 grammar%23NPR 96%25 grammar%23NPR 81%25 grammar%23NPR 85%25 total 90%25 table result query list table work as documentation dataset visualization be clear
generation functionality this functionality use all feature tool so assume that score obtain this step represent global score grammar%23NPR therefore distribution score obtain grammar%23NPR show figure obtain median score classify grammar%23NPR as grammar%23NPR always accord also this case not find particular difference among median value score among skilled unskilled user versus
fact both skilled unskilled user equally appreciate grammar%23NPR according+to score demonstrate that final user be unaware semantic+web technology explore query source grammar%23NPR be main goal grammar%23NPR in+order+to increase usage source user user not know grammar%23NPR be able query dataset first time user answer this question if already write grammar%23NPR query how find use grammar%23NPR manually writing grammar%23NPR query like just write my first grammar%23NPR query use grammar%23NPR nice on+the+other+hand one skilled user answer question above this way grammar%23NPR be cognitively less demanding receive also some criticism concerning some aspect gui e.g. browser render difference be very useful improve grammar%23NPR
criticism regard graph visualization ss become complex huge dataset start query be difficult user
conclusion future work this paper present grammar%23NPR tool visual exploration query source grammar%23NPR unveil intrinsic structure source by provide view dataset allow user visually compose refine query address this source
prove be effective tool facilitate user interaction source moreover write grammar%23NPR query be time-consuming boring task also expert thus navigate schema dataset select class attribute interest strongly simplify formulation query make more pleasant consumption linked data portability test show that grammar%23NPR be able process 61%25 accessible grammar%23NPR endpoint render 59%25 source survey conduct on user reveal good level usability classification as grammar%23NPR complete demo tool also present demo however some limitation arise from evaluation tool
graph visualization ss become messy huge dataset this might affect portability grammar%23NPR therefore currently study different solution solve this drawback for+example apply clustering technique group together some set node similar characteristic limit number node visualize neighborhood node be current focus user first solution allow visualize structure whole dataset but query building distribution evaluation by user about usability browsing functionality
distribution score schema summary browsing
distribution score query generation functionality
affect second option not affect query building but lose possibility represent whole dataset moreover use keyword search technique significantly improve selection element visual query
reference
mine rdf data prolod editor institute+of+electrical+and+electronics+engineers 30th on data engineering grammar%23NPR grammar%23ENT united+states+of+america grammar%23ENT grammar%23ENT page institute+of+electrical+and+electronics+engineers
determine individual score mean add adjective rating scale
usability study 114+123
online index extraction from linked open data+source linked data information extraction grammar%23NPR workshop hold at
visual summary linked open data+source poster grammar%23NPR
grammar%23NPR tool visual linked open data appear poster grammar%23NPR
grammar%23NPR performance storage system expose sparql endpoint
a quick dirty usability scale usability evaluation industry
linked data visualization model poster grammar%23NPR
grammar%23NPH s grammar%23NPR approach linked data survey semantic+web 89+124
expressive scalable query base faceted search over sparql endpoint grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH editor semantic+web grammar%23NPR grammar%23ENT volume lecture note computer+science page 438+453
facet graph complex semantic made easy semantic+web research application 7th grammar%23ENT grammar%23ENT grammar%23ENT proceeding part page 288+302
gfacet browser data proceeding international workshop on interact multimedia content international+medical+corps grammar%23ENT
grammar%23NPH linked data query wizard novel interface access grammar%23NPR endpoint proceeding workshop on linked data on co- located 23rd grammar%23ENT south+korea grammar%23ENT
grammar%23NPH grammar%23NPR linked data browser proceeding poster grammar%23NPR track 10th on grammar%23NPR page ceur-ws.org
grammar%23NPH fundamentals isparql virtual triple approach similarity based semantic+web task proceedings 6th semantic+web on grammar%23NPR grammar%23NPR page 295+309 springer-verlag grammar%23NPR multi- entity knowledge extraction framework scholarly document semantic search grammar%23NPH grammar%23NPH grammar%23NPR yang grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPH information+science technology computer+science university+park united+states+of+america computer+science engineering introduce grammar%23NPR multi entity knowledge extraction framework scholarly document pdf format
implement framework framework encapsulate opensource extraction tool currently leverage tet full text extraction scholarly document filter describe document classification grammar%23NPR header extraction grammar%23NPR citation extraction grammar%23NPR figure table extraction algorithm extraction while run as+a+whole extraction tool each module be highly customizable user substitute default extractor other extraction tool prefer by write thin wrapper implement abstract
framework design be scalable be capable run in+parallel use multi-processing technique experiment indicate that system default setup be cpu bound leave small footprint memory make good run on multi-core machine
good performance use dedicated server core take grammar%23ENT on+average process one pdf document
use index extract information help user quickly locate relevant result published scholarly document efficiently construct large knowledge+base in+order+to build semantic scholarly search+engine part run on grammar%23NPR digital+library search+engine
subject descriptor grammar%23NPR information+search retrieval information filtering grammar%23NPR digital+library system issue grammar%23NPR software performance evaluation efficiency effectiveness general+term metadata extraction introduction permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815834
completely change way researcher search relevant work almost all knowledge+domain grammar%23ENT estimate that total number scholarly article on english be at+least 114%2C000%2C000 unprecedented growth publication papers author find difficult read all relevant paper in+order+to locate useful result some case author cite paper just because be interested particular entity such as conclusion approach result table and%2For figure scholarly search+engine utilize entity help such search be foundation semantic scholarly search+engine
general scholarly document consist entity if not header text body bibliography figure table math algorithm even chemical+formula header encapsulate title author name author affiliation author email abstract publication year venue conference proceedings journal volume number page and%2For series text body contain all descriptive+text including ascii non- ascii character bibliography be list publication cite by current paper usually appear at end scholarly document name bibliography reference even note figure be very illustrative entity use present result certain trend use more frequently modern paper table be effective present structured data such as experimental result algorithm usually express pseudo- code be effective present way solve computational problem math appear as equation algorithm appear not only computer+science paper but also other science domain such as mathematics physics our goal be construct knowledge+base each scholarly document then encode scholarly document ontology
be great+deal effort developing method extract specific entity from scholarly document
widely use extract text attribute pdf document similar tool include grammar%23NPR crawl based digital+library title author be important cluster near duplicate paper base on assumption that no very few research paper have exactly same title author existing open-source header parser include grammar%23NPR grammar%23NPR be bibliography parser such as grammar%23NPR grammar%23NPR parse citation as+well existing table extraction tool include tabula automatic table extractor grammar%23NPR figure extraction tool develop general academic document caption figure biomedical pdf document algorithm extraction tool develop in+addition crawl based digital+library search+engine need filter re turned document by classify as scholarly not
be many existing tool extract semantic entity there not be framework propose integrate all+together provide framework do text-based analysis by convert unstructured data into structured data however focus on text+mining such as analyze log clinical notes through automatic annotation framework here aim at efficiently perform multi entity extraction use existing open source software including but not limited plain text header citation figure table algorithm this framework design be modular scalable have built-in wrapper selection extraction tool such as but easily substitute such as another text extractor e.g. by write simple wrapper implement interface grammar%23NPR be also configurable by specify number concurrent processes allow code process large corpus scholarly document on multi-core machine
paper organize section scribing typical metadata extraction system grammar%23NPR illustrate challenge design new extraction framework including all module how choose implement extraction tool section discuss design implementation section run this framework on random+sample data show performance section describe application application potentially make use framework section summarize discuss future work
design current design metadata extraction system grammar%23NPR include following module text extraction academic document filter citation extraction header extraction four module assemble perl script wrapper wrapper act like job dispatcher once start create job folder launch multiple thread work on batch+job by retrieve list unextracted document from crawl database then processing by perl script text extraction module use plain+text extraction academic document filter implement rule based classifier classifier look+for keyword phrase such as bibliography reference their variant from full text citation extraction module use grammar%23NPR header extraction module employ grammar%23NPR hereafter grammar%23NPR both grammar%23NPR grammar%23NPR require plain+text as input output grammar%23NPR grammar%23NPR further compile into single xml file use ingestion ingestion populate production database new document metadata add document into production repository index full text extraction process have very limited logging function current extraction module have following drawback limitation perl script be difficult maintain have poor portability be good
although rule based document filter achieve generally high accuracy 80%25 90%25 recall be below 80%25 indicate that miss significant fraction scholarly document sophisticated classification algorithm such as develop by base on structural feature supervise machine+learning do+better
grammar%23NPR evaluate against other header extraction tool result indicate that grammar%23NPR outperform other competitor independent evaluation perform verify superior extraction performance grammar%23NPR see below
4. current extraction be limited textual content
be desirable integrate multiple extraction tool figures table algorithm
current extraction module limit log function make difficult trace reason cause failed extraction
current design optimize tet grammar%23NPR grammar%23NPR make difficult switch other extraction tool this ad+hoc design significantly limit applicability extraction system
new design redesign extraction framework face several challenge first framework be highly portable modular i.e. be relatively easy fit another extraction tool into framework and%2For plug unplug one more module from framework for+example use tet plain+text extraction instead remove academic document filter most case require developing adapter thin wrapper specific extraction tool because different tool write different programming+language have different api input%2Foutput requirement etc. accomplish framework define abstract interface implement wrapper deal input output+file return standard extraction status specifically wrapper make system+call run application api either as command+line as service output error+message retrieve parse by wrapper determine status successful fail use control work flow log pluggable feature require centralized design so module change need minimal modification source+code configuration second design consider information share between module make+sure that dependency generate before use framework not waste time duplicate some workload multiple module require define required optional module arrange certain order for+example while grammar%23NPR take pdf file as input other header extraction tool accept plain+text because full text be prerequisite many extraction tool framework by+default always extract full text at first place another example be that if grammar%23NPR use both header citation extraction only need run once when extract header citation extraction module work directly on grammar%23NPR output third grammar%23NPR proposed architecture pi represent individual process blue box enclose framework
handle exception from different extraction tool their result appropriately for+example if full text contain zero line treat as failure extraction even if empty text+file generate
high level design proposed framework depict figure b be very similar except that support different input mode mode job loader load document directly from file+system i.e. folder contain pdf document extract be convenient way run batch extraction job without set+up crawl database repository job loader then create job pool pool work like set dictionary document path derived equivalent as key each process have job controller j end job clean r clean temporary+file integrate extraction result write extraction status log file j module switch process between on+and+off central job+control work like commander commander read external+command e.g. stop command from user by periodically check setting configuration+file signal then pass job controller controller stop fetch new document current job list finish this softtermination feature be especially helpful when framework run mode b because retain integrity extraction job by avoid partially extract document
framework bring+up again shutdown not rerun same document run before not deal document document partially extract without status update database mode b document list retrieve from database job clean report extraction status each document database
work mode b require extra coding coding depend on specific database schema focus on design experiment on framework database query be usually much fast than extraction so be generally not bottleneck
extraction framework each process execute a+number+of extraction module em current default grammar%23NPR grammar%23NPR flowchart
include text extraction module header extraction module grammar%23NPR citation extraction module grammar%23NPR table extraction module grammar%23NPR figure extraction module fx algorithm extraction module grammar%23NPR two design option consider first design option within each process em arrange sequentially i.e. each em invoke one previous finish second design option within each process each em call by subthread em execute in+parallel choose first be because+of substantial time difference cost execute different em for+example depend on extractor citation extraction be ten times slow than header extraction see experiment result below if choose second design option subthread execute header extraction be idle while citation subthread work contrast sequential arrangement utilize cpu more efficiently be easy scale+up up by add more core sequential arrangement be also easy implement because be no need deal synchronization issue for+example as table extraction base on full text keep idle until full text extract flowchart sequential design depict figure job controller job clean not display full text header em require other em be optional next section discuss default extraction tool each em
full text extraction commercial text extraction tool such as believe perform better than open+source counterpart e.g. however be active development so be reasonable evaluate base on their current version
testing sample comprise pdf document randomly select from crawl database grammar%23NPR so sample contain academic document because build ground truth text extraction be not feasible grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH beat some common+sense into interactive application grammar%23NPH al grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH searching common+sense populating grammar%23NPR from proceedings 20th on artificial+intelligence grammar%23NPR provenance bee data available from skc.isi.edu%2Fprovenancebeewiki accessed grammar%23ENT provenance bee software available from https%3A%2F%2Fgithub.com%2Fikcap%2Fprovenancebee accessed grammar%23ENT et al grammar%23ENT grammar%23NPH how ontology make evaluation hidden social dynamics behind collaborative ontology engineering project grammar%23ENT collaborative development large-scale biomedical ontology biomedical research grammar%23NPH john+wiley+%26+sons grammar%23ENT grammar%23NPR collaborative ontology editor knowledge+acquisition tool 11+165 grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPR free collaborative knowledgebase communications association+for+computing+machinery walk grammar%23ENT grammar%23NPH grammar%23NPR interactive tool visualize creation process behind collaboratively grammar%23NPR ontology figure page editor against total page page edit total page against page edit figure property editor against total property property edit total properties against property enhance query answer via crowdsourcing grammar%23NPR maribel.acosta%40kit.edu e.simperl%40soton.ac.uk fabian grammar%23NPH social+science fabian.floeck%40gesis.org grammar%23NPH mvidal%40ldc.usb.ve abstract due semi- structured nature rdf data miss value affect answer completeness query that pose against rdf overcome this limitation present hare novel hybrid query processing engine engine bring+together machine human computation execute query propose model model exploit characteristic rdf in+order+to estimate completeness portion data+set completeness model complement by crowd knowledge use by hare query engine on-the-fly decide part query execute against data+set via crowd computing evaluate hare create execute collection query against data+set experimental result clearly show that our solution accurately enhance answer completeness
subject descriptor information+system crowdsourcing information+system resource+description+framework rdf keyword rdf data crowd knowledge query execution crowdsourcing grammar%23NPR completeness model query introduction initiative encourage publication data+set follow grammar%23NPR recent statistics grammar%23ENT show that more than 1%2C000 data+set be publicly available using rdf other technology majority data+set create by integrate multiple typically heterogenous source contain fair share missing value yet due semi- structured nature rdf data such incompleteness not easily detect negative effect on query processing for+example run query against grammar%23NPR www.w3.org%2Fdesignissues%2Flinkeddata permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america association+for+computing+machinery grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815848
data grammar%23NPR ask movie including their producer film new+york by universal picture return no producer 14%25 movie result set case like this being common occurrence application further technique need improve this data aspect subsequent query processing result recent research suggest that microtask crowdsourcing provide platform implement effective hybrid approach data quality assessment other kind data relational+database community embrace similar idea design advanced query processing system system combine human computational intelligence however work so+far propose comprehensive solution em bedding crowd computing feature into query execution reinforce management instead existing proposal focus on how specify part query resort human involvement typically devise bespeak query+language extension extension apply manually on top established database technology such approach be less feasible scenario scenario confront decentralized large volume semi- structured data this work tackle problem automatically identify portion query against rdf data set that yield incomplete result devise solution where miss value assess via microtask crowdsourcing
present hare hybrid query processing system system combine human computer capability run query against rdf data+set hare aim at enhance answer completeness query by find missing value data+set via microtask crowdsourcing our solution provide highly flexible crowdsourcingenabled query execution no extension grammar%23NPR rdf require user configure level expected answer completeness each query execution go+back our example hare detect that sub- query ask movie producer resort crowd in+order+to collect missing data do hare rely on completeness model detect part query potentially affect by miss value resort crowd query engine efficiently combine crowd answer intermediate rdf result
maintain knowledge+base knowledge+base capture knowledge collect from crowd crowd opportunistically exploit discern whether crowd be likely solve question accurately
evaluate hare craft collection query against version grammar%23ENT execute our system goal experiment be analyze behavior hare when query execute against grammar%23NPR endpoint grammar%23NPR wiki.dbpedia.org%2Frdf type db db-yago db berlin rdf type rdf type db owl country db db db db owl country db owl country portion city db berlin db link country contrary db country value miss
prefix db-yago dbpedia.org%2Fclass%2Fyago prefix db owl dbpedia.org%2Fontology select grammar%23NPR city country where city db-yago result city db owl country country result binding submit crowd b query select city country such that city be capital locate country portion query indicate that out european capital associate country e.g. db berlin
type db db-yago db berlin rdf type rdf type db owl country db db db db owl country db owl country db c crowd state that be country therefore value property db owl country db be db
motivating example depict portion data+set city country miss value rdf graph highlight b query execute against portion query highlight affect by miss value be subject crowdsourcing c crowd answer map into rdf store augment result set subsequent query
grammar%23NPR platform empirical result clearly show that our approach reliably augment response completeness while crowd answer achieve accuracy value from also observe that answer produce reasonable time via crowdsourcing yet increase speed crowd answer be not core issue this work
our main contribution be novel hybrid query engine enhance answer query exploit crowd knowledge formally probe that query evaluation our solution come for+free term time complexity empirically demonstrate feasibility our solution as+well+as quality term answer completeness effectiveness our approach
motivate example rdf data+set data+set contain semi- structured information extract from among other domain contain information about city country portion depict figure grammar%23NPR oval represent uri edge correspond relationship between node annotate corresponding predicate city db berlin db db classify as grammar%23NPR moreover db berlin db associate via predicate db owl country their corresponding country however value property db owl country db miss as show figure grammar%23ENT
query present figure grammar%23NPR retrieve data from this query select city country such that city be capital locate country when execute this query against data+set total number answer be contain entity that classify as european capital line figure grammar%23NPR that link their corresponding country line figure grammar%23NPR however by execute only triple pattern on line reveal that contain binding european capital mean that information available data+set be not possible produce complete answer given query general this problem arise when part query match against portion rdf data+set missing incomplete value this example estimated completeness portion be therefore first goal our work identify part query that affect by incomplete portion rdf data+set
find missing value country our example resort crowd reach via grammar%23NPR platform i.e. crowdsourced binding city db owl country country query from figure grammar%23NPR crowd affirm that out city locate country therefore result set query augment from answer for+instance crowd determine that country value db be db grammar%23NPR crowdflower.com as depict figure grammar%23ENT second goal our work be crowdsourcing potential missing value efficiently combining crowd answer result from data+set during query execution
preliminary rdf grammar%23NPR according principle data publish on describe use resource+description+framework rdf
be graph based data+model where node graph link via directed labeled edge each pair connected node call rdf triple rdf node correspond resource literal string resource either identify by uri unidentified denominate blank node node model existential variable graph rdf graph also denominate rdf data+set follow notation from grammar%23NPR et+al. grammar%23NPR et+al. present formal definition rdf triple graph
rdf triple data+set let b l be disjoint infinite+set uri blank node literal respectively
tuple s p o u b %C3%97 u %C3%97 u b l denominate rdf triple where s call subject p predicate o object rdf data+set rdf graph be set rdf triple
recommended query+language access rdf data be grammar%23NPR
this work focus on select query query return set variable their mapping result select query grammar%23NPR expression define following
grammar%23NPR expression select query let v be set variable disjoint from u b l. grammar%23NPR expression build recursively as+follows triple pattern t u v %C3%97 u v %C3%97 l u v be expression if be expression r be filter condition then filter r be expression let q be grammar%23NPR expression s v finite+set variable select query be expression form select q
focus on query evaluation set semantics
evaluation query over rdf data+set base on mapping mapping be partial function v b l u from subset variable rdf terms domain mapping dom be subset v define
mapping are compatible write if x x all x dom dom further var t denote all variable triple pattern t t be triple pattern obtain when replace all x dom var t t by x
grammar%23NPR algebra let ohm grammar%23NPR grammar%23NPR map set r denote filter condition s v finite+set variable expression grammar%23NPR algebraic operationsare define as+follows grammar%23NPR grammar%23NPR l r l grammar%23NPR r grammar%23NPR l r grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR l grammar%23NPR r grammar%23NPR l r grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR s ohm ohm dom s dom s r ohm ohm r where refer built-in function define
result evaluate query over rdf data+set be function translate query expression into algebraic grammar%23NPR operation formally define as+follows
semantics let d be rdf data+set t triple pattern q expression r filter condition s v finite+set variable let d be function translate grammar%23NPR expression into grammar%23NPR algebra operator as+follows t d dom var t t d d d d d d d d q filter r d r q d select q d s q d furthermore be sequence triple pattern filter expression that combine and all grammar%23NPR expression call graph pattern remainder this paper refer select q as
hare approach following formalize problem tackle this work present architecture our proposed solution
problem definition rdf data grammar%23NPR grammar%23NPR grammar%23NPR evaluate grammar%23NPR virtual data+set data+set contain all triple triple be d. problem identify portion q yield miss value define as+follows all bgp b grammar%23NPR grammar%23NPR tn q identify subset p grammar%23NPR such p d p d once p identify problem resolve missing value enhance final answer q consist on create mapping such p d p d propose hare query engine engine automatically identify portion query yield incomplete result resolve via crowdsourcing figure depict component hare receive as input quality threshold rwanda+defense+force completeness model estimate completeness portion data+set query optimizer generate plan from q execute by query engine engine take into consideration completeness model rdf triple collect from crowd tackle first problem present equation+group potential miss value pass grammar%23NPH contact crowd generate mapping tackle second problem present equation+group hare engine efficiently combine result retrieve from data+set human input produce final+result q
rwanda+defense+force completeness model task human input crowd knowledge query engine crowd grammar%23NPR grammar%23NPR grammar%23NPR query optimizer grammar%23NPH legion+of+doom cloud query plan crowdsourcing triple pattern rdf data+set input result q binding from crowd rdf data output aggregate human input figure hare architecture hare receive as input quality threshold on-the-fly detects portion q that yield incomplete result hare combine intermediate result from data+set value provide by crowd augment answer q
rwanda+defense+force completeness model propose model estimate completeness portion rdf data+set intuition behind our model be capture number different object that resource s have certain predicate p i.e. predicate multiplicity resource md s p
compare predicate multiplicity all resource belong same class i.e. capture number distinct value each resource class c specific predicate p advanced+micro+devices c p following define predicate multiplicity resource say that resource occur data+set d if exist rdf triple d where s be subject object
predicate multiplicity rdf resource give rdf resource s occur data+set d multiplicity predicate p resource s d denote md s p be md s p o s p o d consider rdf graph depict figure contain four node type schema.org movie this figure movie link their producer via db prop producer predicate multiplicity this predicate present each movie for+example resource s db grammar%23NPR have three value predicate p db prop producer therefore md s p be this case
intuition our model now look+at multiplicity all resource belong same class define multiplicity class c with+respect+to predicate p
assume that sub- class relationship materialize d
aggregated predicate multiplicity class each class c occur rdf data+set d aggregated multiplicity c over predicate p denote advanced+micro+devices c p be advanced+micro+devices c p df md s p s p o d s c d e s c correspond triple s rdf type c type mean that subject s belong class c f be aggregation function
that number value predicate rdf data+set exhibit skewed distribution definition f correspond median suppose class schema.org movie comprise only four movie figure median be aggregation function
multiplicity schema.org movie with+respect+to predicate dbprop producer i.e. advanced+micro+devices schema.org movie db prop producer be
completeness rwanda+defense+force resource s given predicate p give by aggregated grammar%23NPR all class where s belong this case class high multiplicity determine how complete s be with+respect+to type db grammar%23NPH db grammar%23NPH db grammar%23NPH db grammar%23NPH db grammar%23NPR db prop producer schema.org movie md db grammar%23NPR db prop producer grammar%23ENT non-stop film md db non-stop film db prop producer md db grammar%23NPR db prop producer grammar%23ENT grammar%23NPR db grammar%23NPR db grammar%23NPR rdf type rdf type rdf type db prop producer db prop producer db prop producer db prop producer db prop producer figure portion data+set schema.org movie be class db grammar%23NPR db grammar%23NPR db grammar%23NPR film db grammar%23NPR be instance this class movie link producer via db prop producer property each movie be md value db prop producer property e.g. md db grammar%23NPR be since this movie have two producer analogously db grammar%23NPR movie db grammar%23NPR db grammar%23NPR film have no producer then md
completeness rwanda+defense+force resource respect predicate give rwanda+defense+force resource s predicate p occur data+set grammar%23NPR be class d such s d s grammar%23NPR d. completeness s with+respect+to p d be grammar%23NPR s p define as+follows grammar%23NPR s p md s p am d if am will otherwise where am will max advanced+micro+devices p advanced+micro+devices grammar%23NPR p for+example suppose movie db grammar%23NPR also belong class dbpedia.org%2Fontology%2Ffilm aggregated multiplicity this class with+respect+to predicate db prop producer be
completeness db grammar%23NPR be
representation crowd knowledge knowledge from crowd capture three knowledge+base model as fuzzy set grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR comprise rdf triple triple belong data+set while not exist accord crowd grammar%23NPR finally grammar%23NPR contain association that crowd not establish follow our motivating example crowd determine that t db db owl country db therefore t belong grammar%23NPR all triple fuzzy set annotate membership degree m m compute by cf section m state reliability crowd answer
representation crowd knowledge given d rdf data+set crowd pool human+resources grammar%23NPR be virtual data+set such+that compose all triple that should be ind representation knowledge crowd denote grammar%23NPR define as tuple grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR be fuzzy set over rdf data compose tuple quad form s p o m such m be membership degree rdf triple s p o corresponding fuzzy set s p o m grammar%23NPR iff o u l according+to crowd s p o belong d s p o m grammar%23NPR iff o b according+to crowd s p o not belong d s p o m grammar%23NPR iff o u b l according+to crowd membership s p o d be unknown
that grammar%23NPR contain triple triple be not d be important consider information store grammar%23NPR when determine completeness resource account answer previously retrieve from crowd analogous definition define completeness resource consider data grammar%23NPR
completeness rwanda+defense+force resource respect predicate grammar%23NPR give rwanda+defense+force resource s predicate p occur knowledge+base grammar%23NPR let grammar%23NPR be class rdf data grammar%23NPR such s d s grammar%23NPR d. completeness s with+respect+to p grammar%23NPR be as+follows grammar%23NPR s p grammar%23NPR s p am d if am will otherwise grammar%23NPR s p o s p o grammar%23NPR s p o grammar%23NPR s p o grammar%23NPR am will max advanced+micro+devices p advanced+micro+devices grammar%23NPR p
illustrate previous definition consider that resource db grammar%23NPR also belong class dbpedia.org%2Fontology%2Ffilm aggregated multiplicity this class over predicate dbprop producer be suppose crowd declare that this movie produce by db grammar%23NPH this fact be part grammar%23NPR then completeness db grammar%23NPR grammar%23NPR be
representation crowd knowledge as grammar%23NPR grammar%23NPR grammar%23NPR different semantics allow+for easily modeling contradiction uncertainty from crowd
contradiction contradiction occur when crowd state that rdf triple t s p o exist not exist at+the+same+time i.e. t grammar%23NPR t grammar%23NPR contradiction degree about t measure as+follows c s p m m m avg m s p o m grammar%23NPR m avg m s p o m grammar%23NPR
value close indicate high consensus about existence triple virtual data+set d illustrate suppose that crowd enquire provide dbprop producer db grammar%23NPR crowdsourced answer be grammar%23NPH be producer membership degree i.e. grammar%23NPR grammar%23NPR grammar%23NPR db grammar%23NPR dbprop producer db grammar%23NPH ii have no producer membership degree i.e. grammar%23NPR grammar%23NPR grammar%23NPR db grammar%23NPR db prop producer o contradiction degree about producer this movie be
blank node specify prefix automatically quiz difficulty level using similarity measure united+kingdom chenghua.lin%40abdn.ac.uk dong grammar%23NPR british+broadcasting+corporation future media technology knowledge learning united+kingdom dong.liu%40bbc.co.uk united+kingdom pang.wei%40abdn.ac.uk united+kingdom eapeh%40bournemouth.ac.uk abstract this paper present semi-automatic system sherlock quiz generation using linked data textual description rwanda+defense+force resource sherlock be distinguished from existing quiz generation system ability control difficulty level generated quiz cast problem perceive level knowledge difficulty as similarity measure problem propose novel hybrid semantic similarity measure using linked data extensive experiment show that proposed similarity measure outperform four strong baseline both pilot evaluation use synthetic gold+standard as+well+as human evaluation give 47%25 gain cluster accuracy over baseline
subject descriptor grammar%23NPR artificial+intelligence miscellaneous general+term algorithm experimentation permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright hold by owner author s publication right licensed association+for+computing+machinery association+for+computing+machinery grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815842 keyword quiz generation linked data rdf educational games semantic similarity text analytics introduction interactive game be effective way helping knowledge transfer between humans machine on one hand effort make design game purpose semi automate wide range knowledge+transfer task by leverage wisdom crowd for+instance symmetric asymmetric verification game develop assist semantic+web task such as ontology building ontology alignment content annotation entity interlinking likewise some researcher develop quiz like games rank rate clean+up linked data this way factual knowledge transfer from humans especially domain expert computer on+the+other+hand work also do unleash potential linked data generate educational quiz aid learner knowledge+acquisition from knowledge+base when build quiz generation system use linked data most existing approach base on domain specific template creation quiz template heavily rely on ontologist linked data expert
system system generate quiz different difficulty level better serve user needs however such important feature rarely offer by existing system some researcher determine difficulty quiz by simply assess popularity rwanda+defense+force resource without consider fact that difficulty level quiz directly affect by selection wrong answer candidate also most common way generate wrong answer candidate be randomly select from result query linked data repository hence provide no means control difficulty level quiz generation our knowledge while different similarity measure widely use measure degree closeness separation target object problem how well similarity measure use represent degree knowledge difficulty terms human perception be still relatively unexplored
this paper demonstrate novel semi-automatic quiz generation system sherlock empower by semantic machine+learning technology sherlock be distinguished from existing quiz generation system by ability control difficulty level generated quiz base on novel linked open data lethal+dose base hybrid semantic similarity measure call tf israel+defense+forces lethal+dose
investigate how well proposed algorithm use represent difficulty level knowledge i.e. difficult medium easy evaluate performance tf israel+defense+forces lethal+dose against four strong baseline i.e. two knowledge based two text-based similarity measure base on observe that knowledge based measure give good performance predict easy cluster compare text-based measure but be inferior prediction difficult medium cluster our proposed hybrid semantic measure tf israel+defense+forces lethal+dose outperform four strong baseline give at+least 50%25 gain overall clustering accuracy in+addition discover human evaluation model accuracy derive from human quiz test indeed show strong correlation pair wise quiz similarity proposed algorithm again give good performance
rest paper organise as+follows section review related work present sherlock architecture respectively hybrid semantic similarity algorithm be detailed section finally experimental result discuss section conclude paper section
related work grammar%23NPR game series symmetric asymmetric verification game present aim motivate human contribute building semantic+web grammar%23NPR be representative symmetric verification game build following concept attempt solve problem ranking rdf triple within description entity other quizlike game focus on ranking rate cleansing linked data assumption underlie game be that frequency question correctly answer imply importance supporting linked data use create quiz however focus game be harness human+intelligence perform task task not be automated rather than create learning experience human
contrast grammar%23NPH et+al. present template based method generate educational quiz conversational agent introduce guide learner dynamically select quiz accord learner needs
be another representative work use linked data template based quiz generation be able generate quiz relate user selected actor actress ask question about director release date character film actor actress appear
common limitation share by existing quiz generation system be domain dependent issue whenever apply template based quiz generation method new domain significant human effort require on task such as create new question template write grammar%23NPR query accord domain specific ontology define rule collect wrong answer quiz in+addition difficulty quiz play grammar%23NPR www.bbc.co.uk%2Fnature%2Fwildlife important role formal learning however as state many quiz generation system suffer from fact that generated quiz be either too simple too difficult largely due lack quantitative+analysis on relation between wrong answer candidate correct answer in+turn motivate develop systematic way measure quiz difficulty level use semantic similarity measure
similarity measure text-based approach measure text similarity use for+a+long+time application natural+language+processing related area textbased measure try identify degree similarity between text unit use statistical pattern word derive from large corpus where most representative measure be cosine similarity average squared euclidean distance some more advanced approach rely on word co-occurrence pattern derive from large corpus indicate degree statistical dependence between text unit statistical dependence then use as measure text similarity further representative approach this line include model model belong topic model family
knowledge based approach contrast corpus based approach purely orient on statistical technique knowledge based approach rely on human organised knowledge e.g. semantic+network wordnet linked open data encode relation between collection concept
be large english lexical knowledge database terms group into different set know as synsets list synonym a+number+of measure develop base on wordnet hierarchy such as access semantic relatedness word entity identify word+sense different context close work our proposed hybrid semantic similarity measure be also use graph information rwanda+defense+force resource computing semantic similarity however note that similarity computation purely rely on statistics direct indirect out connection among rwanda+defense+force resource without consider importance predicate contrast only work rdf data proposed tf israel+defense+forces lethal+dose algorithm deal both literal value textual description
first give brief overview sherlock system as deploy proposed tf israel+defense+forces lethal+dose algorithm control difficulty level quiz generation serve as basis our experiment figure depict overview sherlock quiz generation framework component logically divide into two group online offline within sherlock framework different component interact each via two shared database respectively contain information about question answer quiz distractor i.e. incorrect answer
integration collect two different type data structure rdf data publish by data+collection integration similarity computation ld similarity adaptive clustering template based question answer database incorrect distractor database quiz quiz renderer figure overall architecture sherlock
british+broadcasting+corporation unstructured text describe object entity collect from british+broadcasting+corporation website dataset play two main role i.e. serve as knowledge+base quiz generation calculate similarity score between object entity
description on dataset preparation give experiment section
computation similarity computation module be core offline part architecture similarity computation module first access rdf store text corpus then calculate similarity score between each object entity pair second step module perform k mean cluster partition wrong candidate answer into different difficulty level accord their similarity score correct answer quiz here empirically set k corresponding three predefine difficulty level i.e. easy medium difficult
based question answer generator quiz generator component adopt template based method similar be able boost system situation cold+start coping data from new domain for+instance template following animal be instantiate by replace variable rdf label animal
renderer quiz renderer module realise user+interface through user interact system see figure question correct answer retrieve from dedicated database wrong answer candidate select from result similarity computation module be worth noting that value foaf depiction rdf store provide link image use render quiz user+interface also offer nice feature by allow user tune+up down difficulty level next quiz depend on whether user fail succeed play current quiz
methodology this section describe main algorithm develop sherlock as recall key challenge our work be measure difficulty level quiz this end develop hybrid similarity measure by combine novel linked data based tf israel+defense+forces scheme classical text-based cosine similarity measure call tf israel+defense+forces lethal+dose
rdf dataset formalise as graph direct indirect distance graph use measure similarity between rwanda+defense+force resource as case while report be effective on large scale dataset such as freebase importance predicate rwanda+defense+force resource not consider limit accuracy address this issue propose novel linked data based tf israel+defense+forces scheme by map named graph into vector take predicate information into account resulting linked data based tf israel+defense+forces vector then combine cosine similarity measure calculate semantic similarity between two rwanda+defense+force resource before describe proposed algorithm first give formal definition few technical term including term sentence document corpus
term sentence rwanda+defense+force statement i.e. tuple subject predicate object define as sentence two type combination such as subject predicate predicate object regard as term for+example cheetah will family family+felidae be sentence whereas cheetah will family will family family+felidae be two terms sentence here will be namespace
document corpus named graph graph be related rwanda+defense+force resource be document collection document be corpus
rwanda+defense+force statement show list constitute document this document contain three sentence describe animal cheetah
example rwanda+defense+force document
cheetah will family family+felidae will order will class mammal
www.bbc.co.uk%2Fnews%2Fscience-environment foaf document foaf grammar%23NPR cheetah
relation between term sentence document grammar%23NPR www.bbc.co.uk%2Fontologies%2Fwildlife grammar%23ENT shtml user+interface when incorrect choice make b user+interface when correct choice make
user+interface play quiz
tf israel+defense+forces lethal+dose algorithm
corpus c i.e. collection named graph name graph c name graph b c idf value term c compute use
semantic similarity between b
each term p o c do if term p o exist b then wp tfidf p o idf p o else wp tfidf p o end if end grammar%23NPR link data based tf israel+defense+forces vector ta grammar%23NPR grammar%23NPR wap tb grammar%23NPR grammar%23NPR wbp b compute semantic similarity grammar%23NPR ta tb ta tb
document d contain sentence s p o then say term s p p o be document d i.e. s p d p o d
tf israel+defense+forces lethal+dose algorithm now describe our grammar%23NPR lethal+dose algorithm definition above classical tf israel+defense+forces scheme then apply on rdf dataset give term t document d corpus c calculate as+follows tf t d if t d if t d idf t c log c d c t d be worth noting that apply boolean tf function calculation mean that if triple belong graph be otherwise by apply equation onto rdf dataset named graph then transform into vector once linked data based tf israel+defense+forces vector i.e. ta tb two rwanda+defense+force resource compute then calculate their semantic similarity use cosine similarity summary tf israel+defense+forces lethal+dose algorithm give algorithm
wildlife textual dataset statistics note denote before preprocess denote preprocess
doc avg. doc avg. doc vocab grammar%23NPR length size size wildlife 1%2C190%2C652 26%2C004 18%2C237 grammar%23NPR ta tb ta tb experiment this section explore how well similarity measure use suggest quiz difficulty level that match human be perception compare several well know similarity measure against proposed tf israel+defense+forces lethal+dose algorithm in+particular investigate following research question be similarity measure s appropriate assess quiz difficulty level what extend can quiz difficulty level suggest by similarity measure s match humans perception knowledge difficulty level pilot evaluation on not try give general definition difficulty cover wide range psychological aspect from emotional problem intellectual physical+challenge instead consider notion difficulty sense use quiz generation build as combination predefine candidate of+course study overall difficulty given quiz involve multiple+factor such as intellectual level knowledge cover quiz user knowledge background etc. preliminary study try address problem less complicated scenario difficulty level quiz directly drive by similarity between correct answer wrong answer candidate
data conduct preliminary experiment measure quiz difficulty level base on dataset choice grammar%23NPR easy order mammal class difficult tiger cheetah serval snow+leopard leopard+cat clouded+leopard family+felidae family arctic+fox maned+wolf african wild dong rabbit wild+horse raccoon figure deriving gold+standard dataset use biological classification+system
clustering accuracy different similarity measure measure quiz difficulty level unit number bold+face denote good result their respective tf israel+defense+forces tf israel+defense+forces dataset rwanda+defense+force rwanda+defense+force text text rdf difficult medium easy overall dataset evaluation base on fact that be no readily available gold+standard benchmarking from literature wildlife dataset each animal label use biological classification+system i.e. family order class naturally use as gold+standard evaluation according+to statistic from british+broadcasting+corporation website be most frequently british+broadcasting+corporation website indicate broad public+interest wildlife data in+particular prepare two different version dataset i.e. structure rdf data unstructured textual data
data as wildlife dataset grammar%23NPR website already publish rdf data so harvest rdf data directly from two data+source total be 49%2C897 rdf triple dataset
data in+addition+to rdf data also collect dataset contain textual description object entity e.g. different animal etc. from british+broadcasting+corporation website here textual dataset mainly use calculate text-based similarity score between object entity so use predict quiz difficulty level preprocessing use html parser extract content from html+page then removed wildcard word token alphanumeric+characters follow by stop word removal porter stemming textual dataset statistics summarise table
experimental result tackle first research question pilot evaluation formulate problem perceive level knowledge difficulty as similarity measure problem hypothesis be that if some object entity share lot semantically similar property tend have high degree semantic relatedness subtle difference hence more difficult disambiguate vice+versa
derive gold+standard wildlife dataset one intuitive approach be make use biological classification+system define that if some animal have same family label e.g. cheetah serval then animal be very similar each other hence difficult disambiguate likewise if some animal have same order label but from different family be less similar correspond medium difficulty level when generate quiz similarly quiz generate base on animal same class label but different family order label be most dissimilar correspond easy level note that gold+standard derive independently not rely on any information feature use by baseline tf israel+defense+forces algorithm illustrative example gold+standard show figure
pilot evaluation compare proposed hybrid semantic similarity algorithm tf israel+defense+forces against four strong baseline task predict quiz difficulty level two knowledge based similarity measure use rdf dataset i.e. grammar%23NPR wordnet based measure denote as grammar%23NPR two textbased measure use textual dataset i.e. cosine similarity traditional tf israel+defense+forces
observe from table text-based similarity measure grammar%23NPR outperform tf israel+defense+forces predict difficult easy cluster whilst have similar performance medium cluster knowledge based measure slightly outperform text-based measure about 3%25 overall also found that compare text-based measure knowledge based measure i.e. grammar%23NPR grammar%23NPR give much good performance predict easy cluster i.e. 30%25 high but be inferior difficult medium cluster prediction our tf israel+defense+forces algorithm outperform all four strong baseline three class scale availability grammar%23NPR generation edit by adv. visualization grammar%23NPR x adv. visualization grammar%23NPR x x graph visualization visualization grammar%23NPR x x x graph visualization adv. visualization grammar%23NPR x x x list x x x x list adv. visualization visualization exploration query tool provide online demo related work several researcher attempt support user source visualization browse definition complex query allow fancy visualization result table contain comparison different tool base on visualization grammar%23NPR
show table distinguish between two major group tool that focus on provide overall overview whole structure dataset tool tool provide just instance level view dataset supply query functionality
first group find visualization grammar%23NPR tool tool aim provide user high level analysis dataset
visualization be prototype base on allow build analysis transformation visualization grammar%23NPR automatically provide group statistical+analysis content dataset but not foresee any possibility
second group tool be able provide visual functionality advanced visualization query result but their focus be limited instance level allow visualize instance select through keyword search use powerful tabular view view permit user explore neighborhood this instance grammar%23NPR provide visually appealing way explore information associate instance use graph visualization also grammar%23NPR use same strategy exploration graph visualization but this case each node be class contain list instance user link new node class as if user user build visual query grammar%23NPR implement fascinating approach grammar%23NPR query compose as if user compose natural+language request dataset
allow incrementally build grammar%23NPR query by extend step+by+step main issue this approach be that user require have good knowledge semantic+web technology understand schema source define grammar%23NPR query query retrieve interesting information
report majority tool data visualization require user manually explore dataset be not able provide synthetic schema data contain single source grammar%23NPR differ from tool describe above since provide synthetic representation source schema user use build visual query however grammar%23NPR have some limitation be not able perform keyword query moreover be large area improvement result visualization grammar%23NPR variety tool handle linked data select able connect grammar%23NPR endpoint comparison report table not intend be exhaustive
grammar%23NPR architecture query
consist four distinct component each responsible specific activity name index extraction generation visualization query orchestration
component interact in+order+to produce visual i.e. high-level representation source provide user translate visual query that user compose grammar%23NPR query retrieve result interaction illustrate figure easy reuse all contents extract process store grammar%23NPR grammar%23NPR document database since allow flexible representation index
index extraction rdf graph rdf owl triple use define vocabulary ontology describe intensional knowledge while instance their datatype object properties compose extensional knowledge figure example rdf graph represent source display intensional knowledge convey triple show on top figure while on bottom have triple triple describe three instance compose extensional knowledge
extraction process take as input url grammar%23NPR end- figure example rdf graph partitioning between intensional extensional knowledge
generate set query able extract set index from extensional knowledge extensional group statistical index index compose by set couple c p where c be class p be property south+carolina grammar%23NPR contain object properties p their domain class c
grammar%23NPR grammar%23NPR literal contain datatype properties p their domain class c
grammar%23NPR grammar%23NPR contain object property p their range class c
process also inspect number time each index appear dataset information store together+with each index since use generate table list index extract from extensional knowledge example figure
value class ex sector foaf person foaf organization south+carolina foaf organization ex ceo foaf organization ex sector grammar%23NPR foaf person foaf grammar%23NPR foaf person foaf grammar%23NPR foaf organization ex dbpedia fax ex sector dc title foaf organization ex activity foaf organization dbpedia fax grammar%23NPR ex sector ex sector table class index extract from extensional knowledge source depict figure generation source create by exploit information contain index describe previous section number instance each class number time index appear dataset exploit in+order+to discover how class connect extensional knowledge thus be schema dataset infer from distribution grammar%23NPR
detail grammar%23NPR approach found here http dbgroup.unimo.it%2Flodex_model%2Flodex definition s derive from rdf dataset be pseudograph s c p s o m l l count where c contain set c where c be class rdf dataset
element c represent node pseudograph
p contain properties between class rdf dataset
element p represent edge pseudograph
s p c be function assign each property p p source class c c
o p c be function assign each property p p grammar%23NPR c c
contains attribute class rdf dataset
m c be function map each attribute class c c refer
l be finite alphabet available label
l grammar%23NPR l be function assign class property attribute label
count c p n be function assign each property attribute number time appear rdf dataset class number instance class
attribute represents existence datatype property domain class c c m c while property p p represent existence object property p domain grammar%23NPR c range grammar%23NPR c s p grammar%23NPR o p grammar%23NPR figure representation previous example show figure depict
source represent figure white circle represent class c while attribute show gray box finally edge describe property p each element equip numerical+value represent number occurrence number instance
kind formal definition bring several advantage be easily store retrieve from grammar%23NPR store triplestore involve well+known performance issue issue lead worsen performance grammar%23NPR directly visualize gui grammar%23NPR make possible query building feature
visualization visualization perform by web+application through user interact browse web+server implement while user+interface use different library produce interactive web+application in+particular use polymer manage gui new library library allow design application accord material design principle use use data drive www.w3.org%2Fstandards%2Ftechs%2Fcomponents grammar%23NPR d3js.org%2Ffigure example visual query create on show figure translation grammar%23NPR
create interactive grammar%23NPR allow remote endpoint display result
visualization also present demo
query orchestration query orchestrator manage interaction between gui user compose visual query generation grammar%23NPR query submission remote endpoint
class property attribute select by user visual query participate composition basic grammar%23NPR q
have tree+structure tree+structure overlap graph node tree be class c while leaf be both class c attribute user start compose basic query by select first+class then if user select property this first+class also connected class show query panel edge vertex add tree user also select attribute each class this case tree further enrich edge leaf
query orchestrator translate basic query into grammar%23NPR query through compiler compiler exploit iterative algorithm algorithm traverse basic query tree produce grammar%23NPR query query orchestrator be able compile cyclic grammar%23NPR query any length allow use grammar%23NPR operator optional also nest filter order by offset limit
example introduce figure show simple query build on figure this query compose by select class foaf organization attribute ex activity property ex sector selection this property automatically result selection grammar%23NPR ex sector then also add attribute dc title from this graphical query query orchestrator module generate grammar%23NPR query show bottom part figure
use grammar%23NPR scenario here refer hypothetical use case involve company clean energy+sector company have own product service attempt discover new information on renewable+energy country where locate be very likely look on portal like grammar%23NPR company detect linked clean grammar%23NPR dev.data2000.no%2Fsgvizler formal definition q be out scope this paper found at dbgroup.unimo.it%2Flodex_model%2Flodex%23x1-70003.1 energy data grammar%23NPR this dataset compose triple describe as comprehensive set linked clean+energy data
use grammar%23NPR structure dataset reveal easily browse see figure at+a+glance user have intuition all main class node graph connection among arc besides number instance define each class reflect dimension node focus on color node user understand class define by provider source take from external vocabulary this case see that class definition acquire from grammar%23NPR geonames.org grammar%23NPR use legend fig sect by position mouse on node more information about class show
depict figure sect b source collect organization each organization describe by some attribute sect d together+with average number time each attribute use by instance class for+example not all instance have zip+code whereas have more+than one name moreover class link by some properties sect c by navigate schema user also discover that each organization be link roughly sector but then each sector sector total link organization
user select root node start build visual query organization figure now user add some attribute current class by click buttons on left attribute name m mandatory o optional figure user select optional attribute name abbreviation street class organization user also add other class link current class through specific property by click button on left property property panel sect c figure user add mandatory class properties grammar%23NPR feature sector sector user look+at visual query build fig. sect e use in+order+to focus on different component query add other attribute property class at this point user generate query click grammar%23NPR button button bring user query refinement panel figure
refinement panel fig. user visualize grammar%23NPR query f f generate user manually modify
also choose visualize directly result query by select result tab enable automatic compiler e modify query by use interface on top b c d visualize result that change accord refinement any change query compile automatically send endpoint in+particular user can add remove filter condition on attribute contain query b modify optionality attribute class remove from query c remove pagination result modify page size d insert remove order condition
evaluation propose three kind evaluation grammar%23NPR first analyze portability grammar%23NPR approach then evaluate level expressiveness grammar%23NPR query generate by grammar%23NPR finally provide result usability evaluation perform anonymous user deep evaluation performance process find
data.reegle.info grammar%23NPR visual summary this source be also available at dbgroup
example visual query on source figure distribution micro task execution+time group graph size
grammar%23ENT reachable dataset grammar%23NPR compatible extraction complete table number dataset use perform portability evaluation portability grammar%23NPR endpoint grammar%23NPR design be tool able work each dataset provide grammar%23NPR endpoint thus use complete list grammar%23NPR endpoint contain grammar%23NPR as test set
report number dataset examine dataset be online when perform test process use subset grammar%23NPR operator extract index so just dataset be compatible another well know issue be bad performance some grammar%23NPR endpoint this reason number endpoint be able generate grammar%23NPR decrease remain good result because obtain grammar%23NPR from 61%25 reachable endpoint
extend this portability evaluation gui grammar%23NPR inspect two aspect success failure communication endpoint clarity graph representation grammar%23NPR
result report consult online at dbgroup.unimo.it execute preliminary usability test our laboratory use student find+out how many size graph affect clarity ask student individuate specific node graph different size node measure time take each task provide student task each task each graph size result show figure as see finding time increase almost linearly when dimension graph be less+than node this reason decide not consider dataset have more+than node
number dataset be represent 21%25 total possible solution this issue discuss as future work section
remaining endpoint be not online when test perform re turned user+interface non-standard response grammar%23NPR web+application make request endpoint contain query require response encode through grammar%23NPR grammar%23NPR padding since some endpoint our case be not able encode grammar%23NPR response reply non-standard response finally endpoint almost 60%25 grammar%23NPR successfully pass test
grammar%23NPR expressiveness evaluate level expressiveness query generate inspect how query compose grammar%23NPR benchmark generate use grammar%23NPR set query form by query usually use explore new dataset grammar%23NPR be able generate query propose by benchmark good result take consideration that user without any knowledge grammar%23NPR be able generate grammar%23NPR four excluded query contain grammar%23NPR operator not support by our tool departamento+de+la+union construct grammar%23NPR grammar%23NPR be able generate all query involve any type join filter operation except+for cyclic query indeed grammar%23NPR compiler be able automatically translate basic query structure be tree
usability evaluation grammar%23NPR browse dataset use demo grammar%23NPR available at dbgroup.unimo.it%2Flodex2figure example translation visual query figure into corresponding grammar%23NPR query
section summarize result evaluation perform as online grammar%23NPR compile by anonymous user among user involve enrol from it community be bachelor student divide survey two distinct part first aim verify if graph visualization grammar%23NPR be clear represent structure dataset second part intend prove if visual query panel be powerful adequate way generate grammar%23NPR query survey collect result sparse set user age between years fig. different semantic+web technology skill as show figure be ideal scenario prove effectiveness tool on user different background+knowledge use different dataset survey grammar%23NPR grammar%23NPR pathway database model grammar%23NPR link open open data publish by grammar%23NPR link open data about every
methodology survey enclose short tutorial contain description grammar%23NPR short video where functionality query generation be grammar%23NPR two part compose by microtasks design evaluate effectiveness grammar%23NPR address two main goal
summary browsing functionality propose two anonymous grammar%23NPR generate from two dataset task ask user perform list table
generation functionality ask user generate different query from natural+language request request list table from grammar%23NPR grammar%23NPR
ask compile sus questionnaire reply usability questionnaire in+particular ask score on scale grammar%23NPR survey compile at this url goo.gl%2Fforms%2Ffrsrwklsq4 grammar%23NPR datahub.io%2Fdataset%2Fbio2rdf-inoh grammar%23NPR datahub.io%2Fdataset%2Flinked-open-aalto-data-service grammar%23NPR datahub.io%2Fdataset%2Fnobelprizes grammar%23NPR tutorial be accessible at dbgroup.unimo.it%2Flodexguide.html find+out topic each dataset find+out class large number ofinstances find+out class connect given classchosen by find+out most used attribute classchosen by return all different category return table contain list winner order by name winner table contain date+of+birth winner
grammar%23NPR find award file relate awardof find organization organization win grammar%23NPR grammar%23ENT grammar%23NPR table task query use grammar%23NPR evaluation corresponding dataset
following sentence find schema summary be easy browse permit have overview about structure dataset visualization schema summary be clear
second part ask question grammar%23NPR query generation feature overall tool how evaluate your knowledge about grammar%23NPR if already write grammar%23NPR query how find use grammar%23NPR compare manually writing grammar%23NPR query any comment be good bad unexpected difficult
quantitative evaluation evaluate correctness answer provide by user task list table
summary browsing functionality task belong this section obtain accuracy 91%25 table ask complete task without query dataset but just as api url site url grammar%23NPR not accessible some period+of+time grammar%23NPR grammar%23NPR regularly run bot call read list registered wiki filter+out wiki wiki be inactive not accessible then each wiki site grammar%23NPR make call wiki api collect general site information site statistics like number page page edit page editor also collect some basic semantic grammar%23NPR statistics this information represent use semantic property mention above provenance bee be bot populate content provenance bee extend grammar%23NPR collect timestamp editor information all revision all page concept property give list semantic wiki get from grammar%23NPR provenance bee access each wiki get following aggregate information through api number property number declared properties number used property number page number concept number assertion number day between first last wiki edit number day since last wiki edit represent as semantic property that wiki wiki allow detailed query about their content provenance bee gather detailed data about semantic content wiki each concept property assertion at each point+in+time contribution each user content edit make by each user at each point+in+time data store sql database provenance bee aggregate detailed data generate following statistics about wiki number page editor number property editor number concept editor number page edit number property edit number concept edit represent as semantic property each wiki provenance bee aggregate data store following metrics daily number page edit daily number property edit daily number concept edit provenance bee also calculate count make wiki more comparable proportion total page edit that make by given editor proportion total property edit that make by given editor provenance bee also collect bot blocked user information in+order+to filter+out spam data many wiki use bot populate their content typically make assertion base on already defined properties help provenance bee keep track human editor extract more accurate information about their edit provenance bee extract data continuously save historical data so data only extract once programmatic access provenance bee data provenance bee store all data extract as semantic content provenance summary each wiki assert as semantic property wiki site query use all data store rdf therefore data query wiki content generate dynamically from data through query generate table table display within wiki page when query embed wiki page for+example following semantic grammar%23NPR ask query return table contain page property concept count all wiki category website provenance info true have id have prov page count have prov property count prov concept count data also access programmatically via grammar%23NPR api browse provenance bee data interface show aggregate data all wiki such as total amount properties concept as+well+as total amount edit interface also show data wiki include grammar%23NPR information coverage provenance bee data provenance bee access list wiki from grammar%23NPR select be semantic media site in+order+to make own list wiki some wiki be very large order millions page access detailed history data all page one+by+one via api be not very practical query manage track as wiki connection often time out given query reissue if interrupt in+addition wiki sometimes disable access when hit by so+many query otherwise their performance affect therefore access grammar%23NPR overall statistics semantic wiki track by grammar%23ENT grammar%23ENT monitor site 594+591 total number page 2%2C147%2C483%2C647 total number properties 22%2C467 21%2C323 total number total page edit 2%2C771%2C395 2%2C293%2C975 total property edit 45%2C501 43%2C540 total concept edit 820+516 subset wiki manageable size currently select wiki less+than 5%2C000 page total wiki design more scalable approach extract data from large wiki analyze provenance bee grammar%23NPR this section give overview data currently available as writing this paper provenance bee bot continue gather additional data about new wiki as become available as+well+as increment data about wiki already track as mention earlier track wiki wiki have less+than 5%2C000 page at time write this article be such wiki however require login access api so not collect their data currently track total wiki table show summary statistics all semantic wiki grammar%23ENT grammar%23ENT this table dynamically generate available front+page note that content continue grow particularly amount semantic properties concept note that amount properties create be over 22%2C000 be quite large+number show that this feature semantic wiki heavily use on+the+other+hand be very limited use concept wiki since only concept define total finding be consistent finding our earlier work grammar%23NPH grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPH et al grammar%23NPR now apply large number wiki although our original study include less wiki include wiki large size however be 84%25 increase amount concept create just one year same amount wiki so creation concept appear be on rise figure show data collect particular semantic wiki graph as+well+as summary statistics about wiki pointer grammar%23NPR in+turn offer pointer wiki wiki as+well+as api offer every wiki own page figure show aggregate data all semantic wiki track by top graph show growth property concept contain all site combine over time concept show blue left y-axis properties show green right y-axis bottom graph show number property edit concept edit over time all site combine concept show blue left y-axis properties show green right y-axis rest this section present high-level analysis data wiki note that analysis focus on wiki wiki have less+than 5%2C000 page since be wiki currently target by while prior work grammar%23NPH grammar%23ENT analyze wiki but be all size therefore our data be not comparable because be about different though intersect set wiki concept editor vast majority wiki our sample 92%25 have no concept define wiki have number concepts range between mean about only wiki this sample have multiple concept editor wiki have only editor remaining have concept edit only two wiki have over edit wiki have between concept edit compare wiki last analysis property definition find that only total wiki have more+than property define only 28%25 wiki have no semantic property define contrast high number wiki have no concept define also look+at proportion semantic property define wiki over total amount page range value vary between mean standard+deviation most wiki have value less+than support notion that some page have no property define several wiki have value greater than indicate that many page include several properties be significant outlier value property editor another important aspect semantic use wiki be number contributor create change semantic property only wiki have more+than property creator be wiki our sample at+least two+dozen distinct property editor property important aspect semantic use wiki be number semantic property edit wiki have more+than property edit be wiki wiki have no ongoing property edit on their aggregate statistics about all semantic grammar%23NPR figure data particular semantic grammar%23NPR show lifespan semantic wiki our sample be highly variable range from day over years semantic wiki our sample have expected lifespan over years on+average wiki wiki be dormant be about day maximum almost years figure show lifespan data logarithmic+scale figure show number page number page editor number page edit against lifespan generally correlate increase lifespan figure show number properties number property editor number property edit versus lifespan generally correlate increase lifespan figure show page edit activity property editing activity respectively comparison purpose logarithmic+scale figure show page editor against total page page edit total page against page edit figure show corresponding data semantic property property editor against total properties property edit total properties against property edit while great amount total page total edit seem correlate longer lifespan wiki same trend be not case semantic property wiki small amount properties editor property edit have comparable lifespan larger wiki also note amount property editor wiki not correlate lifespan figure show that amount page editor not correlate amount page page edit figure show similar situation property editor although be much fewer number property editor than page editor semantic wiki tend have few property editor consistent grammar%23NPH grammar%23ENT study but make widely ranging amount edit different wiki mean that large amount semantic content create by few editor both wiki page semantic property be apparent that growth content total page total properties strongly correlate growth edit activity page edit property editor discussion although only analyze semantic wiki wiki be small size some interesting trend highlight from data report this paper semantic wiki community continue grow semantic content amount new concept property create amount edit exist continue grow semantic wiki community create more concepts while there be increase number property create number concepts create almost double most wiki have small number property concept editor majority semantic wiki have few people edit semantic structure wiki lifespan wiki not appear correlate amount semantic content see no evidence that community community adopt semantic wiki have short lifespan contributor contributor create semantic property add large amount content although most semantic wiki have very few property editor be responsible large amount edit properties semantic content appear evolve just+as rest content wiki be strong correlation amount properties amount property edit be also strong correlation between amount page amount page edit as+well so properties appear evolve as rest content conclusion design track provenance semantic content site at+the+moment track provenance terms what user contribute content over time collect content almost wiki 5%2C000 page less report on continuous growth semantic content wiki since start track grammar%23ENT also detect increased use concept significant activity by property editor even where be few co- evolution edit regular page property in+the+future plan extract more fine-grained provenance detail from original semantic grammar%23NPR site provenance information be hard access require change how platform track update currently analyze behavior more detail try understand reason small number user user make property definition be result use enforcement restrictive editing policy be possible that additional user involve creation property if be facility wiki detect resolve conflict collaboratively also plan carry+out analysis analysis focus on evolution wiki over time lead good understanding how wiki community form evolve how characteristic community behavior influence their lifespan evolution acknowledgment gratefully acknowledge support from us national+science+foundation grant grammar%23NPR reference grammar%23NPH et al grammar%23ENT grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH semantic wiki approach application perspective lecture note computer+science reasoning vol. grammar%23ENT grammar%23NPH grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPH analysis knowledge collect from volunteer contributor proceedings on artificial+intelligence grammar%23NPR grammar%23NPH grammar%23ENT grammar%23NPH handbook human computation grammar%23NPH springer grammar%23NPH grammar%23NPH grammar%23ENT grammar%23NPH y. grammar%23NPH v knowledge capture wild perspective from semantic community proceedings on knowledge capture grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH initial analysis semantic wiki proceedings on grammar%23NPR grammar%23NPH et al grammar%23NPR grammar%23NPH y. grammar%23NPH a. grammar%23NPH grammar%23NPH grammar%23NPH l. grammar%23NPH v. grammar%23NPH r democratization semantic property analysis semantic wiki proceedings on semantic computing grammar%23NPR grammar%23ENT grammar%23NPH coordination collective intelligence role team structure task interdependence proceedings 27th on human factor chi grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23ENT grammar%23NPH grammar%23NPH governance social+media case+study promotion process proceedings on weblog social+media figure page page editor page edit versus life+span wiki figure property property editor property edit versus life+span sift key point ansar+al-islam current next frame match key point between current next frame calulate center+of+gravity key point ansar+al-islam eliminate match distance big than calulate vector between center+of+gravity key point ansar+al-islam calulate delta vector from vector adjust ansar+al-islam next frame by delta vector actively ask user interaction interactive adjust ansar+al-islam next frame match false input current frame incl. ansar+al-islam next frame incl. ansar+al-islam figure activity diagram interactive semiautomatic ansar+al-islam fitting repeat until all ansar+al-islam adjust
on each frame thus ansar+al-islam follow linear movement real+object use algorithm extract z key point f1 grammar%23NPR grammar%23NPR fz within grammar%23NPR grammar%23NPR
that computing sift key point on whole image increase processing+time such that human computer cooperation become infeasible since waiting time user raise unacceptable level
florida+keys point fi calculate current f1 next frame f2 highlight white circle figure
assumption rotation scaling object within ansar+al-islam be small between successive frame florida+keys point from current frame f1 match next frame f2 use grammar%23NPR algorithm base on matching result grammar%23NPR m2 mz all key point grammar%23NPR distance big than are eliminate in+case more than key point f1 f2 leave key point elimination center+of+gravity remaining florida+keys point compact+flash g fg f1 f2 calculate show as blue+point figure then center+of+gravity g ag ansar+al-islam current next frame compute figure mark by gray point next vector v compact+flash between center+of+gravity florida+keys point center+of+gravity grammar%23NPR determine if rotation scaling be small vector v current next frame be approximately assumption center+of+gravity florida+keys point compact+flash match linear movement object use adjust ansar+al-islam therefore new center+of+gravity next frame calculate reference vector v current frame new center mark as red point figure difference grammar%23NPR affine+transformation ansar+al-islam use homogeneous+coordinates perform thus ansar+al-islam transform linear motion ansar+al-islam take+into+account
show figure b in+case less+than florida+keys point f1 f2 leave elimination algorithm actively ask user interaction user now adjust polygon ansar+al-islam three intuitive metaphor describe section continue interactive semi-automatic ansar+al-islam fit process possible reason have less+than florida+keys point left be that size ansar+al-islam be too small ansar+al-islam occlude ansar+al-islam mainly contain area object ansar+al-islam change between frame ansar+al-islam fitting continue close cooperation user until all ansar+al-islam detect that not compute automatically cf. grammar%23NPR
automatic ansar+al-islam fit automatic ansar+al-islam fitting employ almost same algorithm as semi-automatic ansar+al-islam fit as illustrate activity diagram figure difference be that in+case less+than florida+keys point leave elimination next frame remain unchanged process continue
idea this process block be that perform interactive semi-automatic ansar+al-islam fitting perform once thus automatically computable ansar+al-islam detect on that basis automatic ansar+al-islam fitting increase accuracy result iteration by iteration cf
because all difficult case resolve cooperation user
ansar+al-islam verification correction intuitive user+interface enable user verify correct result every stage process currently three metaphor be available relocation whole ansar+al-islam by click inside ansar+al-islam drag ansar+al-islam designated area cf. grammar%23NPR ii adjustment vertices single click then drag adding vertices double click between two existing vertices cf. grammar%23NPR iii complete re-creation ansar+al-islam by delete vertices create new vertices by double clicking cf. grammar%23NPR
data export at any time user trigger data export cf. grammar%23NPR current prototype only export ansar+al-islam xml+format valid grammar%23NPR schema tool xml include polygon annotation all ansar+al-islam all frame
grammar%23NPR test performance grammar%23NPR evaluate on car dataset grammar%23NPR pursuit three more dataset annotation already exist test reliability process block illustrate figure including semi-automatic ansar+al-islam fitting based on unknown dataset
test highly dynamic boundary choose dialog because face boundary exhibit desired rapid change further use frame grammar%23NPR key point automatic+adjustment ansar+al-islam b key point actively request interaction figure example used sift key point ansar+al-islam fitting on left ansar+al-islam current frame on right ansar+al-islam next frame white circle sift key point white+line grammar%23NPR match key point blue+point center+of+gravity key point compact+flash confederate+army point center+of+gravity red point new center+of+gravity
video road dataset choose last two video be able compare our grammar%23NPR grammar%23NPR tool as example figure show one frame every dataset
test series same object be use grammar%23NPR as existing annotation give idea how+long annotation process use grammar%23NPR needs processing+time determine cf. grammar%23NPR
result discussion our interactive semi-automatic tool grammar%23NPR polygon based object annotation segmentation work well on different annotation scenario compare annotation time grammar%23NPR annotation time grammar%23NPR total annotation use grammar%23NPR take grammar%23NPR grammar%23NPR long than use grammar%23NPR but be unsurprising since polygon shaped ansar+al-islam grammar%23NPR contain much+more information be more accurate than rectangular ansar+al-islam correspondingly adjust polygon shaped ansar+al-islam take more time
interactive semi-automatic ansar+al-islam fit linear ansar+al-islam tracking be possible assumption that object ansar+al-islam not rotate change+shape strongly overcome restriction extend activity diagram figure feature feature detect deformation rotation thus affine+transformation ansar+al-islam improve substantially from usability point+of+view time+line implement three reason firstly improve navigation video+file secondly arrange ansar+al-islam such occlude ansar+al-islam be behind other ansar+al-islam thirdly describe relations between ansar+al-islam
question whether polygon annotation be worth trouble answer clear yes polygon describe boundary much+more precisely than rectangle
rectangle be sufficient mind that rectangular area be easy process subsequent method easily derive e.g. analysis gaze data prototype version grammar%23NPR be grammar%23NPR ubuntu-world+forum+of+civil+society+networks mac+os
ikw.uos.de cv project grammar%23NPR acknowledgment this work fund by grammar%23NPR as part scalable visual analytics priority program sendero+luminoso grammar%23ENT
reference discrete geometric shape matching interpolation approximation survey technical report handbook computational geometry
interpolation compact operator multidimensional case proceedings 371+400
grammar%23NPH survey semantic image video annotation tool lecture note computer+science page 196+239
tool technique video performance evaluation international+club+for+peace+research
dataset challenge performance evaluation tracking surveillance grammar%23ENT
guarantee intersection free polygon morphing computer graphics grammar%23ENT
grammar%23NPH grammar%23NPH benchmark data evaluate visualization analysis technique eye+tracking video stimulus workshop on beyond time error novel evaluation method visualization grammar%23NPR
distinctive image feature from scale invariant keypoints 91+110 grammar%23ENT
fast approximate nearest+neighbor automatic algorithm configuration
on computer+vision imaging computer+graphics theory application grammar%23NPR
multimedia knowledge social media+analytics laboratory video image annotation tool multimedia knowledge social+media analytics laboratory mklab.iti.gr%2Fproject%2Fvia grammar%23ENT
grammar%23NPR collaborative video indexing annotation discussion system broadband+network workshop on knowledge markup semantic annotation grammar%23NPR page
grammar%23NPH visual analytics video information+technology
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH h. su grammar%23NPR semi-automatic ground truth annotation crowd scene on multimedia expo workshop grammar%23NPR grammar%23ENT
statistics http www.youtube.com%2Fyt%2Fpress%2Fstatistics.html grammar%23ENT grammar%23NPR knowledge+base expo+2015+milan enabling visitor explore city grammar%23NPH grammar%23NPH giuseppe.rizzo%40eurecom.fr grammar%23NPH ocorcho%40fi.upm.es grammar%23NPH grammar%23NPH raphael.troncy%40eurecom.fr grammar%23NPH grammar%23NPH julien.plu%40eurecom.fr grammar%23NPH jcballesteros%40localidata.com grammar%23NPH grammar%23NPH ahmad.assaf%40eurecom.fr abstract this paper present grammar%23NPR knowledge+base knowledge+base collect harmonize description event place transportation+facility user-generated data such as review city expo site this knowledge+base use by set mobile application guide expo+2015+milan visitor city exhibit allow find place satellite event transportation+facility around as grammar%23ENT knowledge+base contain unique events unique place review transportation+facility collect from several static near real+time local global data provider including expo+2015+milan official service numerous social+media platform ontology use as backbone structure knowledge+base follow rigorous development method where design principle generally be re-use existing ontology when exist
think that lesson learn from this development be useful similar endeavor other city large events around world similar ecosystem data provisioning service
knowledge+base data+integration data reconciliation expo+2015+milan smart city grammar%23NPR introduction our information society often refer data as new oil linked open data cloud legion+of+doom capture this opportunity collect vast amount heterogeneous knowledge+base grow number density link every year cover different topical domain such as encyclopedic medical governmental environmental among knowledge+base see as structured permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816944
world be machine processable enclose both data semantics data instance nowadays be essential building intelligent system able take action accord prior knowledge at their disposal crucial link augment human user experience particular task while legion+of+doom depict flourishing scenario available knowledge+base one at our disposal only shallowly cover topical domain smart grammar%23NPR as need context our knowledge+base development grammar%23NPR recent past public administration body open+up action line release open data in+order+to allow data reuse commercial commercial purpose as+well+as increase transparency towards citizen unfortunately such data be not always semantically interoperable public administration even dataset within same organization data own sometimes release by private+company result into data silo specific case city such open data typically contain statistical information about city e.g. gdp population+census flooding alert election+result energy consumption parking slot park administration personnel but more rarely point+of+interest hotel bike station tram metro train+station museum tourism attraction restaurant shop even less event give statistical nature such data live component city life often neglect be up private+company cover this gap nevertheless both public administration body private+company have difficulty elaborate aggregate data provide new service even if have strong relevance improve citizen quality+of+life service give data heterogeneity limited access
proposal build integrated knowledge+base city by combine open private data including data verification reconciliation validation be not new
grammar%23NPR knowledge model city ontology be output expert study over smart city dataset focus on create comprehensive ontology ontology answer need community become grammar%23NPR linkeddatacatalog.dws.informatik.unimannheim.de%2Fstate grammar%23NPR www.3cixty.comdard ontological work carry+out grammar%23NPR knowledge+base simply grammar%23NPR kb overlap grammar%23NPR partially main difference be choice reuse existing ontology instead create new one for+example grammar%23NPR author create class grammar%23NPR grammar%23NPR represent point+of+interest while our knowledge+base simply adopt dul place rationale this choice stand longer maintain largely use class class at loss subtlety perfectly fit our scenario also use catalogue dataset ontology grammar%23NPR as+well+as other source that explain section reuse knowledge resource largely debate community as immediate consequence optimize cost quality turn city data ready consume by developer be goal grammar%23NPR project offer suite uniform api access numerous data+directory heterogeneity then address multitude ad-hoc tool collect data about citizen participation mobility tourism similarly our work collect data from different data+source such as so-called grammar%23NPR service digital ecosystem service tailor expo+2015+milan from social+media platform such as foursquare yelp
collected data then harmonize accord proposed data+model populate grammar%23NPR kb kb act as data marketplace developer be accessible at 3cixty.eurecom.fr%2Fsparql contain link external resource on cover data about past upcoming events happen that illustrate by media photo video place+of+interest hotel restaurant bar theater sight have review transportation+facility bus metro train bike station as+well+as information about estimated time take reach one place from another using public+transportation overall grammar%23NPR kb contain as grammar%23ENT events artist place transportation+facility photo review contribute by user
remainder this paper organize as+follows section present overview ontology use model grammar%23NPR kb section describe expo+2015+milan use case list data+source data+collection mechanism approach follow reconciliation conclude discussion outlook on future activity section
yet another ontology grammar%23NPR data model our ontology design principle focus on optimize coverage terminology context city exploration each entity model look+for existing knowledge resource keyword search grammar%23NPR grammar%23NPR grammar%23NPR while selection criteria be popularity properties base on grammar%23NPR grammar%23NPR www.disit.org%2Fkm4city%2Fschema
www.citysdk.eu grammar%23NPR www.e015.expo2015.org grammar%23NPR lov.okfn.org%2Fdataset%2Flov grammar%23NPR swoogle.umbc.edu grammar%23NPR watson.kmi.open.ac.uk%2Fwatsonwui grammar%23NPR smartcity.linkeddata.es age data favoring schema.org when suitable establish rigid search mechanism where two domain expert analyze knowledge resource resource result from search once consensus reach ontology take add grammar%23NPR data+model data+model therefore consist constellation existing ontology 3cixty.eurecom.fr%2Fontology also publish one additional ontology use model time+travel distance documentation about complete data+model be available at 3cixty.eurecom.fr%2Fdocumentation
detail re-use some concept property from following ontology dul schema dc lode geo transit few additional class properties create describe travel distance define origin distance travel+time nearest metro+station bike data+model design present along example documentation
grammar%23NPR kb population expo+2015+milan grammar%23ENT be international exhibit start on grammar%23ENT welcome up 20%2C000%2C000 visitor prepare knowledge+base enable exploration by visitor creation knowledge+base go through selection prominent data+source timely automatic process data+collection de- duplication solution put place offer high data quality knowledge+base be publicly available via grammar%23NPR endpoint at http sparql via mobile user+interface at https%3A%2F%2Fwww.3cixty.com set query available at 3cixty.eurecom.fr%2Fqueries provide example what type question answer by knowledge+base while dashboard dashboard be available at http dashboard develop show some statistics evolution kb
data+source grammar%23NPR kb contain information about events artist place transportation user-generated content such as media review build use three type data+source grammar%23NPR data+service social+media platform expo+2015+milan manually curated data grammar%23NPR service create goal ease sharing data service among different independent entity particular reference data use support expo+2015+milan visitor large portion data available through grammar%23NPR concern info mobility however growing share data now offer be different nature concern information about hotel restaurant event
precisely most interesting category data available through grammar%23NPR be mobility information concerning municipality such as local public+transportation grammar%23NPR www.loa-cnr.it%2Fontologies%2Fdul.owl grammar%23NPR schema.org grammar%23NPR purl.org%2Fdc%2Felements%2F1.1 grammar%23NPR linkedevents.org%2Fontology grammar%23NPR www.w3.org%2F2003%2F01%2Fgeo%2Fwgs84_pos grammar%23NPR vocab.org%2Ftransit%2Fterms grammar%23NPR data.ign.fr%2Fdef%2Ftopo grammar%23NPR ontologyroutes stop timetable bus metro train current status metro bus+line current status bike share station etc. ii information concerning hotel restaurant iii information about cultural events such as concert theater play museum exhibition then decide combine such wealth data content usually available on social+media platform rationale be increase coverage point interest complement description existing event user-generated activity finally our last data+source be about expo newly built physical site official events including happen each national pavilion editorial team compose five people on-site map geographical+area continuously scrape numerous agenda events pavilion
team investigator analyze rank data+source at disposal decide be important select include knowledge+base strict procedure follow by two team maximize objective+function data semantics instance coverage real-time update output such investigation lead survey survey be cross validate by two domain expert expert decide by consensus agile process establish grant continuously update list source current list compose different data+source use populate knowledge+base list below grammar%23NPR data provide mark editorial data event 3cixty.com www.evensi.com eventful.com www.expoincitta.com www.fieramilano.it fondazionearnaldopomodoro.it www.leonardo-ambrosiana.it www.lastfm.com www.teatripermilano.it place dati.comune.milano.it developers.google.com%2Fplaces www.expedia.com www.facebook.com www.foursquare.com www.isnart.it www.vaxita.com www.yelp.com as+well+as event data+source media https%3A%2F%2Fwww.flickr.com event place data+source transportation+facility https%3A%2F%2Fwww.bikemi.com dati.comune.milano.it user related data https%3A%2F%2Fdevelopers.google.com%2Fplaces https%3A%2F%2Fwww.flickr.com https%3A%2F%2Fwww.foursquare.com www.lastfm.com www.yelp.com
grid data sampling data+collection data+collection process use depend on data+source real-time and%2For batch procedure adapt collection data instance over specific geographical extent use grid sampling using as input grid propose grammar%23ENT grid grid have focal+point center compose 10%2C000 square approximately cell have edge 234%2C000%2C000
cell identifier via property grammar%23NPR cell be grammar%23NPR dandelion.eu%2Fdatamine%2Fopen-big-data grammar%23NPR property define namespace data

all generated instance allow indexing knowledge+base speed+up query evaluation times when computing travel distance query
data+collection process take as input source type source name time coverage collection mechanism materialize collected instance accord data+model use describe entity type that data+source offer each instance mark provenance attribution use dc publisher property generate dump serialize turtle syntax each dump attach dedicated named graph ease data+management batch procedure data+collection prepare on weekly basis optimize expensive collection process collect only delta new instance available each data+source handle real-time data data about availability transport+facility hotel+room collection be continuous
data reconciliation address data reconciliation problem by perform automatic instance reconciliation manual category reconciliation rationale have both type reconciliation be improve data quality by remove duplicate as+well+as by combine information from alternative source hence ease exploitation knowledge+base any data consumer in+fact category reconciliation have objective reduce duplicate category when facet browse data while instance reconciliation avoid have exact duplicate same individual both case reconciliation process apply two main topical type entity knowledge+base event place
instance reconciliation different data+source data+source use populate knowledge+base overlap e.g. same hotel exist several data+source concern 0.67%25 event 26.24%25 place give two data+source namely b instance reconciliation process look+at identify data instance instance be similar according their semantics
output have objective generate grammar%23NPR link from two data+source use silk develop our tailored setting grounding on experimental finding discuss hence develop good performing system such context as highlight by achieved result custom setting further tune accord result analysis our benchmark scenario topical class perform pairwise reconciliation all combination without repetition data+source apply eq. place eq. events
sum p labelp p geop p addressp where p p p labelp grammar%23NPR label distance threshold label be lowercased remove any special char stopwords since all place assign corresponding cell map apply geop exact cell distance threshold addressp grammar%23NPR address distance threshold apply same normalization as do label
sum e labele e geoe e timee grammar%23NPR list concentric nature news semantic snapshot knowledge extraction semantic annotation grammar%23NPH redondo%40eurecom.fr grammar%23NPH giuseppe.rizzo%40eurecom.fr grammar%23NPH raphael.troncy%40eurecom.fr abstract enable have access silo-ed information describe news+article often offer multitude viewpoint that once combine provide broad picture story report on news this paper propose approach approach automatically extract representative feature news+item namely name entity from textual content attach video item subtitle from set document from collect use entity expansion technique approach rely on entity expansion generally try collect process facet behind particular news+item but be often too dependent on frequency based function information+retrieval technique thus neglect multi dimensional relationship that establish among entity propose concentric base approach that enable represent context news+item by harmonize into single model representative entity entity extract use information+retrieval natural+language+processing technique core other entity entity get prominent according different dimensions such as informativeness semantic connectivity popularity crust compare our approach baseline by analyze compactness generated summary on existing gold+standard available on result experiment show that our approach converge fast ideal compact news snapshot improvement 36.9%25 over baseline
subject descriptor grammar%23NPR information storage retrieval information+search retrieval retrieval model search process keyword semantic annotation entity expansion news+item introduction permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT palisade new+york+state united+states+of+america c association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815836
single presentation news+item take individually generally fail illustrate complexity event report offer data space where find different content such as citizen based blog journalistic article social+media post team+up generate multi- rich ecosystem complementary news content
intuitive hypothesis hypothesis formulate be that single news+item be often not enough capture complete story report because generally assume viewer have prior knowledge event be thus incomplete biased even partially wrong if interpret in+isolation most common strategy complement description given news+item be enrich original content additional data collect from external source research initiative approach this problem by exploit absolute+frequency entity extract from additional but related document however result large amount unreliable repeated information leave user burden processing this potentially related data build understanding event grammar%23NPH et+al. propose use entity expansion technique over open produce ranked list entity in+order+to generate so called complement initial set detected entity subtitle other itemrelated entity capture from document ranking achieve use set function measure absolute entity frequency collected document expert rule global popularity entity largely inspire by aforementioned work perform experimental critical assessment this method observe that frequency function pure information+retrieval technique technique use during nss generation neglect intrinsic relationship that entity hold in+particular observe that frequency based ranking their hybrid approach be appropriate spot essential inherent entity but fail consider other entity entity be still important understand context news+item due other reason such as interestingness informativeness popularity
this paper recast problem generate nss by exploit harmonize single model different semantic relationship establish between news entity instead tackle problem from pure list base oriented model where all different news related phenomena project into single dimension propose concentric base approach two main layer call core crust this knowledge+representation model better support complex multi dimensional relation es-tablished among entity involve news+item allow formalize distinction between representative entity good characterize essence news+item relevant be potentially interesting because+of different reason link core this graph based knowledge+representation consider multidimensional nature relationship allow focus different desired feature final nss like representativeness compactness final ranking order then delegate final application display data inevitably project rich spectrum relationship among entity describe event into single easy consume dimension
remainder this paper organize as+follows section present some related work section present critical assessment our previous nss approach enable list key idea work present this paper
then state hypothesis section section describe our approach approach lead concentric model creation propose evaluation section finally summarize our main finding outline some future work section
related work need nss feed certain application be already concept investigate some research works prototype like probe benefit user when browse surrounding context newscast
effort underline importance professional journalism proper annotation illustrate news+event project like news study how disambiguate named entity news domain by continuously learning over news stream domain social+network name entity use identify model events detect breaking news entity also exploit video classification when textual information attach video contain temporal reference e.g.
need contextualize news related document already address some research work such as where author describe challenge generate textual summary news+item base on entitycentric process traverse graph generate subgraph contain most relevant entity structural relationship among particular case this paper relationship use as means building human readable summary instead aim create general shareable data+structure like nss be
some case like this contextualization process not base on entity but on other item like other document text snippet however machine+readable representation context news+item still need
context build+up from different keyword directly mention news+item no use external source in+the+middle another difference be existence human intervention during workflow since keyword manually highlight by user
example news contextualization effort find where author aim complement original news document most interesting reaction from different online platform in+order+to identify relevant candidate propose look+at different dimensions including interestingness informativeness opinionatedness popularity motivation implement concentric model approach present this paper actually include need deal different dimensions
best our knowledge only related work news domain carry+out ground power enrich set initial entity by use entity expansion algorithm be former include naive document collection strategy bring related document propose pure frequency based algorithm ranking entity complement by look into grammar%23NPR knowledge+base ensure coherency final list entity via number length path among improve former several direction document+retrieval mechanism semantic annotation creation nss however fail exploit semantic relationship between spotted entity project different relevancy dimensions into single ranking list ignore other important aspect generated context like representativeness compactness
motivation this section summarize different research effort make+for critically assess extend experiment describe
grammar%23NPR nss generation baseline this section try extend strategy describe by further exploit additional relevance indicator indicator miss during our previous study objective improve grammar%23NPR at n. this measure consider different level relevance give more priority item rank at top position since be more likely examine by user some change change bring some improvement over original approach be relevance document obtain from search+engine communications+security+establishment come order so one on top be potentially more relevant therefore related studied news+item assume that entity spot within high ranked document be more important than find less interesting document weight differently when sum+up score frequency function see section this adaptation enable gain 1.8%25 grammar%23NPR over good configuration from original approach
promote subtitle entity entity detect subtitle better consider since explicitly mention video speech therefore be more likely be relevant report by analyze different ratio weighting subtitle entity versus related document combination bring good outcome obtain percentage increase 2.5%25 grammar%23NPR
attempt attempt not improve even slightly reduced grammar%23NPR be confidence similarly relevance confidence score produce by entity annotator use differently ranked entity when accumulate on frequency this new grammar%23NPR www.google.com%2Fcse%2Falldimension bring percentage decrease grammar%23NPR 0.2%25
interpret popularity dimension candidate entity propose by popularity function see section need combine together+with outcome frequency measure provide single ranked list entity
come from both dimensions simply sum in+order+to go step further create function f grammar%23NPR grammar%23NPR linearly transform score produce by popularity function into value inside range frequency function grammar%23NPR before perform addition unfortunately this lead percentage decrease 1.4%25 grammar%23NPR
perform clustering before entity filtering by filter entity first get rid many noisy annotation partially correct result contaminate generation nss be possible proceed other way around run clustering operation over whole set annotation then filter+out cluster accord their entity centroid intuitively clustering phase benefit from partially correct annotation since still balance cluster by become part however result follow this modified workflow less perform than former approach because filtering stage become too aggressive by remove some important entity cluster cluster have low representative centroid observe percentage decrease 0.6%25 grammar%23NPR
experiment conduct situation not bring significant improvement original score deep study result reveal that prioritize certain ranking dimensions quickly bring valid result but also discard relevant entity entity select before complete workflow be too dependent on frequency function pure information+retrieval technique thus neglect semantic relationship present ideal national+security+service news+event in+fact frequency drive ranking their hybrid approach be appropriate spot essential inherent entity entity be inside final+result but fail consider other entity entity model more specific detail about news+item be relevant national+security+service creation particular reason but barely mention related document
thinking outside+the+box give limitation find approach describe try tackle problem from different angle reconsider condition that national+security+service match in+order+to properly consume by other user application
study that state-of-the-art ranking algorithm reach ceiling performance terms grammar%23NPR first question answer be if data retrieve via collection phase still offer some room improvement by look+at further position n ranking generate by good run at grammar%23ENT grammar%23NPR l1 grammar%23NPR grammar%23NPR f3 grammar%23NPR plot score evolution when grammar%23NPR be white+list three news web+site new+york+times al+jazeera l2 be white+list news web+site filter rank function define
big national+security+service curve figure suggest that cumulative gain keep increase at big n value even if gradient not pronounce
grammar%23NPR run grammar%23NPR
gain be measure weight more match top rank in+order+to make this analysis more rank agnostic figure analyze recall r when size national+security+service go till position slope be now steep clearly reflect that more relevant entity still found at low position ranking potentially move top
grammar%23NPR grammar%23NPR
check how good grammar%23NPR perform compare other configuration term recall complete set configuration be re- run in+order+to see work best top configuration label from show table
first observe that grammar%23NPR grammar%23NPR in+other+words at further position ranking other strategy bring+up high amount relevant entity even if be not well perform ranking at n additionally popularity dimension dimension get+down cumulative gain score seem bring+up relevant entity at high n value accord configuration four best performing run in+conclusion be room improvement but also need change knowledge+acquisition method behind approach as already explained before pure grammar%23NPR grammar%23NPR run their configuration setting rank by
collection filter ranking result source grammar%23NPR schema.org pop exp l2 grammar%23NPR f3 gaussian x x grammar%23NPR l2 grammar%23NPR f3 grammar%23NPR x grammar%23NPR l2 grammar%23NPR f3 x x grammar%23NPR l2 grammar%23NPR f3 grammar%23NPR x x grammar%23NPR l2 grammar%23NPR f3 gaussian x grammar%23NPR l1 grammar%23NPR f3 gaussian x x grammar%23NPR l2 grammar%23NPR f3 gaussian grammar%23NPR l2 grammar%23NPR f3 grammar%23NPR l1 x retrieval technique be not enough explain relevancy entity particular news+item certain entity be important summarize context news+item while be informative user user just want discover beyond obvious fact dimensions like popularity semantic relatedness not be projected into single ranking dimension instead need equally consider when promote important entity so take+part on national+security+service
change final objective task bring as+many relevant entity as possible inside national+security+service prioritize against be too precise ranking be line idea generate flexible applicationindependent national+security+service national+security+service intend be comprehensive enough contain as+many entity as possible better represent context news+item our outcome aim produce solid semantic representation representation properly fee different prototype tool information needs needs vary
hypothesis this section formally describe problem address main hypothesis formulate in+a+nutshell semantic snapshot news+item national+security+service model follow schema concentric entity layer this kind representation help better reproduce context news+event ease task identify different relevant entity various dimensions
this model consider two main entity layer layer annotate news+item core crust
compose small number key entity be essential identify event entity have high degree representativeness better summarize main fact attach event frequently mention related document be therefore spottable via frequency based function sometimes be too obvious user but be key element describe fact be semantically compact sense that exist numerous semantic relationship between each entity let grammar%23NPH grammar%23NPR endi be bag entity belong related document di et union all grammar%23NPH core g grammar%23NPR grammar%23NPR ec ei ei grammar%23NPH frequency prominence ei core f ei t t grammar%23NPR compact ei ej core s ei ej score score crust compose large number entity describe particular detail news+item grammar%23NPR concentricity news+item fugitive grammar%23NPH apply asylum tie mention some specific related document but be not always spottable via frequency based measure
be not necessarily pairwise related not semantically compact their relevancy instead ground on existence special relations including popularity serendipity etc. between entity core
g grammar%23NPR grammar%23NPR ec ei grammar%23NPH core attach ei crust s ei core s s grammar%23NPR two layer aggregate into single structure in+order+to build so+called news semantic context national+security+service our hypothesis differently than other research work such as this structure be graph entity since relationship establish among element inside national+security+service be more important than their absolute ranking therefore provide data+structure data+structure remain as flexible as possible
grammar%23NPR crust give this new nature characterize national+security+service maximize cumulative gain be not priority this study therefore need define new more ranking agnostic objective+function one possibility measure recall r
this metric not consider different degree entity relevance be available ground truth see section in+order+to exploit this additional information extended recall index r define this measure take+into+account different score relevant entity give more accurate idea about coverage provide by particular national+security+service r national+security+service grammar%23NPR grammar%23NPR grammar%23NPR egti furthermore this research aim tackle problem from big perspective by study wide region entity annotation spectrum not only focus particular national+security+service at n. let define grammar%23NPR as list n entity produce by certain approach associated+press define semantic snapshot grammar%23NPR as n first entity grammar%23NPR
grammar%23NPR grammar%23NPR en n grammar%23NPR n introduce so-called compactness com entity set re give certain function f value v com r f v min national+security+service f national+security+service v this measure help indicate if particular set entity be able produce concise nss while still keep goal f national+security+service v. our objective be produce entity set minimum compactness com. function r use quantify coverage
define different concept dimensions above be result grammar%23NPR grammar%23NPR set n relevant entity about particular news+item generate via our former implementation concentric model approach respectively formulate following hypothesis com grammar%23NPR r r grammar%23NPR com grammar%23NPR r r grammar%23NPR according this hypothesis concentric model approach be able produce more concise clean potentially easy consume new semantic snapshot than related research effort implement unidimensional ranking
approach this section present our proposed approach generate news semantic snapshot base concentric model
workflow compose following step figure execute art entity expansion rank strategy good configuration possible bring top position as+many relevant semantic annotation as possible see grey part on left side label as build concentric model news+item tree different steps generate core crust final national+security+service as depict white part on right side
entity expansion ranking first step consist execute expansion approach present generate list entity grammar%23NPR objective be reduce size spectrum annotation consider while build concentric model therefore relax complexity work over entire set entity take as input metadata that news broadcaster offer about item publish query q h t build where h be video headline t be publication date this query allow collect set event related document d from open over semantic annotation process perform remove html+tag other markup annotation feature space be then reduced each document di represent by bag entity grammar%23NPH grammar%23NPR endi where each entity define as triplet surface form type link filtering process prepare entity cluster apply centroid based algorithm base on strict string similarity over link surface form output this phase grammar%23NPR further process promote named entity entity be highly related underlined event base on entity appearance document popularity peak analysis domain expert rule in+order+to produce ranked list entity grammar%23NPR feed concentric nss generation approach
generation core generation process work over set entity annotation document grammar%23NPR in+order+to identify entity high level representativeness particular event as state our hypothesis exploit frequency prominence principle express definition grammar%23NPR spot candidate in+particular absolute+frequency entity within set document d note as fa ei d grammar%23NPR appearance rate all document fdoc ei d consider accord following formula grammar%23NPR e d fdoc ei d fa ei d fdoc ei d order entity accord grammar%23NPR e d top ranked entity start add core until find one one not semantically connect all other already include this way ensure second condition core generation express semantic compactness in+order+to check if entity ei connect other ej identify existing path between particular knowledge+base kb process detect path enable identify other resource r kb kb materialize such connection intermediate resource promote via dimensions such as popularity rarity be essential component original grammar%23NPR algorithm implementation make use grammar%23NPR coefficient measure dissimilarity assign random+walk based weight weight be able highly rank rare resource guarantee that path between resource promote specific relations other general
be number p path between entity ei ej pathi j be pathi j path length as number link among resource r define similarity function german+joint+support+service as german+joint+support+service ei ej pathi j as state two entity consider wellconnected if german+joint+support+service ei ej s
generation take again as input result grammar%23NPR use different similarity function work certain relevancy dimensions in+order+to detect entity be next core as show definition s ei core
core act like contextual anchor where crust candidate entity attach current approach two function ground on different principle consider semantic relationship between resource knowledge+base via number length path between entity ei crust entity core base on definition german+joint+support+service ei ej define similarity function s kb ei core as sum different similarity between ei ej core s kb ei core german+joint+support+service ei ej ej core number document talk simultaneously about particular entity ei core this function note as grammar%23NPR ei core identify document talk about candidate entity core at+the+same+time while keep+in+mind original volume document contain separately let e be set entity function hitss e number document where all ei e mention grammar%23NPR ei core be directly proportional square number page talk about core candidate entity at+the+same+time hitss ei core figure grammar%23NPR based approach generate news semantic snapshot using named entity expansion inversely proportional number page talk about core hitss core
inversely proportional number page talk about candidate entity alone hitss e if entity already highly mention all+over like example very famous+person volume document mention that entity together+with core be also big enough in+order+to consider
ei core hitss core ei hitss core hitss ei different similarity function help populate crust first top c entity spot via each dimension
generation last step entity come from crust attach core via score produce by similarity function describe above in+order+to generate final national+security+service news+item at this stage result be graph different event related entity evaluate this approach different projection function f g l create
evaluation this section describe experimental setting result concentric model approach against gold+standard re-use dataset compose ranked list named entity that each semantically annotate one video item named entity extract from subtitle video image text contain video article relate subject video event suggest by journalist expert build candidate set entity set present participant via online survey ask rate their level interestingness methodology building this dataset thoroughly describe at http github.com%2Fjluisred%2Fnewsentities where link list entity score video be also available
experimental setting this section explain configuration setting use during execution experiment in+order+to produce concentric model based news annotation grammar%23NPR first important parameter consider be length entity spectrum analyze in+order+to go beyond n used by target position study section experiment configure work over first entity come from previous expansion phase
entity expansion ranking in+order+to identify entity expansion configuration potentially serve as good basis generate national+security+service study value r over complete set runs consider table show top configuration run collection filter ranking result source grammar%23NPR schema.org l2 grammar%23NPR f3 gaussian x x grammar%23NPR l2 grammar%23NPR f3 x x grammar%23NPR l2 grammar%23NPR f3 gaussian x grammar%23NPR grammar%23NPR l2 grammar%23NPR f3 grammar%23NPR x grammar%23NPR l2 grammar%23NPR f3 grammar%23NPR x x grammar%23NPR l1 grammar%23NPR f3 gaussian x x grammar%23NPR l2 google+inc. grammar%23NPR f3 gaussian table expansion run rank by r select first two run table as candidate feed concentric model approach first run use second whitelist f3 filtering one week temporal window gaussian function expert rule popularity this configuration be also top result when ranking by
second run grammar%23NPR use second whitelist f3 filtering one week temporal window absolute+frequency ranking function expert rule popularity this configuration be also third fourth grammar%23NPR
be note that restrict web+site em bedding schema.org markup lead poor performance when want maximize recall named entity
generation entity rank accord frequency based function in+order+to perform clustering operation operation explore frequency entity consider two different strategy grammar%23NPR base on jaro-winkler string distance over surface form grammar%23NPR base on exact string match link
entity core usually express most general upper-level concept concept drive story behind newsitem use grammar%23NPR in+order+to discover relationship between candidate entity via similarity function
use path finding algorithm implement during process filter only n- top semantically well-connected entity two entity consider as properly connected if be at+least path length between german+joint+support+service ei ej t crust generation generate crust two function consider following parameter s kb ei core configure work over grammar%23NPR in+order+to find connectivity between entity core however this general+purpose knowledge+base not work well more fine-grained entity available crust even relax threshold t function german+joint+support+service ei ej also consider path length up empirically detect miss relations among some entity that according+to story tell news+item connect each other future work plan rely other news domain specific dataset potentially contain more link about studied event
grammar%23NPR ei core configure work over instance search+engine where hitss e be number document retrieve in+particular set+up engine no particular site crawl no temporal filtering english+language
discard first similarity function there be only one possible configuration this phase
generation in+order+to project final graphbased structure into list evaluate against ground truth two possible approach take+into+account core crust entity core place at top position result list grammar%23NPR entity crust add just
grammar%23NPR all entity crust add list result grammar%23NPR also calculate grammar%23NPR ei core entity core place right position accord this similarity score
result initially perform specific evaluation core consist calculate precision p entity contain this layer result be close 95%25 mean that great majority core entity be ground truth underling their importance as driving+force build nss news+item
execute concentric model approach different configuration select experimental setting total also consider two baseline produce by traditional entity expansion technique identify by run name grammar%23NPR grammar%23NPR respectively in+addition contemplate existence ideal system able generate same perfect ranking available ground truth in+order+to understand how good potentially get assume r r grammar%23NPR table order result terms compactness com grammar%23NPR r r grammar%23NPR concentric model configuration break+down score by video grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR show final average right column bold
first observe that average compactness concentric model approach be already small than original represent by baseline grammar%23NPR grammar%23NPR all video prove our original hypothesis com grammar%23NPR grammar%23NPR com grammar%23NPR additionally if compare good concentric model run grammar%23NPR com good baseline grammar%23NPR com have as ideal objective compactness grammar%23NPR com report percentage decrease 36.9%25 over good baseline thus get closer ideal smallest possible nss
see more intuitive way how good be evolution r value over whole spectrum plot score from concentric based run grammar%23NPR against baseline grammar%23NPR figure
r grammar%23NPR versus recall grammar%23NPR concentric model approach get fast high value r approximately from n n see how dashed line grammar%23NPR get fast high value r mean potentially produce more representative nss from low n position obtained result
7. conclusion different application tool consume information about news benefit from computation summarize explicitly describe context news+item generate such data+structure not rely exclusively on metadata that particular news+item instead be need rely on complement available information via process call named entity expansion however this step bring numerous relevant entity entity need discard one way promote news related entity be rely on pure information+retrieval method but approach be not suitable spot other relevant entity that link main story accord aspect such as popularity serendipity semantic proximity overcome difficulty be able exploit entity semantic+relation propose concentric based model generate national+security+service this model propose two layer core compose most representative entity entity be well connect between com r f v grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR avg grammar%23NPR south+korea grammar%23NPR core crust south+korea grammar%23NPR grammar%23NPR core crust south+korea grammar%23NPR core crust south+korea grammar%23NPR grammar%23NPR grammar%23NPR grammar%23ENT grammar%23NPR core crust grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR f3 gaussian exp grammar%23NPR grammar%23NPR grammar%23NPR f3 grammar%23NPR exp table compactness concentric model result vs compactness baseline ideal ground truth based result set frequency measure crust crust sometimes include un frequent entity that attach core via particular similarity function in+order+to ensure semantic compactness core look+at existence grammar%23NPR path between each entity pair establish connection between core entity crust observe that general+purpose knowledge+base such as grammar%23NPR be not necessarily ideal due fine grained nature entity crust however other dimensions like presence both core crust entity successfully highlight relationship promote relevant entity experiment terms r over set result produce by concentric model approach reveal significant improvement level compactness new method compare traditional expansion method method allow produce more concise at+the+same+time representative national+security+service
future work include extend amount news+item video current ground truth ii increase length spectrum annotation use feed+up concentric model up complete list annotation be hard due quota restriction some service such as iii be able spot not only degree connectivity between entity crust core but also predicate predicate characterize connection iv study role model track evolution news+event over time where expect that core remain more+or+less stable entity crust vary accord particular moment event
this work partially support by grammar%23NPH within grant number grammar%23NPR by european+union 7th framework programme via project grammar%23NPR ga
reference entity centric summarization text summary graph snippet 23rd on page south+korea
search+engine information+retrieval in+practice
grammar%23NPH grammar%23NPH grammar%23NPH discover meaningful connection between resource data 6th international workshop on linked data on grammar%23NPR
grammar%23NPH
name entity disambiguation news domain application
grammar%23NPH
media fragment named entity video classification first workshop on linked media grammar%23NPR rio+de+janeiro
novel metric information+retrieval semantic+network 3rd international workshop on inductive+reasoning machine+learning semantic+web grammar%23NPR page
grammar%23NPH grammar%23NPR citation ranking bring order
report grammar%23NPH
j. grammar%23NPH grammar%23NPH describe contextualize events 2nd international workshop on social news on snow page 759+764 south+korea
j. grammar%23NPH augment tv newscast via entity expansion 11th page 472+476
j. grammar%23NPH generate semantic grammar%23NPR newscast use entity expansion 15th
grammar%23NPR extraction disambiguation named entity on semantic+web 9th on language resource evaluation grammar%23NPR
grammar%23NPH
past supporting interpretation forgotten story by time aware re- contextualization 8th on data+mining page 339+348
grammar%23NPH
translator automatically news+article 24th association+for+computing+machinery international conference page 247+250
t. grammar%23NPR grammar%23NPH a. m grammar%23NPR automatic selection social+media response news 19th on knowledge discovery data+mining grammar%23NPR page
overview record linkage current research direction census+bureau grammar%23ENT semi-automatic ground truth annotation video interactive tool polygon based object annotation segmentation juschoening%40uos.de pfaion%40uos.de gheidema%40uos.de abstract knowledge extraction from video data challenge due high complexity both spatial temporal domain
truth be crucial evaluation adaptation algorithm new domain unfortunately ground truth annotation be inconvenient time consume common annotation tool mostly rely on simple geometric primitive such as rectangle ellipse here propose novel interactive semi-automatic process process actively ask user input if result automatic annotation appear be incorrect brief review related tool video annotation explain our proposed semi-automatic method grammar%23NPR use prototype implementation grammar%23NPR test on two visual stimulus dataset eye+tracking experiment on two surveillance dataset experimental result usability compare existing annotation tool finally discuss properties opportunity polygon based video annotation
subject descriptor grammar%23NPR information+systems information+search retrieval selection process grammar%23NPR computing methodology vision scene understanding video analysis keyword grind truth annotation polygon base semi-automatic user loop introduction according official press statistics about grammar%23ENT video upload every minute just this platform in+case upload video watch annotate manually in+real+time 18%2C000 operator be necessary illustrate importance automatic knowledge extraction acquisition from video data some case successful detection semantic concept permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816947 such as person building car be already possible current automatic content analysis understanding tool as detection method be rapidly developing manual ground truth annotation become even more important development algorithm training evaluation
architecture video visual analytics computer assist user on two low level reasoning process extraction meaningful artifact assessment situation common annotation method video image data no such technique use
grammar%23NPR try transfer this architecture combine computational power computer high level ability human user by this cooperation between human computer quality result improve significantly annotation speed increase slightly
current freely available video annotation tool usually provide only simple geometric primitive like rectangle ellipse significant improvement video annotation be polygon shaped area another problem current tool be that provide no little support easy concurrent annotation several frame overcome drawback propose our novel interactive polygonbased semi-automatic method grammar%23NPR grammar%23NPR actively ask user interaction if result automatic annotation seem be incorrect grammar%23NPR create annotation every polygon based shape more than three edge good usability provide intuitive easy use interaction metaphor
state+of+the+art grammar%23NPR et+al. review compare several image video annotation tool use several criteria like in- output format metadata type granularity localization expressivity annotation according+to their review seven image annotation tool provide polygon based annotation but contrast only seven video annotation tool provide polygon based annotation namely tool however our own test via turn+out that be not able activate polygon based annotation only rectangular annotation marker be available user+interface in+addition notice via only display clipped region input video if resolution be grammar%23NPR
quite old but still in+use tool ground truth generation be automatic 2d propagation object use speed+up annotation process
properly defined specified xml output for- mat grammar%23NPR schema usage annotation do be easy
semi-automatic ground truth annotation tool grammar%23NPR design rectangular annotation pedestrian semi-automatic process rely on assumption 3d linear motion technically base on orb feature matching thus reduce number manually frame due 3d linear motion assumption grammar%23NPR camera need be stationary
perform annotation real-time collaborative approach pool resource several user
implement this still exotic approach thus multiple user index browse annotate discuss same video sequence simultaneously
grammar%23NPR reflect proposed architecture video visual analytics our interactive annotation segmentation tool grammar%23NPR focus on semi-automatic architecture put user loop as show figure grammar%23NPR consist eight main process block two block eight are obligatory process specific order mark white headline figure remaining process block mark gray headline execute by user any sequence repeat as often as necessary until intended annotation be following selected process block describe in+detail
polygon morphing user need identify only on few frame therefore algorithm estimate position contour ansar+al-islam on intermediate frame keep thing as convenient as possible user use polygon varying number vertices on different frame contour ansar+al-islam be either convex concave be intersection free simplicity hole ansar+al-islam omit thus task be morph two selfintersecting polygon different number vertices in+addition all intermediate polygon also not be selfintersecting since represent contour as+well
be several existing algorithm literature concerning polygon interpolation since existing method seem fit our needs implement new very basic form polygon match test whether semi-automatic approach aid computer+vision be viable
two polygon grammar%23NPR grammar%23NPR b grammar%23NPR grammar%23NPR bm start by separate+out translation component by center polygon on their center+of+gravity in+the+future also like extract rotation component beforehand by+means+of computer+vision cope problem different vertex number introduce additional point polygon every point there will be additional point on contour b vice+versa so all intermediate polygon have n m point every point ai position matching additional point on contour b be point on contour small distance match ai arg ai ai ci where ai be set close point ai on every line+segment grammar%23NPH point ai match bj as+well+as bj match ai will grammar%23NPR video annotation process grammar%23NPR https%3A%2F%2Fikw.uos.de cv publication grammar%23NPR verify correct annotation run grammar%23NPR algorithm export result end seed key frame by manual annotation at+least start end frame automatic sift key point fit fine+adjustment area morphing polygon geometry interpolate 2d translation interactive sift key point fit detect computable area load image sequence start user loop figure overview interactive semiautomatic annotation segmentation process
block white headline be obligatory run grammar%23NPR block gray headline be optional use any order at any time block interactive sift key point fit actively ask user interaction in+case automatically generated annotation appear be incorrect thus computer remain work horse process while close cooperation user allow grammar%23NPR achieve sophisticated result
collect into two new polygon call b represent same shapes as b but now n m point each
point from contour match corresponding point on contour b vice+versa unfortunately there be case where predecessor successor relations violate check order vertices b exchange conflicting vertices correct violation obtain intermediate polygon interpolate linearly between matched point as+well+as along translation vector cf. grammar%23NPR
algorithm be very simple straightforward already work many case especially when two polygon be not completely different their shape rarely be in+practice still be only heuristic approach be problem some polygon where point matching be not possible such that predecessor successor relations preserve case self intersecting polygon appear on intermediate frame time complexity o nm algorithm be rather fast make useful test purpose be aware our approach be still at early stage not be adjustable work without error all situation
interactive semi-automatic ansar+al-islam fit within this process block 2d linearly interpolated polygon shaped adjust fit realwhere e e e labele levenshtein label distance threshold label be lowercased remove any special char stopwords use place all events also attach cell apply geoe exact cell distance threshold timee measure timing distance between two event set threshold distance grammar%23ENT
then use as confidence score sp place se events range
manual curation automatic output result reconciliation lead empirical value list above
evaluate performance this process on gold+standard compose instance each type uniformly sample reconciliation procedure perform accuracy 99.6%25 event instance 90.24%25 place instance
then apply resolution mechanism mechanism attempt prioritize choice instance resolve duplicate depend on contribution data+source each instance collect as define by two expert improve efficiency data consumer decide build two graph where store unique individual namely 3cixty.com%2Fplaces 3cixty.com.events respectively such set obtain as distinct union output resolution mechanism all instance not hold any grammar%23NPR link
category reconciliation reconciling category have objective reduce sparsity use different label same category group
address process by use two category thesauri implement skos as pivot place taxonomy use events
alignment lead by two expert domain establish set link from gathered category use skos grammar%23NPR skos grammar%23NPR automatic process then use identify link accord exact match found category alignment define by expert
discussion offer unique testing scenario our knowledge+base 20%2C000%2C000 people visitor expect this international exhibit large portion be able use kb via grammar%23NPR application application endorse by grammar%23NPR establish weekly mechanism data release knowledge+base update central repository design welcome discussion record issue in+addition team member regularly participate world+wide+web+consortium discussion on topic smart city event modeling spatio temporal representation open annotation as+well+as on more local initiative
bring daily discussion on standard approach model data
kb be output specific need explore new opportunity expo+2015+milan visitor exploit transportation business cultural touristic point interests offer by more personal environmental manner research result data+model engineering solution be ready deploy other context grammar%23NPR developer.foursquare.com%2Fcategorytree grammar%23NPR web+application mobile companion guide be accessible at https%3A%2F%2Fwww.3cixty.com
e.g. city worldwide events minimal effort
approach have two stream data specific city related data provider global social+media platform allow be agile build knowledge+base specifically tailor any other city improve precision instance collect from social+media platform by use specific data+feed provide by city
modular+design make extensible any city world make sustainable since use as basis research commercial+activity start during grammar%23ENT kb also offer user as curated data marketplace this exploitation activity currently under+investigation enable api developer build new visual+interface power on top data+model data resource
investigate how mine knowledge+base automatically generate new knowledge thanks+to rigorous mechanism data+source selection offer data quality further validate by data instance selection rigid procedure rdf-ization mining such data become unexploited value research commercial initiative pilot research line be currently exploring focus on generate automatically geographic fingerprint extent soon offer as another type entity knowledge+base another pilot research line focus on use association mining rule align category category currently perform by+hand
this work partially support by https%3A%2F%2Fwww.eitdigital
author like thank activity partner partner contribute daily discussion shape further refine kb
reference grammar%23NPH grammar%23NPR ontology building versus data harvesting clean smart city service computing
rez
ict roadmap smart city 11th extended semantic+web conference grammar%23NPR
active learning expressive linkage rule use genetic programming semantics science service agent on
mining event connection on social real-time instance matching data+analysis grammar%23NPR semantics science service agent on
grammar%23NPH grammar%23NPH geographic summary from grammar%23NPR data 11th extended semantic+web conference grammar%23NPR
grammar%23NPH reuse ontology on semantic+web feasibility+study data knowledge engineering grammar%23ENT visualization state hopper learning about apr%40informatik.unikiel.de mwi%40informatik.unikiel.de dag%40informatik.unikiel.de fboe%40informatik.unikiel.de grammar%23NPH grammar%23NPR economics a.scherp%40zbw.eu abstract present state hopper interactive+multimedia application application use generate map based educational game prototype system use as map implement student project goal game be move from country country by learn fact about country like population gdp size
movement from one country another country on map generate new insight allow player capture new knowledge about country
educational game interactive+multimedia lod introduction be data+model growing attention community offer data+set from various discipline interactive+multimedia application be popular way visualize publish new content technology combination both aspect into single application be promising idea however today application lod be often neither very interactive do show common properties multimedia
develop state hopper prototypical interactive+multimedia application visualize our application be educational game as enable user explore by discover fact about country this demo paper describe concept game as+well+as development prototype implement by student group target audience our application be be interested get new insight about european country playful manner grammar%23NPR make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816952 ever especially child teenager take+into+account as application allow learn map fact about country interesting playful manner usage lod grant possibility enlarge pool country as+well+as properties extend learning experience new topic as query source from different grammar%23NPR endpoint always latest information about country present user
remainder paper organize as+follows section concept game explain section describe development game prototype section reveals interesting detail about game architecture section discuss related work before conclude
game idea state hopper be single player game one travel from random start country target country on european map as show figure goal be reach target country minimum amount move hop
player choose at+least five properties from pool numeric property extract from linked data+source each country as show figure example property be population size area water gdp gross+domestic+product gini+coefficient represent income distribution high point at start game subset five properties randomly choose random start random target country select during game player acquire knowledge about different country reach target country on shortest path instead be able directly navigate from country country player reach target country by estimate country property at+the+beginning all property value be unknown but reveal when move+around player choose five selected numeric property make move value property either be increased decreased select move every neighbor country take+into+account as next hop destination actual next hop derive from criteria value property hop candidate need differ same manner player choose small large ii country small difference property value current country be next hop figure example state hopper run where player be currently target country be as show by radar plot center player have already quite some knowledge about gini+coefficient gdp country task be decide property choose in+order+to do final move
property selection menu state hopper make hop another country values selected property reveal both origin country destination country for+example figure show screen+shot game situation where player reach current position be grammar%23NPR use game visualize value property as+one see from radar plot screenshot all five property already unlock during game player click on any visible country view country be neighbor visualize by yellow+line as see figure player select property move decide direction value property modify by use buttons for+example if property be population player press mean that next country be one have low population than but still largest population among neighboring country situation depict figure be good choice decrease area property since be neighbor country have area small but closest
certain case hop be not possible e. g. be no country small large value selected property be situation figure when player choose increase grammar%23NPR grammar%23NPR however increase grammar%23NPR be not possible since have high grammar%23NPR neighboring country
already derive from radar plot depicted figure where grammar%23NPR value be at most large position attempted move like example be not possible count as penalty hop player select another option
reach target country game end unlock property present aggregated view further strengthen memory knowledge player
player navigation path show recap traversal path over map as show at example screen+shot figure
game mode describe above be also so call free mode mode allow free movement on map this mode player inspect every unlocked property value any country design help player remember choice learn about country discover map country relation player switch freely between game mode free mode at any time
prototype implementation select lod source source contain standard geographical economical data like area gdp in+addition other properties like number internet+user mobile+phone use implement prototype grammar%23NPR endpoint the+world grammar%23NPR grammar%23NPR use all data query from grammar%23NPR endpoint at beginning game selected properties need fulfill following requirement be numeric in+order+to allow small large comparison b there be data available each country game i. e. all country during development process certain design decision make map basis game be map first idea be use whole world all country however find enough properties properties retrieve all country appear quite challenge another option be use only continent but create too few option therefore set country single continent here choose
on country city-state very small+state be not part prototype due their extreme value mostly all property e. g. bailiwick+of+jersey bailiwick+of+guernsey isle+of+man san+marino in+addition have border only one country thus be difficult reach be hardly visible on map
selection next hop neighbor country decide via minimum difference property value
if be no neighbor country country differ property value same manner player choose error+message display penalty hop add furthermore if two more neighbor have same+difference country randomly select
target country ensure that player reach target country every game path through map be simulated from random start country algorithm begin randomly selected start country from+there randomly hop execute
country select by this procedure take as target country unless be direct neighborhood start country this case process repeat
architecture tool overall structure this interactive web+application be client-server architecture use grammar%23NPR backend mix grammar%23NPR manipulation front-end use holy+grail grammar%23NPR make application adaptive different resolution up certain degree grammar%23NPR wifo5-03.informatik.uni-mannheim.de%2Ffactbook grammar%23NPR dbpedia.org grammar%23NPR grails.org
getbootstrap.com figure map view at end game traversal path responsive design architecture from holy+grail use as+follows model store all country their properties their corresponding resource uri all data provider in+particular grammar%23NPR endpoint
map be part model as static grammar%23NPR file at start game all data in+particular property value query from grammar%23NPR endpoint view generate this view resemble html+document embed game engine engine run web+browser on handle further interaction game engine base on event-driven architecture use browser as event generator event processing mostly do by grammar%23NPR identify desired event example be grammar%23NPR listener each country country be possible since map be grammar%23NPR image contain grammar%23NPR path element each country other feature like movement zoom map animation over map implement use feature grammar%23NPR like matrix transformation grammar%23NPR viewport
related work educational games game purpose use grammar%23NPH champion present approach generate terrain data world+war+i battle game use data from multiple source use information from source determine where place what object 3d environment our game have most similarity so+called data games games by+definition let explore data as game data games differ from serious games fact do not intentionally have agenda but provide capability explore data
data games implement by grammar%23NPR grammar%23NPR use for+example bar+chart as game setting use information about united+kingdom their games grammar%23NPR et+al. discuss whether grammar%23NPR jquery.com grammar%23ENT games be viable alternative traditional method data visualization if use learning
hopper include aspect both data games serious games by give player choice freely explore data but also offer achievable goal contrast serious games have specific agenda goal state hopper be more generic character support knowledge+acquisition through playful data exploration
application use interactive+multimedia application use be rare however be few approach towards media oriented semantic search like grammar%23NPR render map timeline next traditional result list another approach improve visualization semantic search result aim at generate simple diagram like bar+chart graph like grammar%23NPR be approach visualize string based query digital+library by use hierarchical treemaps area semantic image search be work by grammar%23NPR et+al. novelty this approach be that most meaningful word text link ontological concepts allow+for find most powerful set concepts represent document consequently image contain towards more interactive+multimedia application on lod find faceted search as approach explore large multidimensional data space faceted search bring into arena semantic+web by facet facet allow explore heterogeneous semantic+web repository along data+type provide another work on interactive+multimedia faceted search be grammar%23NPR combine different distribute heterogeneous semantic+web repository including social+media data such as photo location interactive explorative user+interface
above mention retrieval tool application be good example how interaction specifically design enable user intuitively formulate query interact result case state hopper player not formulate query but rather hypothesis by click on low high value property i. e. by make choice test if their hypothesis be correct result their interaction become immediately apparent i. e. feedback about properness directly reflect player thus unlike retrieval tool user experience state hopper be more about do system how interact system rather than query result minimum effort
conclusion present interactive+multimedia based game call state hopper hopper allow player acquire knowledge about country world concrete prototype use european map data from different source prepare player journey goal be reach specific target country from randomly chosen start country just by explore property like population gdp area
qualitative evaluation pilot user show that game make+fun be very interesting as game have strong educational character quite challenge create state hopper such that enjoy pleasant unlike other games like ego shooter 3d games naturally appeal player by combine different discrete media asset interactive navigational element animation state hopper provide enjoyable way discover relation fact about country spatial context
current prototype easily extend other country continent for+example implement on level united+states+of+america use data.gov as lod provider perhaps more interesting extension be provide support numerical property as+long+as order value exist make comparable among country make game even more appeal implement various difficulty level by specify time+limit each decision other game mode be multiplayer challenge e. g. two player compete reach target country fast unlock all property few possible hop
state hopper application develop context course theoretical foundation application interactive+multimedia at summer term video application be available from youtu.be%2F7oc4k9ml2io
reference grammar%23NPR visualization search interface institute+of+electrical+and+electronics+engineers grammar%23NPR vis.
grammar%23NPR grammar%23NPR
generate game content from open data foundation digital games page 290+291 association+for+computing+machinery
generate interesting monopoly+board from open data computational intelligence game institute+of+electrical+and+electronics+engineers

facet browser heterogeneous semantic+web repository grammar%23NPR page 272+285 springer

mapping game mechanics traditional data visualization grammar%23NPR
grammar%23NPR interface semantic search grammar%23NPR ceur-ws.org
algorithm generate ontology based visualization from semantic search result database application institute+of+electrical+and+electronics+engineers
dynamic taxonomy faceted search springer

interactive semantic exploration data media base on federated cloud infrastructure
grammar%23NPR 298+304
interaction user experience multimedia technology human-centered computing page association+for+computing+machinery
grammar%23NPR based information+retrieval in+support+of concept design
engineering informatics 131+146
bar+chart ball data game foundation digital game grammar%23NPR
link open data drive game generation grammar%23NPR springer case since solution require modify several basic definition ontology even structure
distinction between basic basic concept ontology not always clearly state be usually dependent upon interpretation be case sumo where all concept define terms sumo prevent direct translation sumo into grammar%23NPR this problem also prevent more formal characterization model error next provide some example modelling error describe above cq cq enable automated detection by follow our proposal
truth test let consider cq list contain at+least one item be not empty null instance list list instance item entity grammar%23NPR item list not equal list instance brain brain instance table table not grammar%23NPR brain table some initial experiment atp realize that conjecture entail from neither grammar%23NPR sumo tptp-sumo analyse knowledge require answer each question find following problem
case cq list contain at+least one item be not empty null object only axiomatize be instance list hence proper characterization miss solve this problem include following axiom grammar%23NPR sumo grammar%23NPR as characterization not grammar%23NPR including this axiom grammar%23NPR sumo grammar%23NPR entail list contain at+least one item be not empty null
second cq not prove because too weak characterization concept animal in+particular source problem be instance instance animal+organism part structure animal instance animal animal this axiom only apply instance organism be not case table however be not necessary restrict use this axiom instance organism since ontology already entail that only animal have thus relax antecedent formula by simply remove that restriction
part define as grammar%23NPR hence be reflexive class animal define be disjoint thus from resulting axiom be possible infer that any instance be trivially part be also instance animal contradict disjointness class therefore be more suitable use grammar%23NPR be irreflexive resulting axiom instead part sum+up above axiom rewrite instance grammar%23NPR structure animal instance animal animal grammar%23NPR sumo grammar%23NPR entail by+means+of replace old axiom
problem pass truth test come from unsuitable characterization some basic relation about attribute more specifically grammar%23NPR grammar%23NPR be variable arity relations constrain use attribute as case other variable arity relations define sumo such as partition grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR characterize on basis grammar%23NPR grammar%23NPR grammar%23NPR sumo so-called row operator use proper characterization this problem remain unsolved tptp-sumo however grammar%23NPR sumo grammar%23NPR directly inherit from sumo unsuitable characterization grammar%23NPR grammar%23NPR prevent prove truthtests involve attribute fix this problem include grammar%23NPR sumo grammar%23NPR two axiom characterize grammar%23NPR grammar%23NPR enable grammar%23NPR sumo grammar%23NPR entail
let consider falsity test table be living exist table instance table table attribute table living perform many run use different atp find proof goal from grammar%23NPR sumo grammar%23NPR tptp-sumo likewise atp not find either any refutation from grammar%23NPR sumo grammar%23NPR however new characterization grammar%23NPR grammar%23NPR grammar%23NPR sumo grammar%23NPR enable atp find proof from this proof discover that problem be related following axiom grammar%23NPR dead living grammar%23NPR dead unconscious instance unconscious instance agent grammar%23NPR attribute agent living exist instance attribute agent according axiom be not possible have both living dead as attribute however by dead be instance moreover have dead as attribute imply also have living by
set conflictive axiom decide remove axiom define dead as subattribute unconscious from grammar%23NPR sumo grammar%23NPR this incorrectness hide both grammar%23NPR sumo tptp-sumo due inappropriate characterization attribute
this development phase grammar%23NPR sumo grammar%23NPR pass creative truth test whereas creative truthtests classify as unknown by tptp-sumo respectively creative falsitytests classify as unknown by three ontology
derive competency question from wordnet in+order+to build our benchmark use mapping from grammar%23NPR sumo mapping connect each synset grammar%23NPR into term sumo use three relation equivalence subsumption instance relation denote by concatenate symbol equivalence subsumption instance corresponding sumo concept for+example grammar%23NPR education n grammar%23NPR connect pilot grammar%23NPR grammar%23NPR additionally complementary relation equivalence subsumption also use
mapping from grammar%23NPR sumo use term from core sumo but also from domain ontology however both tptp-sumo grammar%23NPR sumo only use axiom from core sumo thus our first task be obtain mapping from grammar%23NPR core sumo on basis mapping from grammar%23NPR sumo this end each grammar%23NPR synset not map term cover by both grammar%23NPR grammar%23NPR sumo conveniently use structural relation sumo instance subclass subrelation grammar%23NPR inherit term core sumo synset connect note that this process sometimes require modify mapping relation for+example connect sumo class frying belong domain ontology food same domain ontology frying define be subclass cooking cooking define top level sumo consequently frying not define core sumo but cooking be
connect cooking resulting mapping however instead equivalence connect cooking by subsumption map relation be cooking total number mapping core sumo 114%2C948 be slightly small than number mapping sumo 115%2C872 since some term not properly define be mainly due fact that some term mapping derive from old version sumo be not long available current one for+example architect n respectively connect sumo concept salmon architect not appear latest version sumo
obtain suitable mapping from grammar%23NPR ontology want compare design several conceptual pattern question information about antonym process grammar%23NPR
antonym pattern grammar%23NPR provide set 8%2C689 antonym pair including noun verb adjective adverb from 7%2C410 antonympairs properly map core sumo as describe above however only consider antonympairs where both synsets connect use equivalence map relation total pair discard where subsumption map relation use antonym pair propose two conceptual pattern question first pattern base on fact that two sumo class connect antonym synsets grammar%23NPR not have common instance for+example antonym liquescent n respectively connect freezing melting thus from above antonym pair derive following competency question not exist x%29 instance x melting instance x freezing similarly second conceptual pattern state that two attribute connect antonym synsets be not compatible for+example from antonym grammar%23NPR connect awake asleep derive following competency question not exist x%29 attribute x awake attribute x asleep apply two pattern on antonym pair where both synsets be connected used equivalence obtain different truth test by negate above cq also obtain different falsity test
process pattern process use information morphosemantic database contain semantic+relation between morphologically related noun verb from semantic+relation define database select agent result instrument event first three relate process verb corresponding agent result instrument noun from infer 1%2C280 cq by simply state same property terms sumo for+example grammar%23NPR establish that result grammar%23NPR be grammar%23NPR respectively map grammar%23NPR exist x y%29 instance x grammar%23NPR result x y%29 instance as before also obtain 1%2C280 falsity test by negate previous
last relation event connect noun verb refer same process be same process assume that both noun verb map same class sumo thus if noun verb map different sumo class constant our hypothesis be that mapping be wrong follow this criterion grammar%23ENT grammar%23NPR at wordnetcode.princeton.edu standoff file morphosemantic-links.xls.table evaluation sumo based grammar%23NPR ontology grammar%23NPR grammar%23NPR sumo grammar%23NPR p n u t p t n p n u t p t n p n u t p t n truth test 3%2C556 3%2C552%2C361.97 s. 3%2C467 s. 2%2C662 s. antonym pattern s. s. s. relation pattern 1%2C280 1%2C280 1%2C269%2C121.09 s. 1%2C104 s. event pattern s. s. event pattern s. s. event pattern 1%2C857 1%2C756 s. 1%2C824 s. 1%2C306 s. falsity test 3%2C556 3%2C090 s. 3%2C063 s. 3%2C069 s
pattern s. s. s
pattern 1%2C280 1%2C276 s. 1%2C249 s. 1%2C258 s
pattern event pattern 71+259 s. 72+258 s. 72+258 s
pattern 1%2C857 1%2C470 s. 1%2C469 s. 1%2C469 s
propose different conceptual pattern question depend on used mapping relation purpose detect wrong mapping if both synsets connect two different sumo class constant use equivalence map relation be possible prove that two class constant denote different class for+example grammar%23NPR grammar%23NPR respectively connect sumo class death killing hence derive following cq not equal use this pattern derive cq second pattern question focus on case where synsets connect different sumo class constant using different mapping relation be one synset connect use equivalence map relation whereas other synset connect use subsumption be mapping information less precise than first case not suffice prove that class be different this case pattern state that class connect use equivalence not be subclass class connect use subsumption for+example event relate grammar%23NPR connect grammar%23NPR connect pretending
assume that not be subclass pretending derive following cq not subclass from second event pattern question derive cq in+fact this second pattern see as particular case third one where both synsets connect use subsumption map relation this case pattern state connected sumo class be subclass other one for+example event relate appraisal n n respectively connect judging comparing consequently derive following cq not subclass judging comparing subclass comparing judging use this third pattern obtain 1%2C857 cq
total obtain 2%2C212 truth test by state that mapping be not correct corresponding 2%2C212 falsitytests state that mapping be correct
evaluate first-order ontology this section summarize evaluation result tptp-sumo different version grammar%23NPR sumo use methodology propose section this evaluation use set 7%2C112 cq automatically obtain from grammar%23NPR as describe above section
sum up some runtime figure association+of+tennis+professionals vampire when evaluate tptp-sumo execution time+limit grammar%23ENT each ontology provide number pass p column pass n column unknown cq u column together+with average runtime pass t p column pass t n column cq be worth remark that average runtime cq classify as unknown be maximum execution+time grammar%23ENT since no proof found from result be clear that grammar%23NPR sumo grammar%23NPR outperform grammar%23NPR sumo grammar%23NPR terms competency both truth test more pass test falsity test category less pass test efficiency average runtime grammar%23NPR sumo grammar%23NPR be long since additional test require more complex proof similarly grammar%23NPR sumo grammar%23NPR outperform tptp-sumo truth test category since tptp-sumo only pass truth test while grammar%23NPR sumo grammar%23NPR pass falsity test tptp-sumo be ontology less nonpassing test but average runtime be clearly long thus think that number pass falsity test tptp-sumo be large if use longer execution time+limit experimentation
that pass falsity test provide useful information improve ontology as case cq see section additionally pass falsity test also provide very useful information for+example verb grammar%23NPR shout v v be antonym grammar%23NPR map sumo class speaking vocalizing respectively be antonym expect that two sumo class speaking vocalizing not have any common instance as state by next conjecture correspond cq include grammar%23NPR www.vprover.org grammar%23NPR this experimentation use standard 64-bit grammar%23NPR cpu grammar%23ENT desktop machine grammar%23ENT grammar%23NPR falsity test set exist x%29 instance x vocalizing instance x speaking however atp infer above conjecture from grammar%23NPR be grammar%23NPR sumo grammar%23NPR not pass this falsity test this fact serve detect that mapping verb grammar%23NPR shout v sumo be not suitable on+the+contrary cq classify as unknown when evaluate tptp-sumo grammar%23NPR sumo grammar%23NPR prevent detect incorrect mapping
discussion show new framework experimental result evaluate improve large complex grammar%23NPR ontology use atp our result show appropriateness use atp debug grammar%23NPR ontology as+well+as their practical use infer trivial statement
author claim necessity prove formal faithfulness logical translation sumo format kuki+international+force+%28kif%29 suo-kif lack formal notion logical+consequence formal deduction system where that kind mathematical result such as conservative extension rely on thus following our repair just intend improve reasoning capability ontology
possible topic discussion be need some clear quality criteria cq for+instance our cq be universally closed formula form grammar%23NPR grammar%23NPR p where p be not deducible from grammar%23NPR grammar%23NPR without help knowledge contain ontology be our cq include information premises infer from ontology note be not case test from grammar%23NPR problem domain grammar%23NPR thus our set cq include clean version goal all property pi be deducible from ontology remove
our set cq not only have conjecture that expect be deducible but also contain conjecture expect not be deducible obviously be also very interesting experimentation since conjecture allow detection model error when prove
future work new framework present this paper open multiple avenue future research now work in+order+to improve competency grammar%23NPR sumo case pass automatic test imply correct either ontology b some mapping from grammar%23NPR ontology c some grammar%23NPR relation in+parallel also want enlarge our current set cq by gathering more question from grammar%23NPR mapping sumo alternative dataset additionally very interest determine part ontology use solve set cq thus in+order+to further debug grammar%23NPR sumo also explore possibility automatically derive exhaustive set question from axiomatization
formal declarative semantics grammar%23NPR sublanguage kuki+international+force+%28kif%29 give+in
our framework not only allow measure competency different sumo based ontology but also efficiency when solve large set trivial inference this sense investigate more efficient representation ontology additionally our framework act as new benchmark test performance
open research line focus on prove consistency grammar%23NPR for+instance on basis modular approach by including into our framework sophisticate model finder type grammar%23NPR
plan develop automatic procedure exploit grammar%23NPR complete mapping grammar%23NPR automatically infer new semantic properties relation between grammar%23NPR concepts validate consistency resource associate grammar%23NPR such as grammar%23NPR grammar%23NPR grammar%23NPR
acknowledgment be grateful anonymous reviewer their insightful comment this work partially fund by spanish project skater grammar%23NPR comma grammar%23NPR basque project grammar%23NPR grammar%23NPR grant grammar%23NPR grammar%23NPR
reference grammar%23NPR reengineering ontology first-order reasoning
j. semantic+web inf. syst. 80+116
grammar%23NPR v. grammar%23NPR semantic+web primer 2nd edition
grammar%23NPR nucleus open data grammar%23NPR k. et+al. editor semantic+web grammar%23NPR page 722+735 springer
automated consistency checking expressive ontology grammar%23NPR wrong interpretation success fink grammar%23NPH et+al. editor grammar%23NPR 5th int. workshop on acquisition representation reasoning grammar%23NPH grammar%23NPR grammar%23ENT
linked data story so+far int. j. semantic+web inf. syst.
grammar%23NPR crystallization point data semantics science service agent on 154+165
editor wordnet electronic lexical+database
pez rez grammar%23NPH
methodological guideline reuse general ontology data knowledge engineering
grammar%23NPH sweeten ontology grammar%23NPR
rez grammar%23NPH et+al. editor grammar%23NPH
grammar%23NPH grammar%23NPR ontology semantic+web grammar%23NPR page 166+181 springer
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH v. grammar%23NPR grammar%23NPH version reference+manual grammar%23NPR stanford+university computer+science department logic group
methodology design evaluation ontology grammar%23NPR workshop on basic ontological issue grammar%23NPH sharing grammar%23NPR grammar%23ENT
semantics grammar%23NPR workshop on institute+of+electrical+and+electronics+engineers standard upper ontology grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPR spatially temporally enhanced knowledge+base from artificial+intelligence
reduce owl entailment description logic satisfiability semantics science service agent on 345+357
reasoning support expressive ontology language use theorem prover
et+al. editor foundation information grammar%23NPR page 201+218

grammar%23NPH grammar%23NPH grammar%23NPH
linked data be merely more data et+al. editor grammar%23NPR spring symposium link data meet artificial+intelligence page

grammar%23NPR instantiation based theorem prover first-order logic system description grammar%23NPH et+al. editor automated reasoning grammar%23NPR page 292+298 springer
first-order theorem proving vampire editor computer aid verification grammar%23NPR page springer
o grammar%23NPR modular consistency proof grammar%23NPR et+al. editor grammar%23NPR
25th conf. on grammar%23NPR grammar%23NPR grammar%23ENT

grammar%23NPH introduction syntax content grammar%23NPR editor grammar%23NPR spring symposium compiling background+knowledge grammar%23NPR grammar%23NPH repr. question+answering page
towards standard upper ontology et+al. editor grammar%23NPR 2nd int. conf. on formal ontology information+systems grammar%23NPR grammar%23ENT page association+for+computing+machinery
link lexicon ontology mapping wordnet editor grammar%23NPR institute+of+electrical+and+electronics+engineers int. conf. on inf. grammar%23NPH grammar%23NPH grammar%23ENT volume page 412+416
standard grammar%23ENT from sigmakee.cvs.sourceforge.net%2Fsigmakee sigma
first-order reasoning on large ontology et+al. editor grammar%23NPR workshop on large theory grammar%23NPR proceeding ceur-ws.org
task based approach ontology evaluation grammar%23NPR workshop on ontology learning population grammar%23NPR grammar%23ENT

expressivity efficiency common-sense ontology et+al. editor paper from workshop on context ontology theory practice application grammar%23ENT page
map ontology into grammar%23NPR editor paper from workshop on semantic+web grammar%23ENT page
design implementation vampire ai communications 91+110
e brainiac theorem prover ai communications 111+126
grammar%23NPR problem library associated infrastructure 337+362 grammar%23ENT figure tet versus grammar%23NPR on number character word line
extremely complicated formatting pdf file lack standard transformation schema from pdf file text file only perform baseline comparison against this paper compare number pdf file number line character word extract pdf file use most recent release tet version grammar%23NPR version at time write then use built-in wc command count line character word result tabulate table this table imply that grammar%23NPR have good performance terms number extractable pdf file although be file file neither extract in+fact be only one file tet extract but grammar%23NPR can not but be file otherwise
figure compare number character grammar%23NPR word grammar%23NPR line grammar%23NPR file extractable by this chart indicate that grammar%23NPR extract more character line than tet but extract less word file document give current result be difficult conclude whether tet grammar%23NPR absolutely beat implement framework because tet be paid software grammar%23NPR have free license experiment on grammar%23NPR only
tet grammar%23NPR execute by launch system+call successfully document pass grammar%23NPR otherwise drop log
tet versus grammar%23NPR on extractable file
tet fail success overall tet success 1+942+943 tet fail grammar%23NPR overall 41+959+1000 successful extraction output contain at+least one line
academic document filter rule based classifier discriminate between academic academic document by identify textual pattern such as bibliography reference terms from full text
more sophisticated approach develop by base on set structural feature use supervised machine+learning algorithm four type structural feature example tabulate table table example structural feature
specific feature file+size kilobyte page+number
specific feature document length character word line ratio between reference mention token
specific feature appearance section name e.g. abstract etc.
feature appearance this paper this book etc.
classification quality re- train classifier new sample new sample extend training sample describe by add 2%2C000 manually labeled document randomly select from crawl repository existing digital+library also supplement sample set manually thesis resume slide book labeling perform by three people independently verify by domain expert revised training data yield excellent testing result precision recall from 10-fold cross validation be over 95%25
our knowledge be good classification approach separate scholarly document from corpus crawl pdf document
given document filter take full text pdf file filter need obtain some structural feature directly from pdf file as input output value
header extraction grammar%23NPR et+al. compare list metadata extraction tool scientific pdf document base on sample select from grammar%23NPR digital+library partial score scheme use assess performance individual tool on following metadata field title author abstract also evaluate author last+name year information their second third round evaluation their evaluation deliver good result result significantly beat grammar%23NPR other competitor
attempt verify conclusion use more heterogenous sample select from grammar%23NPR goal be investigate good performance over grammar%23NPR this sample comprise paper title and%2For author mistakenly extract by grammar%23NPR correct by end+user manually extract paper title author as ground truth extract title author use then compare automatically metadata against ground truth focus on title author because be most essential field crawl based digital+library search+engine evaluation perform by three people independently visually compare ground truth automatically metadata only guideline this user study be be consistent their own judging criteria posterior survey indicate that user generally follow two principle all word parse correctly qualify correct title extraction exception letter accent ligature all author parse qualify correct author extraction exception letter accent ligature final judgement report base on majority+vote each field table show evaluation result result be general consistent give heterogeneous property our sample be clear that outperform grammar%23NPR significantly term both title author by at+least 30%25 good senario
sample be biased against grammar%23NPR result table represent good performance over grammar%23NPR empirically most case achieve comparably quality when grammar%23NPR extract well overall performance over grammar%23NPR strongly motivate employ header extraction academic document
header extraction execute on research paper only so corresponding em arrange grammar%23NPR execute either stand alone as service
latter be more efficient because load required library file only once em handle error output by appropriately such as service not respond first run generate grammar%23NPR file then extract field correspond predefined customizable metadata schema into xml file
versus grammar%23NPR
extraction grammar%23NPR incorrect correct overall grammar%23NPR correct 65.7%25 grammar%23NPR incorrect 34.3%25 overall 8.2%25 91.8%25 100%25 author extraction grammar%23NPR incorrect correct overall grammar%23NPR correct 57.5%25 grammar%23NPR incorrect 42.5%25 overall 9.6%25 90.4%25 100%25 citation extraction grammar%23NPR be citation extraction tool develop by
parse citation perform baseline comparison between by look+at number reference item extract out quality title author years three field appear almost all reference item regardless+of conference paper journal article book be few exception such as cite website author field be empty cite forthcoming paper year information miss
be existing dataset use benchmarking citation extraction such as dataset grammar%23NPR dataset use however usually not provide original pdf file file input
build ground truth by manually extract citation from list selected paper
first experiment randomly select scholarly document from grammar%23NPR compare number automatically reference item against real+number
comparison result tabulate table result indicate that while both grammar%23NPR have similar number under- parse paper tend overparse citation
grammar%23NPR versus on number item
under- parse match over parse grammar%23NPR here under- parse be number paper automatic item be less+than real+number meaning number match over parse be then selfexplanatory
comprehensive comparison citation extraction quality require large+number manually reference from paper accurate automatic title author match algorithm another issue be align reference item automatic item manually result be beyond scope this paper here focus on paper paper cover typical citation style
most conference journal paper plain+text well extract difference between grammar%23NPR be minor however be few case case make their parsing result different first seem that not work well reference item start acronym e.g. grammar%23NPR when parse reference tend combine two consecutive title form wrong title on+the+other+hand if paper contain appendix follow reference section grammar%23NPR treat appendix as part last reference string make mistake when be control+character insert among textual content basically both grammar%23NPR apply conditional random field field explain most case work nearly equally well difference be likely due training sample
advantage grammar%23NPR be that extract citation context context be snippet text around location where reference mention not extract citation context directly but output grammar%23NPR file contain label text body correspond label reference item
be still possible extract citation context by extra coding this project use grammar%23NPR as default citation extract tool
table improvement new framework compare current be integration table figure algorithm em be couple existing open+source tool extract table figure such as table extraction algorithm propose by figure extraction algorithm propose by recently develop package call grammar%23NPR extract figure table one command independent evaluation suggest that take grammar%23ENT grammar%23ENT on each page achieve f1 measure 90%25 therefore be arguably good open+source figure table extraction tool besides integrate figure table extraction so not need implement individual em
execute by call com piled binary output directory figure grammar%23NPR file set timeout grammar%23ENT in+case program hang when be pro cess unusual file output grammar%23NPR file concatenate into single grammar%23NPR file by wrapper figure table save as png file into output folder paper
algorithm extraction algorithm extraction automatically extract pseudo- code reference context from paper generate synopsis each pseudocode be handful open source software dedicate on algorithm extraction most recent successful develop by hereafter grammar%23NPR
core+program write perl wrapper
take text+file output directory as input parameter write xml file supplied directory
set timeout grammar%23ENT
extraction goal this framework be abstract away logic coordinate run extraction process let user focus on define actual logic extract metadata
framework implement show be more usable than perl widely use academic challenge be present standard implementation i.e. grammar%23NPR grammar%23NPH prevent interpreter from execute more+than one thread at time as result grammar%23NPH essentially prevent grammar%23NPR job from be able gain performance benefit through multi threading however multi-processing not suffer from grammar%23NPH limitation because allow multiple instance interpreter therefore multiprocessing use instead multi threading in+order+to meet performance goal framework
code split+up into four main module core module be responsible main functionality framework run overall extraction process contain grammar%23NPR class class be where user software library configure execute extraction core module internally manage all parallelization runnables module let user define their own runnables contain base grammar%23NPR class class inherit by extractor filter class define extractor filter user extend either extractor filter class override extract filter method respectively
utils module contain various useful tool for+example function call provide user easy way start pass data get result from external process while specify time+limit process finally log module contain log handler that extraction framework use by+default full source+code extraction framework be available online on grammar%23NPR
experiment validate framework run on dedicate virtual+server logical core grammar%23ENT server have grammar%23ENT ram
first experiment set run pdf document randomly select from existing digital+library grammar%23NPR github.com%2Fseerlabs%2Fnew-csx-extractor single process goal this experiment be investigate overall performance framework term runtime each module system+resource consume average runtime one document be about grammar%23ENT figure illustrate distribution average runtime all em among all submodules algorithm extraction be currently follow by citation extraction
header extraction be fast take only grammar%23ENT document on+average if not perform algorithm extraction take on average grammar%23ENT process one pdf file use single thread framework handle all exception produce by specific extraction tool run without hang crash average cpu usage be less+than 15%25 peak cpu usage 55%25 happen only two spike runtime distribution depend on actual extraction tool number report here give guideline terms em opt time sensitive job
average runtime document second distribution em
second experiment run framework on same dataset by vary number process process goal be investigate how system+resource cpu memory average runtime each pdf change process figure show that cpu usage increase linearly start about 13%25 when process be less+than then asymptotically increase around 90%25 keep steady even process increase clearly cpu fully use rest 10%25 use by operating+system
memory however increase very slowly start from 56%25 grammar%23NPR only use by less+than 70%25 when process average runtime document normalize single thread runtime decrease quickly power law fashion when process be less+than then asymptotically reach limit around 14.3%25 grammar%23ENT with+respect+to single thread runtime
coherence cpu usage average runtime imply that default implementation framework be cpu bound cpu usage not follow strict linear relation out process total number core
believe be due resource use by specific extraction tool as contain internal processes use multiple core memory+cache memory+cache stack+up more overhead more process slow increase memory indicate that framework leave negligible footprint memory so memory occupy by library module figure overall performance multiple process
application grammar%23NPR view as knowledge capture framework by as be capable encapsulate runnable knowledge capture tool organize result
usable maintainable modular scalable feature apply many knowledge capture project
grammar%23NPR digital+library search+engine provide free access 6%2C000%2C000 academic document while metadata this project frequently request widely use various project metadata quality have urgent need improve due mistakenly title author citation be also fraction mi classified document i.e. academic classify as academic this digital+library host crawl database more than 21%2C000%2C000 pdf file will significant improve metadata quality by run grammar%23NPR on big repository build knowledge search+engine out entity use automatic knowledge index technique be valuable platform author quickly locate existing experimental result related work
long propose that next+generation search+engine be semantic e.g. current solution by google+inc. use knowledge graph billion entity gather from wide variety source
entity then logically link answer user question actually recent work attempt develop system extract simple knowledge statement from plain+text from science textbook slight modification grammar%23NPR can fit such system into framework use plain+text as input knowledge extraction likewise be useful build semantic academic search+engine large knowledge+base populate by entity extract from academic document grammar%23NPR launch project called publish a+number+of work on knowledge extraction e.g. grammar%23NPR framework be appropriate efficient solution such project obtain collection metadata paper knowledge entity
6. summary develop grammar%23NPR framework extract multiple knowledge entity within scholarly document consist following module plain+text extractor use grammar%23NPR scholarly document filter header extractor use grammar%23NPR citation extractor use grammar%23NPR figure table extractor use grammar%23NPR algorithm extractor design easily use maintainable modular scalable except+for full text extraction header extraction module all module be pluggable so user easily substitute default extraction tool their preferred alternative test on dedicated server core grammar%23ENT take on average grammar%23ENT document run all default extraction tool single thread minimum grammar%23ENT launch processes framework leave very small footprint memory be first step construct knowledge+base scholarly ontology use scholarly big data
work be build semantic scholarly ontology at scale also extend work on other scholarly related document e.g. scholar resume slide syllabi integrate system into grammar%23NPR spark framework significantly boost system efficiency
acknowledgment thank useful discussion gratefully acknowledge support from national+science+foundation
reference grammar%23NPR www.pdflib.com%2Fproducts%2Ftet
grammar%23ENT
grammar%23NPR poppler.freedesktop.org access grammar%23ENT
tabula tabula.technology access grammar%23ENT
grammar%23NPH algorithm search+engine software developer proceeding 3rd international workshop on search driven development user infrastructure tool evaluation suite page new+york new+york united+states+of+america association+for+computing+machinery
grammar%23NPH grammar%23NPH grammar%23NPH automatic identification research article from crawl document grammar%23NPR workshop on scale classification classify big data from
v. grammar%23NPH grammar%23NPH enable expert build knowledge+base from science textbook proceedings 4th on knowledge capture grammar%23NPR page 159+166 new+york new+york united+states+of+america association+for+computing+machinery
grammar%23NPH grammar%23NPH grammar%23NPH figure metadata extraction from digital document 12th on document analysis recognition grammar%23NPR page 135+139
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
figure search+engine architecture chemistry digital+library grammar%23NPR page 369+370 grammar%23ENT look beyond text figure table caption from computer+science paper grammar%23NPR workshop on scholarly big data
large-scale extraction use knowledge from text proceedings on knowledge capture grammar%23NPR page 153+160 new+york new+york united+states+of+america association+for+computing+machinery
construct textual kb from biology textbook proceeding joint workshop on automatic knowledge+base construction scale knowledge extraction akbc-wekex page grammar%23NPR united+states+of+america
guideline electronic+text encoding interchange
grammar%23NPH grammar%23NPH m. y grammar%23NPR open source crf reference string parsing package proceedings on language resource evaluation grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH
acknowledgement indexing expand semantics contribution citeseer digital+library
proceedings 3rd on knowledge capture grammar%23NPR page new+york new+york united+states+of+america association+for+computing+machinery
search need shake-up nature 08+2011
grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPR architectural approach unstructured information+processing corporate research environment
engineering 9+2004
semantic search
proceedings 12th on page 700+709 new+york new+york united+states+of+america association+for+computing+machinery
grammar%23NPH automatic document metadata extraction use support vector machine proceedings 3rd association+for+computing+machinery on digital+library grammar%23NPR page
grammar%23NPH number scholarly document on public grammar%23NPR one grammar%23NPR grammar%23ENT
rate+of+growth scientific publication decline coverage provide by science citation index grammar%23NPR 575+603
grammar%23NPH evaluation header metadata extraction approach tool scientific pdf document proceedings 13th association+for+computing+machinery on digital+library grammar%23NPR page 385+386 new+york new+york united+states+of+america association+for+computing+machinery
grammar%23NPH grammar%23NPH automatic extraction table metadata from digital document
proceedings 6th association+for+computing+machinery on digital+library grammar%23NPR page 339+340 new+york new+york united+states+of+america association+for+computing+machinery
grammar%23NPH grammar%23NPH grammar%23NPH automatic system extract figure caption biomedical pdf document grammar%23ENT on bioinformatics biomedicine
grammar%23NPR combine automatic bibliographic data recognition term extraction scholarship publication proceedings 13th on research digital+library grammar%23NPR page 473+474 grammar%23NPR springer-verlag

hidden markov model structure information extraction on machine+learning information extraction
grammar%23NPH grammar%23NPH
indexing ranking chemical+formula chemical name digital document association+for+computing+machinery grammar%23NPR inf. syst. grammar%23ENT grammar%23ENT
grammar%23NPH
detection pseudocode scholarly document use machine+learning grammar%23NPR page 738+742
qualitative+analysis usability perl python tcl grammar%23NPR tenth
grammar%23NPH grammar%23NPH h. h grammar%23NPH grammar%23NPH
ai digital+library search+engine on innovative application artificial+intelligence grammar%23NPR grammar%23ENT methodology construct calculation model scientific spreadsheet grammar%23NPH computer+science martine.de.vos%40vu.nl grammar%23NPH computer+science j.wielemaker%40vu.nl computer+science guus.schreiber%40vu.nl grammar%23NPH computer+science b.j.wielinga%40uva.nl grammar%23NPH research+centre food jan.top%40wur.nl abstract spreadsheet model frequently use by scientist analyze research data model typically describe paper report serve as single source information on underlying research project as calculation workflow model not make explicit reader be not able fully understand how research result calculate trace back underlying spreadsheet this paper propose methodology derive calculation workflow underlie set spreadsheet starting+point our methodology be cell dependency graph represent all spreadsheet cell connection automatically aggregate all cell graph represent instance duplicate same quantity base on analysis formula syntax subsequently use set heuristics incorporate knowledge on spreadsheet design computational procedure domain knowledge select quantity quantity be relevant understand calculation workflow explain illustrate our methodology by actually apply on three set spreadsheet from existing research project domain environmental life+science result from case+study show that our calculation model approximate ground truth calculation workflow terms content size but be not perfect match
corresponding author second affiliation computer+science j.l.top%40vu.nl permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee
new+york united+states+of+america copyright grammar%23ENT grammar%23ENT grammar%23ENT
subject descriptor grammar%23NPR computing methodology simulation modeling model validation analysis grammar%23NPR computer application physical+science engineering general+term algorithm human factor verification keyword spreadsheet calculation model graph aggregation heuristics introduction this article propose methodology semi automatically derive calculation workflow from scientific spreadsheet model by aggregate base on formula syntax heuristics
be main tool use by scientist store analyze research data scientist typically describe their computational spreadsheet model corresponding simulation paper report write publication not serve as documentation but rather provide reader concise explanation underlying concept interpretation simuation result although more more scientific computational model associated data become publicly accessible model be often too big too complicated people understand easily in+practice publication serve as single source information on underlying research project however be desirable if reader publication be able fully understand i.e. on procedural conceptual level how research result calculate trace back underlying spreadsheet main problem be that calculation workflow spreadsheet not make explicit goal this study be solve this problem by explore extent calculation workflow underlie set spreadsheet make explicit distinguish different level model play different role our study first be theoretical model model describe written publication explain concept relation system interest theoretical model be abstract model describe text present diagram equation like differential+equation our first case+study on glacier y d grammar%23NPR h x
theoretical model then translate into procedural statement calculation i.e. calculation model
model still contain domain concept relation but formulate such way that enable actual calculation model result calculation model may not describe written publication our glacier case study calculation model represent by numerically equation equation be solution differential+equation fragment show follwing equation c hi bi hi finally equation from calculation model implement spreadsheet syntax organize table i.e. computational model our glacier case+study look like following formula grammar%23NPR grammar%23NPR c11-h10 b11-b10 grammar%23NPR grammar%23NPR f10 c grammar%23ENT c grammar%23ENT this paper propose methodology semi automatically derive calculation model from computational model construct calculation model provide insight on how research result calculate ideally construct theoretical model model provide more understanding domain knowledge include calculation workflow but expect that this model not directly construct from computational model
consider computational model as set quantity relate each by spreadsheet formula in+order+to construct calculation model need determine quantity computational model be relevant how recognize starting+point our methodology be cell dependency graph this graph spreadsheet formula abstract in- output cell i.e. node dependency between cell i.e. edge cell dependency graph provide good understanding data+flow structure spreadsheet
basic+assumption our methodology be that spreadsheet contain many redundant cell form instance duplicate same quantity automatically aggregate remove redundant cell base on syntax formula subsequently use set heuristics distinguish important auxiliary quantity achieve further aggregation cell dependency graph
resulting set node from fully aggregated graph label serve as input calculation workflow
our methodology use three different type knowledge i.e. knowledge on spreadsheet i.e. design syntax knowledge on computational procedure knowledge on research domain spreadsheet model first two type use aggregation algorithm heuristics incorporate all three type knowledge
explain overview basic+principle our methodology section section illustrate our methodology by actually apply three case+study be all existing research project domain environmental life+science each case+study compare our result manually calculation model create from equation description corresponding research paper
related work numerous researcher different subfield computer+science information+science develop approach support spreadsheet developer during development use their spreadsheet be many commercial tool available end-user like for+example spreadsheet detective slate however tool primarily design detect fix error be less suitable support user understand entire calculation workflow within workbook
most scientific approach also focus on error spreadsheet application may at+the+same+time help user better structure understand their data approach use visualization technique support user understanding several specifically visualize data+flow dependency within spreadsheet dependency between different part spreadsheet for+example visualize animation three-dimensional space large block formula other approach visualize dependency entire workbook graph like structure apply visual abstraction technique make graph manageable but as technique only offer visual representation on different abstraction+level graph remain unchanged still too complex understand
colleague develop toolkit create abstract representation spreadsheet+program facilitate understanding checking complex spreadsheet examine cell dependency graph entire workbook aggregate semantic class i.e. group spreadsheet cell similar formula structure data module i.e. group spreadsheet cell input distinguished result cell
as already observe by approach mention above not implement be currently not available use by third+party although be not able try analyze corresponding tool approach provide useful insight on analysis visualization formula dependency spreadsheet aggregation selection information within cell dependency graph remain issue be focus present study
methodology basic+principle calculation workflow want include quantity that represent domain knowledge be essential understand calculation research result assume that set spreadsheet multiple cell refer same quantity as refer different instance copy that quantity number spreadsheet cell consequently number node edge cell dependency graph be therefore great than number quantity present spreadsheet develop algorithm algorithm recognize aggregate instance andcopies base on analyze formula syntax
also assume that only part quantity computational model have direct contribution computation research result be therefore essential calculation workflow other part consist auxiliary quantity support developer design test computational model therefore leave+out calculation workflow use knowledge on spreadsheet design computational procedure develop set heuristics apply heuristics on aggregated graph select quantity quantity have direct contribution use as input calculation workflow
our approach roughly consist three steps
starting+point our methodology be construction cell dependency graph then aggregate cell dependency graph base on formula syntax finally aggregate cell dependency graph by apply heuristics explain illustrate our methodology by apply on three case+study first two step develop independent from case+study heuristics on+the+other+hand develop base on analysis case+study
different step describe more detail next section all algorithm develop use be publicly grammar%23NPR
construction aggregation cell dependency graph automatically generate cell dependency graph by parse all formula set spreadsheet analyze dependency between corresponding cell algorithm
create cell dependency graph all cell do all cell contain formula add here aggr step simplify lookup aggr step formula group all cell sign cell inputcell end create pair outputcell grammar%23NPR grammar%23NPR end collect pair grammar%23NPR grammar%23NPR input grammar%23NPR input end all input grammar%23NPR do if input not outputcell then input source cell create pair %3A%5B source cell end if grammar%23NPR grammar%23NPR source pair transpose grammar%23NPR into grammar%23NPR dependent grammar%23NPR dependent end aggregate cell dependency graph by add three different aggregation step original algorithm algorithm first simplify formula built-in lookup function function consider dynamic copy function as function refer value another spreadsheet cell but represent corresponding cell address as formula however as formula syntax be such grammar%23NPR prolog www.swi-prolog.org grammar%23NPR plsheet https%3A%2F%2Fgithub.com%2Fdata2semantics that refer whole cell range lookup function yield lot redundant node edge cell dependency graph if data lookup function refer be present function solve replace by single reference target+cell
aggregate copy equivalent region i.e. block cell contain formula same syntax algorithm generalize formula in+order+to compare their structure relative cell position formula aggregate over multiple column row sheet rectangular area instead original formula list aggregated formula now use determine dependency spreadsheet
last recognize all spreadsheet cell cell copy value another cell i.e. original dependency graph replace corresponding cell original cell
create formula group all cell do all cell contain formula generalize formula replace cell coordinate by variable x y constraint thereof end end all generalized formula group formula same key syntax relative cell position if formula group then represent group as node grammar%23NPR f sheet x y formula row column sheet area range else single formula end if end grammar%23NPR single formula formula group development application heuristics develop set heuristics distinguish relevant from auxiliary variable spreadsheet few study report general+term on design characteristic spreadsheet
study state that spreadsheet usually be mixture input+data computation contain check balance provide visual feedback user developer combine this knowledge in-depth analysis our three case+study formulate set concrete heuristics
this study apply heuristics manually as be most fast easy+option give small data+set formulate heuristics such way that if prove succesful eventually be integrated our algorithm
manual application heuristics node resulting graph manually label enable comparison ground truth calculation workflow follow method grammar%23NPH colleague by label node textual value that found corresponding table header if node represent aggregation over range cell either column row this range include label either by provide all corresponding header value by provide coordinate range
case+study set grammar%23NPR characteristic three case study glacier hormone pollutant sheet cell 525+2220+2703 formula 312+1676+1666 formula type copy arithmic copy average if average grammar%23NPH grammar%23NPH large grammar%23NPR application our methodology illustrate by apply three existing scientific+research project use spreadsheet perform analysis select our case+study by perform literature search select journal paper from domain environmental life+science that make spreadsheet publicly available as supplementary data contain both textual description equation equation explain variable computation spreadsheet
each case+study manually construct ground truth calculation workflow from corresponding journal paper subsequently generate set quantity include calculation workflow follow step our methodology compare variable from this ground truth set quantity construct by our methodology in+order+to determine whether our methodology perform accord our expectation
raw+data all case+study model contain spreadsheet spreadsheet not connect other spreadsheet not contribute computation model result remove spreadsheet from our analysis
first case+study base on spreadsheet model reconstruct surface profile former mountain glacier ice+cap use one sheet i.e. advanced version model reconstruct surface profile glacier second case+study be spreadsheet model calculate free serum concentration hormone humans our case+study use sheet sheet contain input+data calculation binding constant calculation free hormone concentration last case+study be spreadsheet model score dangerous chemical pollutant relate exposure scenario human risk evaluate uncertainty scoring procedure use seven sheet i.e. sheet contain scoring matrices pollutant sheet collect corresponding scoring result
glacier case+study contain less content cell formula than other two case+study furthermore formula cell glacier case only contain arithmic operator while other case+study also contain copy action built-in spreadsheet function table
construction ground truth calculation workflow written publication our case study contain model section this section researcher explain how translate their theoretical model into quantity expression that include calculation model
use this calculation model as ground truth calculation workflow our study however be aware that be not ground truth strict sense term as be our interpretation calculation model be present written publication
determine quantity i.e. variable connection be present equation model section include as node edge respectively graph this process use two additional rule as first rule only include quantity quantity be directly related domain knowledge desrcibed written publication second rule concern manifestation i.e. abstraction+level quantity their paper researcher choose different abstraction+level quantity their calculation model choose abstract over instance for+example over individual hormone pollutant explicitly include different abstraction+level same quantity their equation pollutant case for+example researcher distinguish quantity quantity be related individual pollutant from that same quantity be related all pollutant construction our ground truth follow abstraction+level abstraction+level choose by researcher
written publication also contain information on how variable equation implement spreadsheet i.e. computational model not use this information construction ground truth calculation workflow yet this information be useful analyze explain result when compare ground truth automated calculation workflow
result ground truth calculation workflow result ground truth calculation workflow show table figure ground truth pollutant case not show as be too big display
spreadsheet model glacier case+study build on initial equation describe driving stress i.e. stress stress cause glacier deform as function weight surface gradient ice model hormone case+study base on set equilibrium equation describe concentration free hormone unoccupied protein binding+site both case study researcher numerically discretize this equation derive approximate solution equation describe this process model section their paper use equation as ground truth calculation workflow both case study
hormone case+study equation model section apply five individual hormone follow approach researcher by consider hormone as instance interpret equation as applicable single hormone x pollutant case+study written publication contain separate section on uncertainty evaluation not include this section ground truth calculation workflow as think equation this section describe procedural aspect computation rather than domain knowledge include computation
comparison ground truth calculation workflow manually analyze agreement between node semi-automatic calculation model grammar%23NPR manually construct ground truth calculation workflow glacier case+study figure manually construct ground truth calculation workflow hormone case+study x correspond hormone t dht grammar%23NPR c alb grammar%23NPR grammar%23NPR bind protein k protein x correspond association constant particular protein hormone bind table size ground truth calculation workflow three case+study node edge glacier hormone pollutant 90+109 ground truth calculation workflow consider node variable agree when node label match variable name match if applicable cell range node match discretion step variable
determine recall by divide number match by number variable present ground truth calculation workflow determine precision by divide number match by number variable present semi-automatic calculation model
case+study result automatic aggregation all aggregation step reduce number node edge cell dependency graph table but grouping formula be by+far most effective aggregation step
total reduction differ case+study be large glacier case
all case study number node edge aggregated graph be big than number ground truth calculation workflow difference size be most obvious hormone pollutant case study
development application heuristics all case+study analyze potential cause difference size between aggregated graph ground truth calculation workflow use as basis heuristics achieve further reduction cell dependency graph table
categorize our observation three main theme i.e. disturb symmetry auxiliary calculation unit conversion first theme correspond layout spreadsheet table spreadsheet table usually design such+that content logically group arrange symmetric pattern pattern recognize by our algorithm use aggregation process however certain element spreadsheet table disturb original symmetric pattern for+example several spreadsheet pollutant case contain macro buttons merged cell cell influence coordinate spreadsheet cell as perceive by algorithm another example be that missing data+value hormone case copy equivalent region interrupt by empty spreadsheet cell cell result incomplete aggregation
second theme cover auxiliary calculation calculation be present spreadsheet i.e. calculation calculation be not directly related domain knowledge but rather calculation procedure calculation provide researcher additional information on calculation procedure usually calculation locate separate spreadsheet section be not part main flow computation hormone case for+example researcher introduce additional calculation evaluate their iterative approach pollutant case researcher use additional calculation evaluate uncertainty parameter value scoring procedure special case be derivation value constant parameter from empirical data calculation consider be related domain knowledge however calculation usually occur separate spreadsheet only resulting value use main flow computation therefore consider as auxiliary calculation
third theme deal unit conversion many spreadheets be necessary convert unit measure variable before use further calculation conversion result additional variable spreadsheet table in+fact still represent same quantity be no added+value calculation workflow for+example spreadsheet hormone case contain variable variable represent concentration hormone both gram mol percentage
manually apply heuristics on case+study spreadsheet run aggregation algorithm table both hormone pollutant case+study remove grammar%23NPR medical problem-to-medication figure show f1 score figure show precision recall medical task training set this experiment contain positive example 15%2C696 negative example test set contain positive example 12%2C257 negative example this experiment first notice that coverage grammar%23NPR grammar%23NPH knowledge+base be poor technique have f1 score figure show that grammar%23NPR direct lookup have high precision while grammar%23NPH knowledge+base double recall considerably low precision lsa have good performance reach maximum f1 measure precision recall most competitive technique be dre f1 score precision recall grammar%23NPR alone show improvement over all individual technique maximum f1 score reach precision recall when add grammar%23NPR feature dre performance relation extraction significantly improve reach maximum f1 score precision recall medical problem procedure figure show f1 score figure show precision recall medical problem procedure task training set this experiment contain 3%2C217 positive example 20%2C723 negative example test set contain 2%2C438 positive example 12%2C501 negative example grammar%23NPR have no coverage this task zero positive example be directly related grammar%23NPR grammar%23NPH resource also have very+little coverage f1 score precision recall maximum lsa f1 score this task be precision recall grammar%23NPR be strong baseline maximum f1 score precision recall grammar%23NPR feature be most informative this task maximum f1 score precision recall grammar%23NPR feature combine dre further improve recall f1 score precision recall medical task notice large performance improvement compare baseline when investigate relation extraction performance find that procedure name contain abbreviation other unusual figure f1 score at various threshold task figure p versus r task figure f1 score at various threshold task figure precision versus recall tasklanguage not usually find text prevent argument match corpus result poor feature set significantly low performance corpus based baseline compare medication task 6. related work closely related line research be area knowledge+base completion research this area focus on predict relation relation not explicitly mention knowledge+base but infer from additional fact already know this work be very similar sense that explore existing knowledge structured source infer new fact goal knowledge+base completion task be predict relation between entity comply semantics ontology often evaluation perform on ablated subset data from knowledge+base contrast grammar%23NPR drive by characteristic relation extraction task produce feature feature contribute performance recent work area knowledge+base completion include learn distributed representation concept relation from given instance relation between concepts introduce approach relational learning base on factorization three-way tensor represent entity relation knowledge+base learn word em bedding model entity their relation capture correlation among fact explicitly state knowledge+base be vast amount work on extend knowledge+base by parse external text corpus among many this work be complementary in+fact as show experimental section feature use by technique combine grammar%23NPR feature further improve their performance 7. conclusion large structured knowledge source hold great promise task require domain specific relation instance when semantics granularity knowledge source not align requirement particular task there still be implicit knowledge structured source be appropriate describe our task detect relation between concepts show that relation grammar%23NPR not align relation our task as expose by poor recall when use grammar%23NPR do direct relation lookup consider longer path relation grammar%23NPR not provide any improvement when use path lookup but by generalize path instance use as feature classifier be able achieve significant improvement over grammar%23NPR lookup approach as+well+as corpus based relation detection+system finally classifier incorporate both corpusbased feature mined path feature significantly outperform all other configuration 8. reference grammar%23NPR www.nlm.nih.gov%2Fresearch%2Fumls grammar%23NPR grammar%23NPH grammar%23ENT grammar%23NPR library large linear classification grammar%23NPH learn res. grammar%23NPH grammar%23NPH medical+relation extraction manifold model 52nd annual meeting grammar%23NPH grammar%23NPH domain restriction hypothesis term similarity semantic consistency grammar%23NPR page 131+138 grammar%23NPH cross document relation prediction use distributional semantics preparation grammar%23NPH grammar%23NPH learning hierarchical+structure linear relational em bedding nips page 857+864 grammar%23NPH grammar%23NPH three-way model collective learning on multirelational data proceedings 28th on machine+learning page 809+816 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPH translate embeddings modeling multirelational data advance page grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH connect language knowledge+base em bedding model relation extraction corr. grammar%23NPH knowledge graph text jointly embed proceedings grammar%23ENT conference on grammar%23NPR page grammar%23NPH grammar%23NPH grammar%23ENT reasoning neural tensor network knowledge+base completion advance page 926+934 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH novel use statistical parsing extract information from text proceeding first north+american grammar%23NPH page 226+233 grammar%23NPH shortest path dependency kernel relation extraction proceedings conference on human language technology page 724+731 grammar%23NPR min grammar%23NPH li wan grammar%23NPH grammar%23NPH distant supervision relation extraction incomplete knowledge+base proceedings conference north+american grammar%23NPH grammar%23NPR improve competency first-order basque+country grammar%23NPR grammar%23NPR javier.alvez%40ehu.eus basque+country grammar%23NPR grammar%23NPR paqui.lucio%40ehu.eus basque+country grammar%23NPR grammar%23NPR german.rigau%40ehu.eus abstract introduce new framework evaluate improve first-order grammar%23NPR ontology use automated theorem provers on basis competency question cq our framework include both adaptation methodology evaluate ontology framework first-order logic new set trivial cq design evaluate grammar%23NPR version sumo significantly extend very small set cq propose literature new cq automatically generate from small set pattern mapping wordnet sumo apply our framework demonstrate that grammar%23NPR sumo grammar%23NPR outperform tptp-sumo in+addition use feedback provide by set improved version grammar%23NPR this new version outperform previous terms competency for+instance automatically infer from grammar%23NPR sumo grammar%23NPR while be neither deducible from tptp-sumo
subject descriptor grammar%23NPR artificial+intelligence knowledge+representation formalism method general+term experimentation introduction ontology use wide range application knowledge based system like any other component system ontology require repetitive process refinement evaluation during development application lifecycle ontology evaluate by consider their use application when perform correct prediction on inferencing in+order+to enable good reasoning capability inferencing process be able deduce from ontology as+much correct implicit knowledge permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815841
possible author propose use set competency question cq evaluate ontology ontology be goal that ontology expect answer proposed methodology apply any formal ontology if there exist decision algorithm underlying logic
general process cq be not automatic but creative depend on size complexity ontology process create suitable set cq be by very challenging costly task
grammar%23NPR be currently most common formal knowledge+representation formalism be unable cope expressive ontology like grammar%23NPR dolce sumo fortunately state-of-the-art automatic theorem provers like vampire e be highly sophisticated system prove provide advanced reasoning support substantial conversion expressive ontology including first-orderized grammar%23NPR sumo tptp-sumo grammar%23NPR sumo despite preliminary experiment as far as know be no previous work work apply methodology first-order grammar%23NPR ontology use
contribution this paper be manyfold first following present new framework evaluate improve grammar%23NPR ontology use second introduce new set very large trivial cq design evaluate grammar%23NPR version sumo our set include creative cq development more than 7%2C000 automatically generated cq test our set creative cq extend question from grammar%23NPR grammar%23NPR problem domain grammar%23NPR grammar%23NPR question describe additionally also devise develop novel automatic procedure generate very large set trivial cq from small set conceptual pattern on basis knowledge encode wordnet grammar%23NPR mapping sumo third as result application our framework create new version grammar%23NPR sumo finally use our new framework cq carry+out empirical comparison existing grammar%23NPR version sumo
our experimental result new version grammar%23NPR sumo outperform tptp-sumo all previous version grammar%23NPR sumo for+example from grammar%23NPR infer that table not have brain but organism not be dead table eat not infer however yield opposite result from tptp-sumo grammar%23NPR sumo grammar%23NPR four case both new version grammar%23NPR table some figure about sumo tptp-sumo grammar%23NPR sumo sumo tptp-sumo grammar%23NPR sumo object 20%2C081 2%2C920 1%2C009 class 5%2C563 2%2C086 2%2C124 relation 369+208+208 attribute 2%2C153 total 28%2C166 5%2C282 3%2C407 sumo new set cq be freely obviously this type trivial inference be very useful wide range knowledge intensive application
help validate consistency associated semantic resource like grammar%23NPR derive new explicit knowledge from furthermore grammar%23NPR use world-wide anchor different type semantic resource wordnets many therefore similar inference obtain other semantic resource language other than english likewise grammar%23NPR connect several database such as grammar%23NPR grammar%23NPR grammar%23NPR thanks+to cloud initiative interlinking diverse database fully operational upper level ontology promise data enable machine more easily exploit content
next two section first introduce sumo grammar%23NPR version then our adaptation methodology propose by section illustrate process improve ontology ontology yield grammar%23NPR sumo grammar%23NPR by provide some example next process automatically obtain new set cq from grammar%23NPR mapping sumo describe section section report on competency tptp-sumo different version grammar%23NPR sumo last two section respectively provide some concluding remark discussion our future research line
first-order have origin ninety when group engineer from push formal ontology standard their goal be develop standard upper ontology promote data interoperability information+search retrieval automated inference natural+language+processing
express suo-kif be dialect both suo-kif use write grammar%23NPR formula but syntax go beyond fol. consequently sumo not directly use by without suitable transformation
exist different proposal convert large portion sumo into grammar%23NPR ontology author report some preliminary experimental result evaluate query timeout different option when translate sumo into grammar%23NPR evolved version translation describe found hereinafter grammar%23NPR use reengineering around 88%25 sumo obtain grammar%23NPR sumo both tptp-sumo grammar%23NPR adimen.si.ehu.es%2Fweb%2Fadimensumo grammar%23NPR www.globalwordnet.org grammar%23NPR www.ontologyportal.org grammar%23NPR www.tptp.org grammar%23NPR sumo inherit information from top middle level sumo from now on core sumo thus discard all information from domain ontology table provide some figure compare explicit content sumo tptp-sumo grammar%23NPR sumo
be easy see that most significant difference between tptp-sumo grammar%23NPR sumo be number object object be due fact that tptp-sumo introduce many instance instance infer from knowledge ontology
example practical inference capability grammar%23NPR grammar%23NPR sumo be cq boys be not domestic+animal instance grammar%23NPR boy not instance prove use next section also provide some cq cq not infer from neither tptp-sumo grammar%23NPR sumo
competency question grammar%23NPR ontology this section describe how adapt methodology introduce evaluate improve large complex grammar%23NPR ontology use state-of-the-art like vampire e
author propose evaluate expressiveness ontology by prove completeness theorem set cq proof completeness theorem require check whether given cq entail by ontology not this purpose use vampire grammar%23NPR work by refutation within given execution time+limit similarly also use e other that work by refutation
if conjecture entail by ontology then eventually find refutation give enough time space however theorem prove grammar%23NPR be very hard problem so be not reasonable expect find proof every entail conjecture thus if association+of+tennis+professionals be able find prove conjecture then know for+sure that corresponding cq entail by ontology however if association+of+tennis+professionals not find proof not know if conjecture not entail by ontology b although conjecture entail association+of+tennis+professionals not be able find proof within provided executiontime limit due semi- decidability problem grammar%23NPR increase execution time+limit be not solution grammar%23NPR this paper all axiom be consider universally close
www.vprover.orgjectures not entail same reason use other system system not work by refutation for+example by model generation be not general solution overcome this problem consider three possibility when test ontology given cq test be pass ii pass iii unknown as next describe
propose our method base on set cq write as conjecture language ontology set cq partition into two class truth test falsity test depend on whether expect conjecture entail by ontology not for+example let consider cq men not be pregnant organism not be dead instance human human attribute human pregnant not instance human instance grammar%23NPR organism not attribute according common+sense knowledge first cq be truth test whereas second be falsity test since organism be dead follow this division our method proceeds two steps first step deal set truth test as conjecture truth test classify as pass if association+of+tennis+professionals prove that corresponding conjecture entail by ontology classify as pass if association+of+tennis+professionals prove that conjecture not entail however as discuss above when no proof found not know whether conjecture entail not thus classify truth test as unknown second step deal set falsity test suppose not infer from ontology hence classify falsity test as pass when association+of+tennis+professionals prove that corresponding conjecture entail by ontology as unknown when association+of+tennis+professionals not find any proof
unknown truth test treat as pass since not be able prove that corresponding conjecture entail by ontology on+the+contrary unknown falsity test treat as pass when test classify as pass unknown case truth test proceed correct ontology correction be creative directed by problem at+hand case truth test not obtain any information test not classify as pass since find no proof thus correction be hard even not know if any correction require with+respect+to falsity test proof generate by use isolate controversial axiom perform additional test when necessary feedback provide by be very helpful detect modelling error as continuously evolve our method produce more precise useful outcome but semi- decidability problem grammar%23NPR still remain framework where underlying logic be decidable like grammar%23NPR our framework be also applicable advantage become exact
carry+out experiment describe this paper proceed as+follows first collect all creative query available from literature in+particular question from grammar%23NPR grammar%23NPR problem domain question describe then extend this reduced set cq new creative cq
creative cq classify manually as truth test old plus new cq falsitytests all new when apply our framework use this reduced set creative cq as dataset development as explain section improve competency ontology only consider result trace return by when prove cq this reduced set creative test second test competency new grammar%23NPR version sumo automatically create very large set cq derive from grammar%23NPR mapping sumo as describe section our framework use this large set more than 7%2C000 automatic cq as dataset test see section
improve first-order ontology this section report on experience improve grammar%23NPR ontology apply our framework set creative cq in+particular provide some example truth falsity test explain how use feedback provide by improve our ontology as result this process derive
discuss above section improve ontology when conjecture correspond truthtest not prove entail when prove that conjecture correspond falsity test entail former case get no more feedback from since no proof found hence have manually check ontology search modelling error latter case obtain proof proof include incorrect axiom roughly speak modelling error refer ontological concepts relation object that use define structure main feature ontology so-called basic concept concept serve describe knowledge knowledge contain ontology from now on basic concept for+example instance subclass disjoint partition attribute grammar%23NPR grammar%23NPR grammar%23NPR be basic concept grammar%23NPR sumo whereas grammar%23NPR animal be example basic concept according+to our experience be three typical type error than informally describe as+follows missing characterization modelling error refer basic concept c but ontology lack axiomatization c. be simple case since solution consist axiomatize c
too weak characterization modelling error refer basic concept c characterize but way too weak this case solution consist repair characterization update one more axiom c
unsuitable characterization basic concept modelling error refer basic concept c be well characterize terms some basic concept b but b be not suitable defined be most grammar%23NPR path mining structured knowledge grammar%23NPR research height new+york united+states+of+america mabornea%40us.ibm.com grammar%23NPH grammar%23NPR research height new+york united+states+of+america kjbarker%40us.ibm.com abstract large source structured knowledge be available many domain enable construction application require relational knowledge but in+spite+of apparent availability relational content semantics granularity source not always match requirement specific task yet even when coverage explicit relational knowledge source seem inadequate there be implicit knowledge complete space relation instance this paper show that explicit relation instance be insufficient our task detect relation between concepts but by mine relational path between pair concept training set then use generalization path as feature classifier achieve good result on relation detection task than use state-of-the-art corpus based relation extractor further show that by combine mined path based feature feature use corpus based extractor achieve even good performance category subject descriptor grammar%23NPR artificial+intelligence learning knowledge+acquisition grammar%23NPR artificial+intelligence knowledge+representation formalism method semantic+network general+term algorithm experimentation keyword semantic+relation relation detection structured knowledge exploitation structured medical knowledge knowledge mining 1. introduction relational knowledge be important content many task information extraction question+answering knowledge discovery be application relational knowledge be essential example information needs relational knowledge include example direct movie jaw jaws film director d%29 conference be new+york c conference c location grammar%23NPR drug treat anemia d drug anemia d%29 satisfy such information needs intelligent system typically rely on large knowledge repository relevant domain knowledge acquire through natural+language+processing nlp technique over large corpus also be curated by domain community both form knowledge capture have limitation limitation affect performance on target task corpus based technique produce noisy knowledge graph semantic granularity match textual expression relational knowledge but not match require given task for+example text mention test splenic+fever but particular task be dependent on knowledge+base for+example require more fine-grained representation such as test splenic+fever most glaring weakness curated knowledge+base on+the+other+hand traditionally be poor coverage accordingly interested exploit curated knowledge long believe that if only have enough structured domain knowledge repository use directly solve information needs our intelligent system relational knowledge for+example simply look+up knowledge+base several domain such as biomedicine large curated knowledge repository be now available yet find that be still lacking intelligent task use many case seem just+as noisy as automatically extract source grain their representation be just+as inappropriate semantics be just+as vague this paper describe new approach relation discovery from structured knowledge+base show that our knowledge+base appear have poor coverage desired semantic+relation but that this weakness be due flawed expectation that task specific relation knowledge+base relation align semantically by mining long relational path use path as feature classifier achieve good coverage our test set related entity pair also show that by combine feature mine from curated source permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815840features from corpus based relation extraction system achieve good result than either approach alone problem domain particular task interest our work determine semantic+relation among entity data be collection structured unstructured medical information correspond particular patient this data include among other thing patient medical problem medication medical procedure relation extraction task be part large summarization project that aim produce summary grammar%23NPR many case relationship between medication medical problem treat procedure medical problem target be not explicit relation extraction technique require make relationship explicit use case particularly challenge existing textbased relation extraction technique because grammar%23NPR medication procedure often not appear same sentence as medical problem treat target in+fact while medical problem often mention free text portion medication procedure often give separate table this co-location problem not be issue if simply look+up relational knowledge structured knowledge+base one popular such structured source be although grammar%23NPR be rich source relational knowledge including relation link medication procedure problem have poor coverage on our data address weakness both text-based knowledgebased relation extraction on our problem use known entity pair medication medical problem pair procedure medical problem pair as training data mine relational path path then use as feature train logistic classifier each relation resulting classifier significantly outperform simple knowledge+base lookup baseline as+well+as classifier base on corpus derived feature achieve good performance on our test+data however by combine feature from both approach 2. architecture our work use system base on general-purpose learning architecture as represent figure system allow determine relation between pair term using supervised learning supervised relation extraction algorithm require training step our setup training data+set contain positive negative example pair term annotate given set relation e.g. diagnosis cause treat training set figure contain sample treat relation instance during training phase system build model given relation training phase complete system use relation extraction by apply desired relation model pair term output system be score indicate probability that particular relation apply figure test pair be simvastatin cholesterol score generate treat relation be construction our dataset describe in+detail section number positive negative training test example each evaluation task appear section train relation model use logistic regression be many machine+learning classification algorithm algorithm use our task including support vector machine neural+network current relation extraction experiment focus primarily on feature+extraction process intend experiment different machine+learning algorithm as future work experiment report this paper use grammar%23NPR open+source library large linear classification task handle millions instance feature grammar%23NPR support logistic regression linear support vector machine provide wide range classification algorithm our classifier build use l1 regularization c coordinate descent algorithm as solver address relation extraction task system incorporate multiple feature generator component grammar%23NPR grammar%23NPR technique propose this paper have role feature generator feature generate by grammar%23NPR derive from mining structured data knowledge+base grammar%23NPR coverage go beyond entity be directly related structured source significantly increase recall compare direct lookup compare feature extract from large corpus 3. relational mine direct lookup relation contain ten millions relation instance medical domain structured resource this size have adequate relational coverage use simple lookup given task check that possibility ask domain expert identify relations that map relation interest our use case this paper focus on two class relation relation between medication medical problem relation between medical problem procedure mapping between our task relation relation appear table figure architecture supervised learning based relation extraction grammar%23NPR first task be determine how medicationmedical problem pair procedure medical problem pair our training set appear related directly by relation table positive pair our training set existence relation instance target relation type count as true positive if no relation instance found positive pair count false+negative target relation hold negative pair our training set indicate false+positive problem data+set this direct lookup give precision recall formula+one score procedure-tomedical problem data+set positive example cover by target relations this exploration lead conclude that our task direct lookup relations not be adequate indirect relation path our second experiment determine relational coverage our task be consider path relation be no reason assume that granularity semantics relation match relation our use case be be possible that semantics intend capture between medication medical problem procedure medical problem not match single relation as assume by direct mapping table focus first on problem relation compute all path connect procedure medical problem pair our training set through maximum three relation result show that such relation path link 72%25 positive example pair training set suggest that there be path content improve recall task here be example relational path length two two relation appear between medical procedure medical problem our test set relation type simplify readability argument concept represent by their grammar%23NPR grammar%23NPR analyze grammar%23NPR grammar%23NPR example show positive pair from our training set medical procedure grammar%23NPR contrast medical problem grammar%23NPR sinusitis be no single relation directly connect concepts however this example show be connection between two concept through relational path procedure analyse body+part grammar%23NPR nasal sinus this body+part be finding site medical problem use path relation extraction new argument pair variabilize argument result generalized path analyze c2 compactness simplicity represent generalized path remainder paper as concatenation their relation type name only generalize path such as this also appear between negative example be equally be counter indication procedure medical problem relation following argument also connect by our example generalize path grammar%23NPR analyze grammar%23NPR grammar%23NPR this case medical procedure grammar%23NPR abdomen analyze body+part grammar%23NPR abdomen be finding site medical problem grammar%23NPR reflux this case however x-ray not use treat diagnose reflux total 8%2C416 distinct generalized path appear among example training set medical procedure task length generalized path be limited three relation also experiment path length up four relation this case notice that number generalized path training set increase significantly 54%2C141 path but path length four have little impact performance task among 8%2C416 path procedure task path have length one direct relation have length two 8%2C057 have length three medication task encounter 10%2C563 unique generalized path training set path length one path length two 9%2C987 length three use generalized path relation extraction need determine path more correlate ground truth one possible way estimate that correlation be by computing precision recall f score each path compute precision recall formula+one score each path then explore different threshold filter set path table mapping between relation domain task relation relation relation problem acetaminophen fever aspirin myocardial+infarction medical melanosarcoma furuncle drainage boil skin benign adenoma removal parathyroid adenoma alopecia skin biopsy area alopeciamaximize formula+one score subset path for+example set formula+one threshold at reject all path individual grammar%23NPR be below then compute precision recall formula+one score remaining path as set over our test+data give positive example pair concept if above threshold path hold between concept consider related pair count as true positive negative pair abovethreshold path hold count as false+positive then compute formula+one score whole set above threshold path perform multiple experiment where vary threshold parameter between obtain good result among all experiment formula+one score details appear section this experiment confirm that simply use subset path as predictor relation perform poorly mined relational path as feature show previous experiment look+at individual path not significantly improve performance over direct lookup single relations next design experiment detect if be any signal when consider all path compute use pair training set capture this notion use generalized path as feature create classifier refer herein as grammar%23NPR grammar%23NPR each relation give pair concept classifier produce score represent probability that concepts relate given pair concept feature set contain all generalized path path appear between two concept figure show example feature medication medical problem pair grammar%23NPR grammar%23NPR simvastatin hypercholesterolemia use all path between positive negative example as feature significantly increase performance relation extraction result formula+one score detail section not all be strong tendency inspect generalized path assume semantics base on intuition suggestive label relation type reinforce this tendency for+example tempt look+at generalized path such as d s p conclude that make+sense if some drug treat symptom that symptom associate medical problem accept that as intuitive re formulation knowledge that drug treat medical problem figure computing path feature given test pair find relational path between two concept variabilize argument produce generalized path use path as feature classifier train over all positive negative pairssimilarly tempt dismiss path such as c2 as be vacuous based on relation name as expose by our experiment intuition make too strong assumption about semantics relation base on their label consistency their use source contribute our mining approach attempt uncover actual use pattern relational path with+respect+to training data resulting generalized path score as useful extent discriminate our positive negative training example furthermore generalized path path consistently indicate absence target relation be as useful as indicate presence generalized path show this paper be negatively informative in+spite+of their assumed semantics 4. experimental methodology dataset description purpose our task be relate patient medical problem clinical data do so apply relation extraction detect medication medical problem association proceduremedical problem association some relation extraction technique require offline training phase where model build each relation at runtime relation model apply each pair entity obtain score indicate probability that entity relate when score relation exceed threshold medical problem conclude associate medication procedure summary one important characteristic distinguish from traditional relation extraction be that relation not explicitly mention text medication medical procedure represent table while medical problem mention various text section our data+set this task build from grammar%23NPR divide into grammar%23NPR training grammar%23NPR test grammar%23NPR come from variety patient encounter including inpatient clinic hospital visit each data+set present domain expert table row correspond medical problem identify column correspond medication domain expert mark cell table if treatment relation be possible between medical problem medication annotator present similar table medical problem vs. procedure on+average be medical problem medication procedure annotator spend on average grammar%23ENT annotate each task cell mark by expert be positive example medication medical problem pair procedure medical problem pair our data+set not label be negative example result report this section compute over aggregated argument pair all test grammar%23NPR our annotation task perform by two independent annotator annotator be medical+school graduate at+least one year practical clinical training disagreement rate be 10%25 disagreement resolve by consensus baseline technique compare grammar%23NPR three baseline approach use both knowledge+base statistical method first observe that direct lookup relation have low performance this task implement baseline system base on map our target relation into relation as show table if positive target pair entity be explicitly related though relation then consider true positive if positive example pair be not directly related be false+negative similarly negative example produce either false+positive if be directly related true negative if be not this method achieve good precision but have very low recall as detailed section improve recall also collect database relation extract from text run statistical relation detection+system on large medical corpus grammar%23ENT result 1%2C093%2C003 instance our target relation call this resource grammar%23NPH use relation lookup same way as accomplish relation extraction task as expect this approach have high recall at expense low precision also implement baseline relation extraction system base on lsa grammar%23NPR inspire by domain restriction hypothesis this system corpus based lsa similarity between test pair known argument pair use rank with+respect+to likelihood be argument target relation this baseline allow evaluate precision recall trade-off result much large coverage overall good performance compare two previous approach finally compare relation extraction technique name similar lsa system detect relation by measure corpus based distributional similarity between test entity pair training pair also make use feature derive from domain ontology such as semantic type argument entity enable relation extract even case where term not mention same sentence even document this technique have high performance among considered baseline evaluate grammar%23NPR two configuration first use supervised technique create model including only grammar%23NPR feature evaluation section below refer this technique as grammar%23NPR also use grammar%23NPR combination dre feature call this technique grammar%23NPR dre 5. evaluation this section present detail experimental evaluation relation extraction perform experiment medical task one experiment medical task each experiment run all technique describe section using data describe section evaluate performance each system compute precision recall f1 score illustrate precision recall tradeoff experiment different value threshold use pair score above threshold classify as positive example relation only show precision recall tradeoff grammar%23NPR dre grammar%23NPR lsa be not possible demonstrate precision recall tradeoff direct lookup grammar%23NPH technique since output relation extraction case be binary based on existence instance grammar%23NPR n-ary predicate quality event have type dul event since fred perform word+sense disambiguation entity link some entity resulting graph link external source grammar%23NPR grammar%23NPR property fred graph divide two macro category role role role outgo edge from event node all other edge be non- role edge role edge broadly classify into agentive passive oblique role
further information on fred semantics please refer wit.istc.cnr.it%2Fstlab-tools%2Ffred
knowledge reconciliation present method reconcile knowledge extract from text use fred main issue reconcile two fred graph consist detect node two graph correspond same entity
next section define graph alignment based method solve problem define above graph first compress by merge node remove unnecessary label two compressed graph align by establish correspondence between node first graph node second graph maximise score function score function combine similarity between aligned node similarity between aligned properties maximise score function have effect align node node have high similarity in+turn connect edge high similarity therefore both node edge similarity structural information consider at end aligned node map individual original graph grammar%23NPR relation add between aligned node if node compressed graph correspond more than one individual original graph pick at+random
compression graph compression aggregate cluster node in+order+to obtain abstract graph less more informative node this step be necessary two reason first same entity represent by different equivalent node collapse all equivalent node reduce number cross graph association find increase their quality second enable aggregate type information node therefore increase amount information help associate node graph
edge similarity some similarity measure node edge use by optimizer define alignment score function similarity be positive negative element element have negative similarity tend not associate while element positive similarity tend associate note that alignment algorithm perform global optimisation hence local part alignment penalise favour global reward for+instance two edge positive similarity not align because imply align their endpoint node negative similarity similarly two node negative similarity align enable aligning incident edge positive similarity
similarity distinguish among three kind node pair relevant compatible incompatible first check if both node refer named entity if so check whether refer same named entity different label named entity compare by string matching by their alignment public resource grammar%23NPR if label be equal associate same grammar%23NPR entity pair node consider relevant otherwise consider incompatible if two node not refer named entity check similarity all cross- node pair label remind reader that node have more+than one label see if node share equivalent similar concept label similarity compute by similarity use grammar%23NPR if corresponding entity be same kind otherwise be zero if two node share same label refer word similarity high than threshold our experimental evaluation two node consider compatible all other case pair node consider incompatible node similarity assign as follow on relevant pair on compatible pair on incompatible pair
similarity similarity between two edge define terms their type specifically distinguish between compatible incompatible edge base on their property type possibly their thematic role if both edge be non- role edge consider compatible if both edge be role edge consider compatible only if their role be both agentive grammar%23NPR passive grammar%23NPR all other case edge consider incompatible edge similarity set compatible edge incompatible edge
once similarity among node edge define our problem define terms graph alignment problem graph alignment be widely studied problem have many application several field formulate as quadratic assignment problem reduce grammar%23NPR problem formulation adopt here design specifically directed multi graph pair node connect by more than one edge be similar other previously proposed formulation compute optimal alignment be np hard problem hence no polynomial time algorithm know however since size knowledge graph generate from text be not very high this kind graph be usually sparse standard optimization technique be affordable reduce our problem grammar%23NPR grammar%23NPR use standard solver optimization grammar%23NPR optimizer often converge proved optimal+solution on small medium problem instance provide good approximation proved error bounds on larger instance our experiment optimal+solution find fraction second most case large problem instance be possible apply known efficient heuristics change slight lose quality
experimental analysis implement reconciliation method as grammar%23NPR use solve integer linear program
no compete approach benchmark evaluation available evaluate our method by analogy against existing benchmark related problem do that use part grammar%23NPR gold+standard cluster
be extension european+central+bank corpus annotate event coreference coreference also contain entity coreference grammar%23NPR omit tool url version paper keep anonymity table performance our reconciliation tool resolve crossdocument coreference metric recall precision formula+one grammar%23ENT dev. grammar%23ENT dev. grammar%23ENT dev.
50.03%25 72.75%25 58.03%25 grammar%23NPR 37.88%25 66.22%25 46.31%25 grammar%23NPR 49.58%25 69.75%25 56.24%25 grammar%23NPR 36.23%25 46.14%25 36.89%25 blanc 46.79%25 77.07%25 56.73%25 notation since interest reconcile pair document align pair document from corpus all possible way evaluate result each pair pair total evaluation compare cluster mention generate by our tool cluster mention from ground truth restrict document under+consideration remove singleton cluster cluster contain only mention correctly match corresponding mention consider mention match ground truth mention if first entirely include second one use this procedure instead exact matching because mention produce by our tool often refer head+word few important word express corresponding entity fred graph while mention gold+standard include long sentence
compute precision recall formula+one score several metrics report tab. average value standard+deviation on all pair reconciled document
tool be able perform 72.75%25 precision on+average 50.03%25 recall grammar%23NPR measure consider difficulty cross document coreference task result promise note that state-of-the-art tool achieve 75.9%25 precision 61.2%25 recall on grammar%23NPR dataset however that tool specifically design while our tool be more general perform reconciliation over abstract representation knowledge contain multiple text document be just possible application furthermore our method provide knowledge graph as output directly use as machine+readable data
conclusion present novel method generate integrate knowledge graph extract from multiple text document our tool rely on fred machine reading tool generate abstract representation text document integrate generated knowledge by+means+of optimization technique graph alignment assess performance our tool by analogy resolve crossdocument co- reference result show that our method be effective ongoing work focus on build specific gold+standard semantic reconciliation problem perform large-scale evaluation
reference robust entity cluster via phylogenetic inference
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH abstract meaning representation sembanking
unsupervised event coreference resolution rich linguistic feature 48th page grammar%23ENT grammar%23ENT grammar%23ENT
resolve event noun+phrase their verbal mention grammar%23NPR page 872+881
cross document co- reference resolution use sample based clustering knowledge enrichment transaction page
ontology matching
grammar%23NPR 2nd edition
new graph based method pairwise global network alignment grammar%23NPR bioinformatics grammar%23NPR grammar%23NPR
improve tree kernel based event pronoun resolution competitive information editor grammar%23NPH page grammar%23ENT grammar%23ENT
grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH sigma simple greedy matching align large knowledge+base in grammar%23NPR grammar%23NPR page 572+580 new+york new+york united+states+of+america association+for+computing+machinery
grammar%23NPH joint entity event coreference resolution document grammar%23NPR grammar%23NPR grammar%23NPR page 489+500 grammar%23NPR united+states+of+america
global alignment data+mining system biology method molecular+biology series
grammar%23NPH c
document coreference resolution use latent feature grammar%23NPR linked data information extraction at grammar%23NPR grammar%23ENT
stream cross document entity coreference resolution 23rd int.
on computational+linguistics grammar%23NPR page grammar%23NPR united+states+of+america
v. grammar%23NPH grammar%23NPR semantic similarity toolkit conference+system demonstration page 163+168 grammar%23NPR
large-scale cross document coreference using distributed inference hierarchical+model 49th grammar%23NPR page 793+803 grammar%23NPR united+states+of+america
probabilistic alignment relation instance schema grammar%23NPR vldb grammar%23NPR 157+168 grammar%23ENT
grammar%23NPH grammar%23NPH v. grammar%23NPR
grammar%23NPH grammar%23NPH fast approximate quadratic programming graph matching grammar%23NPR one
ontology learn from text look back into future
grammar%23NPR grammar%23NPR grammar%23ENT grammar%23ENT handling uncertainty case+study on tennis+tournament result extraction from tweet jochemverburg%40gmail.com grammar%23NPH m.b.habib%40ewi.utwente.nl m.vankeulen%40utwente.nl abstract involve different type uncertainty due imperfection extraction tool inherent ambiguity unstructured text this paper discuss several way handle uncertainty from social+media our study case be extract tennis game result two grand+slam tennis+tournament from tweet analysis do find extent be useful use semantic+web domain knowledge fact repetition author trustworthiness improve certainty relation
introduction 500%2C000%2C000 tweet send every+day make use this vast amount information require extract structured information out this heterogeneous unstructured information however from tweet challenge due various source uncertainty in+addition+to error take+place during process information contain user contribution be often partial subject evolution over time conflict other source sometimes untrustworthy require handle uncertainty involve fact this paper investigate different source uncertainty propose method improve certainty relation validate our method use case+study where result two grand+slam tennis+tournament extract from tweet
case description this paper use two self-collected tweet dataset about french+open grammar%23ENT grammar%23NPR grammar%23ENT aim extract game result tennis domain be useful as case+study several reason first due popularity large+number tweet post during big tournament furthermore mentioned entity cover grammar%23NPR grammar%23NPR like grammar%23NPR grammar%23NPH give opportunity not only extract disambiguate entity but also extract defeat relation like this case large-scale automatic analysis validation be possible since ground truth relation be available last permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c association+for+computing+machinery isbn grammar%23ENT grammar%23ENT
dx.doi.org%2F10.1145%2F2815833.2816960 figure number tweet day each tournament be domain have advantage over other sport domain like football match tweet about just few times for+example et+al. have problem their event detection less popular game
dataset collect from use keywordsearch rolandgarros grammar%23NPR tweet all language collect but only english consider our research
major difficulty from tweet be that short message often lack context however as have closed domain safely assume that if mention tennis+player extract link directly entity tennis+player without spend much effort on disambiguation process usefulness hashtags already test by adedoyin-olowe et+al.
grammar%23NPH grammar%23NPR keyword search be in+essence not different dataset grammar%23NPR dataset contain 2%2C683%2C270 584%2C400 english tweet respectively distribution find figure
extraction method use modular approach module use each step replace by another implementation approach module be b c
first grammar%23ENT use extract name entity type person from tweet alternative alternative focus on grammar%23ENT tweet be available however prefer grammar%23ENT due efficiency for+example system grammar%23NPH et+al. take grammar%23ENT tweet on set 10%2C000 tweet whereas grammar%23ENT need only grammar%23ENT tweet grammar%23ENT use do more analysis on large scale output grammar%23ENT module use as input module as explain later this section
semantic+web grammar%23NPR use link disambiguate ne belong specific subclass for+example male participant be subclass person subclass generate by make set entity use list name do by take all entity classify as superclass then check partial match name entity subclass link also make more normalization possible since identify entity replace by their uri uri be method et+al. still have difficulty by use grammar%23NPR no training data manually added ground truth need extract ne specific domain contrast grammar%23ENT method propose by grammar%23NPH et+al. still need new training method test two data+source grammar%23NPR grammar%23NPR grammar%23NPH
include section check how use different knowledge source influence result
process domain specific kb construct by match player name take from corresponding website grammar%23NPR grammar%23NPR entry tennis+player kb
this domain specific kb use match name text if name person be part name entity consider as match this method not look+at other entity grammar%23NPR since assume that data+set restrict domain enough mean if there be entry kb grammar%23NPR be football+player grammar%23NPR text our tweet still link tennis+player entry
last step be defeat do both men woman singles regular expression use extraction due simplicity first regular expression make match extract person entity against player name tweet expression then expression use find all defeat relation
grammar%23NPR grammar%23NPR entity grammar%23NPR grammar%23NPR beat defeat def. win grammar%23NPR evaluation do by compare relation against ground truth game result there all player also automatically link entry tennis+player
initial result use grammar%23NPR find table maximum result be upper+bound result not exceed found by check every result ground truth whether both player mention together at+least tweet lead maximum possible recall maximum precision be theoretically future work comparison re method also validate effectiveness this method
assumption make that tweet this case dataset use training used source extraction method not need any additional training hence whole dataset use test
handle uncertainty uncertainty come from both data+source extraction method cause at every level for+example grammar%23NPH grammar%23NPH mention that error output grammar%23ENT module propagate module if tennis+player not recognize as person by grammar%23ENT module then not be grammar%23NPR dbpedia.org grammar%23NPR www.wimbledon.com grammar%23NPR www.atpworldtour.com%2Fshare event-draws.aspx year grammar%23ENT grammar%23NPR draw ms table initial result precision recall formula+one measure max. french+open max. grammar%23NPR figure formula+one measure mutual+exclusion analyze further on if link name wrong player then re contain mistake lastly method also extract wrong relation
research already do on grammar%23ENT tweet technique re also research
here focus on analysis uncertainty cause by data+source future research want investigate uncertainty result from extraction method
mutual+exclusiveness inspire by dong et+al. model mutual+exclusion fact improve quality if relation type player def. player grammar%23NPH player b def. player be both only result most frequent take take in+case have same frequency figure show formula+one measure each tournament when result filter on mutual+exclusion formula+one measure general go up since precision rise result be more likely be correct good note that recall drop at at grammar%23NPR formula+one measure increase 10.8%25 grammar%23NPR increase 6.4%25
mutual+exclusion work as at each tournament player not meet once however throughout their career meet multiple times therefore future research do take+into+account time constraint
fact repetition noisiness informal language widely use tweet increase uncertainty information first limited length character force author use abbreviation abbreviation lead lack context by limit dataset only tweet certain hashtag keyword context easily establish second different author have different writing+style
conservative re method lead low recall since grammar%23NPR formula+one measure tweet top n author difficult make regular expression match all writing+style less conservative method lead low precision though since extract more false+positive for+example our regex extract relation grammar%23NPR def grammar%23NPH from text grammar%23NPR beat by grammar%23NPH therefore be useful check whether be factor factor lead more precision
grammar%23NPR grammar%23NPH et+al. grammar%23NPR et+al. mention that repetition fact give more confidence result check whether filter result on only mention n times more lead good precision figure show influence different threshold on formula+one measure
threshold get high precision increase while recall drop be because some result some match at early round tweet few times hence throw their result away lead low recall formula+one measure give combination precision recall figure show show grammar%23NPR optimal threshold be increase 5.1%25 formula+one measure drop quickly afterwards at peak be at threshold increase 8.4%25 stay bit constant before drop this difference show that good threshold differ every case future research need do find automatic way find optimal threshold check whether be correlation between size corpus threshold dataset be large also have high optimal threshold
trusted author since no verify whether tweet be true be difficult say whether author be trustworthy therefore do some analysis check whether most tweet account be more trustworthy than also want check whether some specific account journalist for+example provide true information this analysis be disregard mutual+exclusion rule since bias result figure show show low threshold consider more relation thus improvement result from use mutual+exclusion be more significant
this analysis tweet top n most tweet author consider analysis do check effect contribution top author if only their tweet use extraction completely exclude from dataset figure show formula+one measure each n. by consider only tweet top n author good result peak achieve at n n grammar%23NPR at this peak result improve by 33%25 result grammar%23NPR improve by 21%25 this peak formula+one result consider n author slowly go+down until become similar value including all author figure limit n but be author be good note that line tweet top n author only decrease gradually mean that result analyze tweet large crowd barely influence by tweet small group author wisdom crowd
analysis individual author show large difference within top n author as+well give lot result high precision some almost our analysis good author high formula+one show that author use standard+format probably robot account beside author be times re tweet author seem be regular people tweet out interest not sports professional journalist since be substantial overlap author two dataset further research do find+out whether existence trustworthy author be generalized as+a+whole furthermore be also useful research whether most trustworthy author have certain characteristic common such as have high number follower have official account
semantic+web our proposed extraction approach use semantic+web task grammar%23NPR have advantage that have lot information available aid like this case nick name all player problem be that be difficult analyze truth fact dong et+al. do some analysis on amount truthful fact different grammar%23NPR dataset but not show trustworthiness each source interest see effect different source on be that certain data+source lead better result our case grammar%23NPR grammar%23NPH analyze have initial idea influence grammar%23NPR on
find nick name first list participant link entry not all player find no qualitative+analysis make whether correct entry match leave analysis result extraction grammar%23NPR out player play both men woman tournament found at out found at grammar%23NPR whereas use grammar%23NPH all player at found out player at grammar%23NPR figure show consequence
be that formula+one measure be high grammar%23NPR because both precision recall grammar%23NPR be high whereas grammar%23NPH more player name link quick analysis link show that correct link establish
result automatically be only possible if grammar%23NPR grammar%23NPH link future research need find+out origin difference use multiple grammar%23NPR source together lead even better result result show that different grammar%23NPR source lead some degree uncertainty result
combination previous experiment show that uncertainty reduce by use domain knowledge ensure a+number+of repetition take+into+account mosttweeting author use specific knowledge+base test run find good combination factor mutual+exclusion seem be beneficial all case when take only most tweet author n good result achieve at threshold minimum number repetition t grammar%23NPR show good combination find result be clear enough find this combination manually however future work grammar%23NPR query interpretation generate use context concepts co-occurring token along single token keyword query use find direct relation all keyword query relate via properties axiom relevant concept indirect relation find via named class expression found equivalent subclass axiom
formally define direct indirect relation further this section
name annotation value class grammar%23NPR represent as l grammar%23NPR as define section slight abuse notation use name grammar%23NPR name entity denote names entity appear property axiom relevant concept grammar%23NPR k k be two partitioning subset keyword query where k be set co-occurring and%2For single term query find use co-occurrence measure present section k be set token query be not k k q k cosine similarity measure use by match l grammar%23NPR k function search concepts grammar%23NPR where l grammar%23NPR match k similarly match name grammar%23NPR k use same similarity measure find concepts grammar%23NPR where name grammar%23NPR k match
relation direct relation among query token be function l grammar%23NPR name grammar%23NPR direct relation among token k k be dr k k match l grammar%23NPR k match name grammar%23NPR k indirect relation indirect relation form via relation among token query via properties other concept equivalent class grammar%23NPR concept not directly appear keyword query but indirectly via grammar%23NPR grammar%23NPR axiom indirect relation among token k k give by grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR k k grammar%23NPR grammar%23NPR match l grammar%23NPR k grammar%23NPR grammar%23NPR match name grammar%23NPR k grammar%23NPR grammar%23NPR collect all other class class be indirectly related grammar%23NPR through either grammar%23NPR grammar%23NPR axiom grammar%23NPR k k similarly construct from class grammar%23NPR grammar%23NPR
direct indirect relation enable effective during search process class match via direct relation have classname related context word query indirect relation use extract class name not directly appear query
search result ranking search result rank by ranking model ranking model build use algorithm supervise machine+learning algorithm design build ranking model training data generate from querylogs feature vector grammar%23NPR generate each combination query result querylog component such grammar%23NPR rank feature ranking feature be similarity value between query all context axiom properties concept training data feature vector then use build model base on ranking model learn weight feature use ranking model model then apply get score all pair query concept search result
score sort obtain top- k ranked search result
evaluation benchmark dataset compare our system search function grammar%23NPR large widely used biomedical ontology repository our experiment large portion ontology openly available total download from grammar%23NPR together ontology contain 2%2C062%2C080 class our index maintain term frequency all term as+well+as co-occurring token require co-occurrence statistics frequency further use calculation lexical co-occurrence term query single co-occurring token query find use grammar%23NPR grammar%23ENT statistics input our procedure
first use find direct relation among query token further use extract result indirect relation found via equivalent class subclass-of axiom
evaluation measure real user query from grammar%23NPR querylog grammar%23ENT grammar%23ENT use evaluate our framework more than 50%25 query be multiple token query use 2%2C000 query evaluate our search result standard ir ranking measure use evaluation vis-a-vis grammar%23NPR performance grammar%23NPR querylog consist query clickedposition click ontology id concept id first select query click information be available then select query click ontology be openly available so that evaluation be fair ontology such as grammar%23NPR be not openly available final set query use evaluation be 2%2C173 use standard ir measure compare our search result vis-a-vis grammar%23NPR
result analysis discussion our result indicate good performance multitoken query when query keyword relate each other
user enter keyword keyword relate each other query token have high co-occurrence value and%2For have direct indirect relation among use by our approach interpret query understand intent user average grammar%23NPR grammar%23NPR our approach be grammar%23NPR be respectively multitoken query also evaluate system single token query average grammar%23NPR grammar%23NPR value our approach be grammar%23NPR be respectively our result find be statistically significant 95%25 confidence+level use grammar%23NPR signed rank test outperform grammar%23NPR especially multitoken query evaluate difference between grammar%23NPR value each query difference calculate between grammar%23NPR value use our approach use grammar%23NPR same set query please refer figure detailed analysis grammar%23NPR value query 50%25 grammar%23NPR people.cs.umass.edu vdang grammar%23NPR bioportal.bioontology.org%2Fare better our result depict good overall performance
than query 35%25 have same level performance
good performance be query where token query relate detect use our co-occurrence procedure direct indirect relation based context driven approach number query negative difference be 15%25 where grammar%23NPR perform good attribute ontology popularity consider while ranking
grammar%23NPR query grammar%23NPR difference grammar%23NPR grammar%23NPR positive difference indicate good performance figure grammar%23NPR difference conclusion future work search right concept large ontology repository be challenging task this paper present novel concept search approach approach incorporate three major technique context each concept extract by property axiom relevant concept query interpretation base on context ranking search result use learn rank algorithm our comprehensive evaluation evaluation involve more+than 2%2C000 query 2%2C062%2C080 concept show multi token query our approach outperform grammar%23NPR search function on two widely used ir performance measure grammar%23NPR grammar%23NPR future plan evaluate our search system use human based evaluation compare our performance other related keyword based concept search system in+addition+to grammar%23NPR system also plan evaluate our system without learning rank model
design more feature such as ontology popularity further improve performance our ranking model
thank prof. grammar%23NPH grammar%23NPH stanford+university share grammar%23NPR querylog
reference ranking ontology aktiverank grammar%23NPR page springer grammar%23NPR
grammar%23NPH information content based ranking metric linked open vocabulary proceedings 10th international conference on semantics grammar%23ENT grammar%23ENT grammar%23ENT page
ontology search empirical evaluation semantic+web grammar%23ENT page 130+147 springer
grammar%23NPH falcon searching browsing entity on semantic+web proceedings 17th international conference on page new+york new+york united+states+of+america association+for+computing+machinery
grammar%23NPR more than semantic+web search+engine semantic+web
find rank knowledge on semantic+web proceedings 4th page 156+170
grammar%23NPH grammar%23NPH s. a
grammar%23NPR search metadata engine semantic+web proceedings on information knowledge+management page 652+659 association+for+computing+machinery press
link open data essential quick start guide edition mono monochrom
grammar%23NPR ranking semantic+web data by tensor decomposition
grammar%23NPR scalable ranking method semantic+web data context 2nd workshop on knowledge+base system
search browse link data swse semantic+web search+engine semantics science service agent on grammar%23NPR special+issue on semantic search
optimize search+engine use clickthrough data proceeding eighth association+for+computing+machinery grammar%23NPR international conference on knowledge discovery data+mining grammar%23NPR page 133+142 new+york new+york united+states+of+america association+for+computing+machinery
authoritative source hyperlinked environment association+for+computing+machinery

query answer over owl ontology proceedings 8th on semantic+web research application volume part grammar%23NPR page 382+396 springer grammar%23NPR
grammar%23NPR efficient construction data catalogue by stream based indexing linked data
short introduction learning rank grammar%23NPR transactions grammar%23NPR grammar%23ENT grammar%23ENT
t. y grammar%23NPR learn rank information+retrieval found trend inf. grammar%23NPR 225+331 grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
grammar%23NPH get lucky ontology search data driven evaluation framework ontology ranking
semantic+web 12th new+south+wales grammar%23ENT grammar%23ENT proceeding part page 444+459
grammar%23NPH
pagerank citation ranking bringing order
falcon concept search practical search+engine ontology institute+of+electrical+and+electronics+engineers transactions on system man cybernetics part 810+816
learning joint query interpretation response ranking proceedings grammar%23NPR international conference on page republic canton
conference steering+committee
top- k exploration query candidate efficient keyword search on graph shape rdf data proceedings grammar%23ENT on data engineering grammar%23NPR page 405+416 washington+dc district+of+columbia united+states+of+america
grammar%23NPH sindice.com weave open linked data grammar%23NPR page 552+565 grammar%23ENT reduce search space using clustering technique topic identification grammar%23NPH grammar%23NPR computer+engineering computer+science swedish e- science research+center abstract current challenge be scalability one technique deal this issue be reduce search space generation map suggestion
this paper develop method prune that search space by use clustering technique topic identification further provide experiment show that be able generate partition partition allow high+quality alignment highly reduced effort computation validation map suggestion parts ontology partition other technique still need find mapping mapping be not partition
knowledge+representation data+mining introduction recent year many ontology develop contain overlapping information often want use multiple ontology for+instance company want use community standard ontology conjunction company specific ontology application need use ontology from different area from different view on one area case be important know relationship between concept relation different ontology further data different source within same domain annotate different but similar ontology knowledge inter ontology relationship will this case lead improvement search integration analysis data address this major issue much research recently devote i.e. finding mapping between concept relation different ontology e.g. research field be very active own yearly workshop as+well+as yearly event grammar%23NPR make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright association+for+computing+machinery isbn grammar%23ENT grammar%23ENT
dx.doi.org%2F10.1145%2F2815833.2816959
grammar%23NPR e.g. focus on evaluate system system automatically generate mapping suggestion many system build overview found e.g. at ontology+matching web+site www.ontologymatching.org
recent work e.g. define challenge challenge need address when deal large ontology
scalibility be important challenge as show by fact that many participant grammar%23NPR have performance problem when deal large ontology one technique deal scalabiblity be reduce search space generation map suggestion instead deal all concept pair c2 belong first ontology c2 second ontology only subset pair consider each such subset call mappable part reduce search space lead reduced computation during generation map suggestion
expectation be that map suggestion compute therefore validation effort domain expert expert decide whether mapping suggestion be correct not reduce
previous approach reduce search space focus on segment partition ontology use initial alignment for+instance subgraphs concept good coverage initial mapping select initial alignment use partition ontology base on their structure ontology partition use variant rock exploit link measure base on structural linguistic similarity between concept approach selection mappable parts different ontology base on existence mapping part some approach use locality anchor concept mapping i.e. descendant ancestor neighbor reduce search space
selection concept base on sub schema
selection mappable sub schema base on similarity concept e.g. their name sub schema
this paper describe method prune search space not require initial alignment as other approach focus on align concepts do by use clustering technique topic identification before actual generation map suggestion section further method demonstrate set experiment section use different clustering technique system on ontology anatomy track grammar%23NPR show that be able gener-a l g n m e n t o n t o l o g e s general dictionary instance corpus domain thesaurus matcher matcher matcher checker conflict user ii accept suggestion reject filter combination suggestion mapping figure existing framework extension
partition partition allow high+quality alignment while highly reduce effort computation validation map suggestion parts ontology partition other technique still need find mapping mapping be not partition finally summary future work give section
background from knowledge+representation point+of+view ontology contain concept relation axiom instance concept relation often organize hierarchy use is- a relation task be create alignment between ontology i.e. set mapping between entity from different ontology
most common kind mapping be equivalence mapping mapping use is- a inverse
large+number system develop base on computation similarity value between entity two input ontology describe as instantiation framework figure part computation map suggestion contain different component
component use modify original ontology e.g. extract specific feature concept ontology reduce search space find mapping suggestion this specific component be focus this paper matching component include one several several matcher matcher calculate similarity between entity from different ontology mapping suggestion then determine by combine filter result generate by one more matcher by use different preprocess match combine filter technique obtain different alignment strategy result part be set map suggestion part grammar%23NPR mapping suggestion present domain expert expert accept reject accepted mapping suggestion be part final alignment any sub- set final alignment be acceptance rejection suggestion also influence further suggestion further conflict checker mapping debug system use avoid conflict introduce by mapping suggestion there be several iteration part ii
technique k mean be partitional clustering technique attempt find user specified number cluster k represent by their centroid algorithm guess initial centroid populate k cluster by assign each data+item close centroid
each centroid update base on item assign cluster assignment update step repeat until be no change cluster configuration
good value k be not easy but parameter sensitivity+analysis help possible measure accuracy final cluster be silhouette index base on intra- cluster cohesion inter cluster separation algorithm use euclidean distance accuracy derive from sum squared error sse base on distance between data+item their centroid sse min approach value k minimize sse value select while sse max approach maximize marginal decrease sse curve
grammar%23NPR be density based algorithm where density define by count number data+item within specified radius data+item algorithm gather data+item data+item be density connect same cluster essentially two data+item be density connect if can reach from same data+item by following chain data+item dense region
workflow present general workflow generation mappable part two ontology detailed algorithm describe section
data+collection preprocess phase ontology represent suitable format cluster cluster+analysis phase clustering algorithm run on output previous phase sensitivity+analysis use select good parameter value clustering algorithm text+mining preprocess phase cluster treat as document use label concept cluster process obtain suitable format next phase topic identification phase identify main topic cluster document cluster from same ontology merge into larger cluster base on their topic finally mappable part generation phase cluster from different ontology connect into mappable part use topic
experiment setup experiment use following setup
use ontology anatomy track grammar%23NPR contain ontology anatomy part grammar%23NPR american+medical+association contain concept grammar%23NPR contain concept give 9%2C026%2C626 potential concept pairing further reference alignment grammar%23ENT equivalence mapping be available
represent ontology as binary symmetric+matrix where mi j if concept concept j relate via is- a regardless+of level otherwise cluster use both k mean grammar%23NPR as similarity measure use grammar%23NPR similarity k mean cosine similarity grammar%23NPR k mean sensitivity+analysis use several technique silhouette index grammar%23NPR sse max text+mining preprocess phase tokenized concept label each cluster convert into lower+case remove stop word perform truncated token long than character
base on resulting token cluster represent use tf-idf vector contain list grammar%23NPR topic mappable part
cluster grammar%23NPR cluster keyword artery american+medical+association artery blood vessel trunk grammar%23NPR digit common superior grammar%23NPR artery type artery anterior circumflex internal external superior articular branch inferior bone american+medical+association joint bone vertebra digit foot grammar%23NPR phalanx grammar%23NPR grammar%23NPR grammar%23NPR carpal grammar%23NPR rib grammar%23NPR hand grammar%23NPR vertebra cartilage rib bone head phalanx foot digit hand hair part grammar%23NPR follicle hair stratum consider keyword cluster topic identification phase define base on keyword set topic appropriate granularity ontology current experiment this phase perform manually cluster same ontology same topic merge then relate topic two ontology obtain mappable part
each approach investigate influence on performance existing system sambo system system traditionally perform well track as+well+as all participating system track grammar%23ENT use standard measure from grammar%23NPR precision be ratio number found correct mapping total number map suggestion recall be ratio number found correct mapping total number correct mapping f measure be harmonic+mean precision recall also compute how+much search space prune
result report separately clustering technique grammar%23NPR k mean where k determine base on sse min km-sse-min sse max km-sse-max silhouette index grammar%23NPR generate cluster keyword topic mappable part
show some topic mappable part generate use grammar%23NPR topic artery consist original cluster from american+medical+association from grammar%23NPR most common keyword part also show topic hair part only appear grammar%23NPR thus not contain mappable part grammar%23NPR mappable part generate table sse min sse max grammar%23NPR
quality alignment table show result grammar%23NPR combine sambo topic related mappable part as reference projection full reference alignment on each mappable part projection contain only concept pair full reference alignment both concept include mappable part table show result cluster algorithm combine one variant each system when consider all topic table each measure pair value give where compute with+respect+to full reference alignment second with+respect+to projection full reference alignment on mappable part result f measure other variant system clustering technique give+in
show first column how many concept comparison need perform by system cluster algorithm number comparison reduce small fraction 1.8%25 2.9%25 potential pairing
number map suggestion validate reduce fraction range from 11%25 43%25 depend on system specific cluster technique
result topic related mappable part grammar%23NPR combine sambo
artery bone digestive epithelium head heart lymphatic muscle nervous reproductive respiratory tissue vein discussion approach allow partition ontology use mappable part ontology system will perfom well find similar behavior system general precision recall system be often good mappable part than on whole ontology see table expected disadvantage be that as fewer suggestion generate recall compare full reference alignment be low however when compare computation validation effort proposed approach found region where amount effort lead much improved return table computation effort when use mappable part reduce by at+least 97%25 all system while validation effort reduce by at+least 60%25
our experiment km-sse-max always require few comparison generate few mapping suggestion
also have low recall compare full reference alignment between 16%25 18%25 recall system grammar%23NPR always require most computation 264%2C536 pair generate most mapping suggestion each system between 28%25 43%25 suggestion originally generate by system also have high recall compare full reference alignment between 30%25 43%25 recall system although not always with+respect+to mappable part only
critical part topic generation be decide on original cluster merge e.g. table preliminary experiment very small cluster lead bad result current experiment be manual step rely on domain expert prot%C3%A9g%C3%A9 plugin grammar%23NPR decision+making grammar%23NPH grammar%23NPH grammar%23NPH erman christian informatik.uni-mannheim.de abstract this paper introduce grammar%23NPR plugin grammar%23NPR help grammar%23NPR be possible solve multi attribute decision+making problem encode straight forward extension standard formalism allow specify background+knowledge term ontology while each attribute represent as weighted class expression on top such approach compute good choice good k choice take background+knowledge into account appropriate way show how implement approach on top existing semantic+web technology demonstrate benefit help interesting use case illustrate how convert existing resource into expert+system help grammar%23NPR
subject descriptor artificial+intelligence knowledge+representation reasoning information+systems application decision+support+system expert+system data+description+language semantic+web description language ontology language owl general+term theory economics keyword utility theory decision+making introduction study preference decision+support+system be ongoing research subject artificial+intelligence gain more popularity every+day since first attention multi attribute utility theory numerous approach propose including probabilistic possibilistic fuzzy graphical model amongst one recent permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor
credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright hold by owner author s publication right licensed association+for+computing+machinery association+for+computing+machinery grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816957 approach step+forward be use logical language e.g. encode decision theoretic problem
follow this line research introduce grammar%23NPR plugin grammar%23NPR encode decision+making problem semantic+web language owl grammar%23NPR use standard reasoning technique perform non-standard reasoning task ranking choice with+respect+to set weighted attribute specify by user in+this+regard think as decision+support+system plugin use ontology as background+knowledge subset individual appear this ontology define as possible choice
base on multi attribute utility theoretic assessment yield ranking choice each attribute represent by class description weight by utility value assert by user yield compact representation user preference over attribute then preference relation lift set choice via aggregation attribute that choice satisfy due their class membership
theoretical underpinning grammar%23NPR weight logic e.g. see in+particular grammar%23NPR base on weighted framework refer as dl decision base not require any specific dl language provide flexibility sense that use tractable fragment e.g. grammar%23NPR family el if scalability be important expressive fragment when require by domain that need model
our approach support data+type be desired property since use numeric domain be common literature decision+theory see this work briefly present part theoretical framework propose introduce our grammar%23NPR plugin grammar%23NPR base on this formalism
understand as generic out-of-the-box expert+system turn ontology specific domain into powerful decision+support+system domain describe by that ontology such expert+system apply support decision+making various domain within this paper present use case illustrate how use grammar%23NPR generate reading proposal this use case also demonstrate that be very easy apply grammar%23NPR any kind domain ontological representation be available as knowledge+base
introduce theoretical foundation our framework section section first give general description our plugin then present use case case base on excerpt from grammar%23NPR deal book grammar%23NPR wiki.dbpedia.org%2Fauthors finally conclude section
theoretical foundation this section introduce theoretical underpinning our plugin be framework on weighted our aim be use priori preference relation over attribute ontological class derive posteriori preference relation over choice ontological individual
introduce utility+function u define over set attribute x while define pending utility+function u over choice derive from u
represent each attribute original decision+making problem as class be named class complex class description define vocabulary ontoloy assume that total transitive preference relation i.e. x over x give as+well+as function u x r r represent i.e. u grammar%23NPR u iff grammar%23NPR x function u be weight function assign weight class x grammar%23NPR denote utility class x x by u x u reflect agent preference relation over set attribute grammar%23NPR greater utility attribute more preferable attribute be
partition attribute set x into two subset desirable desirable be set attribute negative weight denote x undesirable x be set attribute negative weight
call ni as set named individual let now c ni denote finite+set choice in+order+to derive preference relation posteriori over c i.e. c respect x introduce utility+function u c c measure utility choice c relative attribute set x utility+function u over attribute as aggregator simplicity abuse notation use symbol both choice set attribute whenever be obvious from context following define particular u call utility
utility choice give consistent knowledge+base k set choice c utility choice c c be c grammar%23NPR x c u x
be easy see that induce preference relation over c i.e. grammar%23NPR grammar%23NPR also notice that each choice correspond set attribute membership logically entail i.e. k x c now introduce notion generic grammar%23NPR denote u as+follows
grammar%23NPR grammar%23NPR be pair u u u where u be utility+function define over x u be utility+function define over c
grammar%23NPR u encode user preference u define their aggregation via u define utility choice next put thing together introduce notion decision base interpret as model artificial agent decision situation decision base be triple consist dl knowledge+base k finite+set available choice c encoded user preference along utility+function choice grammar%23NPR u
decision base decision base be quadruple d k c u where k t be consistent knowledge+base t be acyclic grammar%23NPR i grammar%23NPR c ni be set choice u u u be grammar%23NPR
role k be provide assertional information about choice at+hand along general terminological knowledge information that agent require reason further over choice this work assume commonly used rationality criterion criterion be rational agent always pick+up choice s maximum utility
system description implement our approach as grammar%23NPR plugin available at https%3A%2F%2Fcode.google.com%2Fp%2Fudecide first briefly describe functionality architecture our plugin section then present use case case show how user interact plugin section this use case also illustrate how plugin use as out-of-thebox expert+system any knowledge+domain available as ontology
implementation our grammar%23NPR plugin be compatible both version as reasoning component use grammar%23NPH reasoner turn+out be good owl reasoner our purpose regard performance issue
choice motivate by evaluation result report at dl.kr.org%2Fore2014%2Fresults.html.2 grammar%23NPR require grammar%23NPH run background connect via grammar%23NPR
implementation be straight forward first ontology need load via standard grammar%23NPR file menu
ontology act as knowledge+base switch grammar%23NPR tab user specify set possible choice by specify class c define grammar%23NPH instance c treat as choice choice correspond set choice c theoretical framework attribute their utility then be specified on top vocabulary define type choice attribute their corresponding utility specify connection grammar%23NPH establish via http illustrate step subsequent section more detail each attribute request from reasoner name individual satisfy intersection attribute class expression class class define type choice
result show consist ranked list all individual return by at+least one query their utility utility derive from satisfied attribute since grammar%23NPH not support instance satisfaction query anonymous class expression create temporary ontology ontology transfer grammar%23NPH on calculation time merge knowledge+base ontology add this ontology equivalent class axiom between each utility assertion class expression named dummy class then separately query individual name dummy class
as describe on our homepage recommend configure grammar%23NPR load knowledge+base already on start-up speed+up calculation because+of increasing computation time memory limitation require do when work large knowledge+base if knowledge+base already load into grammar%23NPR on start-up only very small temporary ontology need transfer grammar%23NPR otherwise union both temporary grammar%23NPR like thank grammar%23NPH support related using grammar%23NPR potentially very big knowledge+base transfer
use use case apply our approach domain book author in+particular use our framework support user find interesting author by specify interests as attribute instead work artificial example use existing subset grammar%23NPR deal chosen topic core domain contain relevant information about book their author with+respect+to our use case grammar%23NPR suffer from restricted set terminological axiom incompleteness sparse usage some property overcome problem decide extend core domain information about city in+particular add each city list as birth death place author country locate furthermore add some axiom specify nationality class e.g. spanish define as class person be+born die city locate nationality be
by use nationality class nationality author no nationality object property assertion exist still infer by their birth death place
that nationality specify directly only 21.3%25 author while 46.9%25 have derived nationality via our axiomatization obviously there exist some people people be+born nationality be not spanish
since final choice make by user gain coverage be more useful than loss precision
extension illustrate that context reasoning based approach be possible leverage background+knowledge background+knowledge seem not be relevant at+first+sight information locate that some author be+born thus affect ranking choice if specify that prefer spanish author as attribute also show that reasoning based approach help overcome some problem relate incomplete data knowledge+base dataset some instruction on how use found at https%3A%2F%2Fcode.google.com%2Fp udecide wiki grammar%23NPR
that user want find new author author write book book be similar one user like first+of+all feasible choice define as instance class dbp author figure depict screenshot grammar%23NPR tab class choice belong specify respective text field upper right corner
arbitrary concept description be specified as+long+as be signature previously loaded ontology
suppose that our user like author grammar%23NPH
like read author that influence by grammar%23NPH express by positive weight attach attribute grammar%23NPH attribute specify by user see grammar%23NPR tab on left side figure note that concept description specify grammar%23NPR support by grammar%23NPR editor all attribute specify within dialog+box use auto complete functionality grammar%23NPR as+well+as syntax check capability only if class expression be syntactically correct button enable add grammar%23NPR
www.w3.org%2Ftr%2Fowl2-manchestersyntax%2F%23the_grammar overall nine attribute specify first three attribute express that user prefer author that influence by favorite author by add negative value fourth attribute user ensure that three author user already know rank low ranking choice fifth attribute add increase utility author receive some award by moreover user specify that user like author write book book belong genre horror fiction science+fiction attribute have relatively low utility value finally be specified that user like american british author slightly prefer british
result result finally calculate only include individual individual satisfy choice class expression at+least attribute this calculation start by click on button ranked choice present on right side figure descending order base on their utility good choice be author grammar%23NPH follow by grammar%23NPH low ranked choice thus most reasonable choice user be look+at author grammar%23NPH more detail give that attribute specification underlying knowledge+base be complete correct however often be case that user want explore result more detail for+example get explanation about their ranking position do by click on proposed choice figure illustrate grammar%23NPH utility score base on fact that grammar%23NPH influence by grammar%23NPH by grammar%23NPH that win at+least one award that be+born new+york therefore classify as american satisfied attribute highlight left panel furthermore all assertion about selected choice show panel low right corner again use grammar%23NPR default present this information vice+versa be also possible select one multiple attribute result choice highlight that satisfy selected select attribute s not show figure
use case presented example illustrate both benefit as+well+as some drawback our approach first+of+all apply our grammar%23NPR plugin directly domain book author without need any further modification extension result expert+system make proposal about interesting author book only required ingredient be ontology cover domain appropriate way decide use grammar%23NPR feature comprehensive grammar%23NPR but flat inexpressive grammar%23NPR thus potential reasoning capability our approach have only limited impact with+respect+to our use case majority attribute whether not hold decide by direct look-up
give one example nationality class illustrate how overcome limitation by add additional axiom relevant data from other domain cover grammar%23NPR
chosen use case be extreme case where have very large grammar%23NPR comprehensive set choice be not know in+advance agent while have relatively inexpressive grammar%23NPR on+the+other+hand there be case where have only limited number choice together+with rich logical axiomatization relevant background+knowledge our tool design support both case as+well+as any kind hybrid scenario figure screenshot grammar%23NPR show ranked list author accord attribute specification
conclusion further work this paper present grammar%23NPR plugin grammar%23NPR
compute utility set choice by aggregate utility value satisfy attribute since each attribute correspond class description standard reasoning technique use check whether attribute satisfy use grammar%23NPR reasoning system conduct required reasoning task result this computation present user as ranked list choice our knowledge introduce first system system allow encode solve multiattribute decision problem term weighted description logic
implement our approach as grammar%23NPR plugin our approach easily use by grammar%23NPH community
demonstrate within our use case how use grammar%23NPR as expert+system recommend new author user moreover also show that our current implementation by use reasoning system grammar%23NPR be capable deal large real-world dataset currently investigate dataset from biomedical domain from domain life+science as direction future research aim extend our framework plugin deal uncertainty e.g. be uncertain particular degree that author born be spanish person lifting utility choice expected utility
reference multi attribute decision+making use weighted description logic a. y grammar%23NPR editor grammar%23NPH volume proceeding page ceur-ws.org
push el envelope grammar%23NPR page 364+369
grammar%23NPH grammar%23NPH grammar%23NPH tractable reasoning efficient query answer description logic grammar%23NPH lite family reasoning 385+429
grammar%23NPH expressive power weighted propositional formula cardinal preference modelling grammar%23ENT
work preference less be more
grammar%23NPR
multi- criterion decision+making theory application recent development

decision multiple objective preference value tradeoff new+york
grammar%23NPH decision+theory principle approach grammar%23NPR
grammar%23NPH grammar%23NPH weighted description logic preference formula multiattribute negotiation proceeding scalable uncertainty management cma+2071
grammar%23NPH grammar%23NPR system description semantics science service agent on grammar%23ENT cloud grammar%23NPR data knowledge+management multi- centre biomedical studies grammar%23NPR grammar%23NPR amtsafara trifon spiros zervakis abstract among basic+research tool bio medical+science be epidemiological studies typically involve a+number+of hospital clinic research+centre scatter around world often refer as multi centre study clearly effectiveness importance multi centre study increase number participate centre enrolled patient but at+the+same+time this natural distribution production research data require sophisticated data knowledge+management infrastructure support participating unit this kind infrastructure be not only expensive build maintain but also not reuse as often tailor specific study
this work present cloud based system system allow user without any computer+science background design deploy administer platform aim manage share analyse clinical data from multi centre study proposed system provide zero administration zero cost online data knowledge+management tool that enhance re- usability by introduce study template ii support bio medical needs through specialised data+type able capture specialised knowledge like repeated therapy treatment iii emphasise data filtering export through expressive yet simple graphical query engine
introduction large-scale epidemiological studies typically involve a+number+of different stakeholder including hospital clinic research+centre physically distribute around world often refer as multi centre study studies be invaluable as collect large amount data from different region correlate draw useful conclusion on important research question however physical distribution participant asynchronous nature data+acquisition pose a+number+of issue including collection organisation processing data
support by european+union greek funds through grammar%23NPR project national strategic reference framework research funding program grammar%23NPH
make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT new+york united+states+of+america
association+for+computing+machinery isbn grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816949 grammar%23ENT
tackle data knowledge fragmentation address issue arise from coordination geographically distributed participant a+number+of platform that focus on storage management bio medical data knowledge propose however platform either design specific task study be thus unusable any other study require significant computing infrastructure expert information+technology it setup tuning typically reconfigure existing platform another study set+up one up from+scratch result time-consuming meeting between scientist different principle try understand+each+other be needs ii resource consuming it infrastructure infrastructure require outsource it specialist regular maintenance upgrade keep+up technological requirement due issue great number multi centre study lack resource still perform by resort manual procedure such as collect data on paper exchange data by post either as hard+copy as electronic copy store removable media email enormous often outdated spreadsheet file often sensitive patient data among participant therefore concern like data freshness integrity participant coordination degree involvement timeliness result lose between version exchanged spreadsheet hard+copy patient data unanswered email request
this work present cloud based service manage share organise clinical patient data from multi centre study proposed system cover all functional requirements pose by multi centre study enable researcher easily organise share data knowledge generate by research activity propose innovative integrated framework create platform multi centre study enable user no prior it knowledge design launch easy transparent way platform tailor specific needs their studies ii perform basic advanced+user management task manage user assign user privilege permission perform access+control on data iii record organise manage clinical patient data by resort a+number+of built-in customisable data+entry form iv search filter information by use powerful yet simple pointand click mechanism pose restriction on data extract requested information a+number+of format output including raw+data pie column chart spreadsheet due cloud infrastructure computational resource allocate on+demand provide elasticity fault+tolerance way be transparent end-user
contribution this work be twofold figure platform creation editing
propose cloud base zero cost zero administration tool tool offer both fundamental advanced+user data knowledge+management functionality multi centre study best our knowledge be first cloud based system focus on multi centre study allow user deploy their own platform within minute alleviate need rely on expensive custommade solution solution require it infrastructure skill maintain
present architectural consideration solution behind proposed tool describe a+number+of novel service service allow user without any prior it knowledge create administer launch use personalise platform
system be currently beta+testing multi centre study lead by university+hospital grammar%23NPR already use by more than public hospital
rest paper organise as+follows section describe implemented service functionality outline system architecture describe demonstration plan present at conference while section discuss related work
system overview our system+support three different type user correspond three data access+privileges more specifically have it administrator ita be responsible update maintenance system have access edit delete filter all data database ii study administrator sa be typically in+charge ongoing study access all data relate specific study iii participant ongoing study access only data add by
supported functionality question platform question be backbone multi centre study support set question particular multi centre study introduce concept platform technically platform be custom-made set question together+with all necessary user administration data knowledge component study platform typically create administer by sa more specifically sa be able design launch new platform by input platform name define a+number+of study question b rearrange delete question through drag-and-drop action each question sa specify three key element data+type question text question values this process perform design tool figure our system+support all standard figure platform branching logic
such as string integer date decimal etc. additionally support title multiple choice lists complex data+type
use introduce new questionnaire section
choice restrict possible answer fixed set
allow data input from dynamic custom-made dropdown menu that sa dynamically create stores edit during platform design maintenance lists share different study same user use ensure data consistency enhance data+integrity enforce validation input define new list sa specify unique name define content subsequently when specify question sa set data+type list select drop-down lists for+example figure third question specified be user-defined list hospital participate study
complex datatypes support medical operation operation involve group recurring question as treatment plan therapeutic protocol usually consist set medicine periodically recorded data e.g. name start end date outcome advantage create complex data+type include good data model knowledge capture thus rich query possibility flexibility design study complex repeat medical processes
logic sa also specify branching logic question this functionality allow sa specify values question that require enable disable following for+example figure sa specify that if answer question received drug id be yes question drug name dosage id id respectively enable
typically specific part study use as be minor modification other study
demographic data be typical resuable part many biomedical multi centre study since hardly change among different studies support this reusability system offer sa possibility create platform template template use different studies figure show platform template mean demographic data
existing template involve two different scenario simple editing editing include add delete modify question their branching logic ii structural editing editing relate modification question order data+type such edit affect consistency data knowledge cause compatibility problem between data knowledge new template be thus limited action not cause such issue
figure sa not allow change patient name integer figure example template demographic data
filtering tool
chart tool sa be able extract information from study while participant only extract information add by such information retrieve present as set tuple use filtering tool ii graph use chart tool
filtering tool figure use powerful yet easy-touse query issue mechanism mechanism allow user filter retrieve ii export third-party application store record by constraint simple pointand click interaction data filter export involve two-step process first step user define query output by check question question use data projection i.e. data export second step user apply one more filtering condition on data filtering condition introduce by present all distinct value store specific question allow user define one one satisfy filtering criteria this way user define conjunction disjunction both on question on data query result then export spreadsheet present as list tuple figure sa select export result hospital clinic patient name notice checked box first column all record be input between grammar%23ENT grammar%23ENT grammar%23NPR grammar%23NPR hospital involve patient between year age notice specified constraint
filtering tool be able capture complex data+type describe earlier allow user set filter every complex data+type ii include construct like concurrent episode diagnosis treatment
supported query include show patient older than year age temperature greater than patient diagnose microorganism notice that name microorganism be value already store database
show patient between year age patient live+in diagnose figure use chart tool two variable
salmonella grammar%23NPR undertake therapy ampicillin cefotaxime have progress as eventual outcome their condition notice application boolean+operator both on question on data store each question
show patient patient suffer from massive hemoptysis have initial episode bacteremia accept dose between grammar%23ENT grammar%23ENT antibiotic amikacin subsequently have second episode bacteremia notice that query apply complex data+type recurring episode treatment as describe above
chart tool figure use extract present information from data directly as pie column bar donut chart support creation single multiple variable graph depend on constraint define by user this way user dynamically create graph query single variable like how many patient database be male female but also query multiple variable like how many patient be male female each hospital participate study show figure
system architecture main idea system be allow user design build platform through series simple adaptive processes do transparently through simpleto follow wizard from user without any grammar%23NPR train while cloud based architecture automatically adapt resource infrastructure by rely on cloud elasticity
cloud functionality provide by open source platform grammar%23NPR owncloud.org setup over medium-sized computing infrastructure available at university campus figure present high-level view system architecture different type module implement be responsible perform all necessary communication grammar%23NPR platform provide elasticity service while storage manager perform all necessary storage retrieval operation data knowledge+base backend our backend implementation use lamp framework as backend infrastructure while rest module develop use php grammar%23NPR
study manager module be responsible creation edit management studies consist a+number+of module utilise manage participant data associate study ii filter extract data request by sa platform grammar%23NPR high-level view architecture
use create edit manage platform template utilise by different studies user manager module utilise by grammar%23NPH create manage sa also by sa create manage participant their role specific study security access+control module enforce security policy system control access+privileges over data security feature include certificate password based authentication single sign-on policy role based user management finally user+interface module be responsible identify hardware use connect sytem pc tablet smart+phone adjust viewing component accordingly
poster presentation also plan demonstrate functionality system at conference
interested reader find more information about project at www.uop.gr trifon grammar%23NPR
related work over years many solution aim at management sharing bio medical information propose follow focus on approach relate system specifically design multi centre study proposed system focus on specific study put+forward architecture service tailor problem at+hand present information+system information+system use manage multi centre study cancer similarly grammar%23NPR introduce platform collect prospective data on patient multiple+sclerosis while present system hiv aids prevention treatment finally a+number+of popular+revolutionary+army system platform e.g. grammar%23NPR www.hicdep.org project e.g. cascade www.ctu.mrc.ac.uk%2Fcascade grammar%23NPR www.cobred.eu also launch support different type focused multi centre study ii clinical biomedical data+integration
large+number existing specialised system needs dictate by each different multi centre study lead researcher design system be able support class functionality need multi centre study most prominent paradigm this line+of+work be redcap project grammar%23NPR system redcap provide principled way design construct manage database database be able support multi centre study however deploy system specific study study coordinator need get+in+touch redcap project inform grammar%23NPR specialist on specific needs requirement study at+hand subsequently iterative possibly long process refinement correction database design redcap grammar%23NPR expert deploy database user be able enter data obviously any subsequent change specification result redesign database porting inserted data new database on+the+other+hand grammar%23NPR system while support online creation study questionnaire offer limited data+type e.g. not offer custom drop-down lists complex data+type ii no data export functionality e.g. pie column chart spreadsheet grammar%23NPR compatible output iii query engine limited expressiveness iv run on dedicated hardware no adaptation policy e.g. elasticity resource low quality+of+service guarantee
reference grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
electronic patient record on steroid distribute peer-to-peer secure privacy conscious vldb
et+al. global approach management emr electronic medical+record patient hiv aid experience dream software
medical informatics decision+making
et+al. grammar%23NPR international online registry platform collaborative outcome research multiple+sclerosis multiple+sclerosis
et+al. information+system medical+record support hiv treatment rural
p.a. grammar%23NPH grammar%23NPH research electronic+data+capture redcap metadata driven methodology workflow process provide translational research informatics biomedical informatics
grammar%23NPH grammar%23NPH grammar%23NPH flexible database platform biomedical research multiple user+interface universal query engine db be
grammar%23NPH grammar%23NPH integrated platform analyze molecular biological data within clinical study grammar%23NPR workshop
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR lightweight mobile health record grammar%23NPR
grammar%23NPH grammar%23NPH
cloud en abled information+appliance healthcare
grammar%23NPR
grammar%23NPH jaish+muhammad grammar%23NPR grammar%23NPR semantic integration data information+system multicenter epidemiological studies on cancer grammar%23NPR
grammar%23NPH grammar%23NPH
challenge integration biomedical data+set vldb
grammar%23NPH grammar%23NPH grammar%23NPH context sensitive clinical data+integration grammar%23NPR workshop grammar%23ENT semantic reconciliation knowledge extract from text through novel machine reader grammar%23NPH misael.mongiovi%40istc.cnr.it grammar%23NPH diego.reforgiato%40istc.cnr.it grammar%23NPH aldo.gangemi%40istc.cnr.it grammar%23NPH valentina.presutti%40istc.cnr.it grammar%23NPH andrea.nuzzolese%40istc.cnr.it grammar%23NPH sergio.consoli%40istc.cnr.it semantic technology lab istc-cnr grammar%23NPR abstract this paper describe novel method generate integrate knowledge graph extract from multiple natural+language source by grammar%23NPH machine reading tool generate abstract representation text document be key problem human robot speak dialogue interaction issue issue arise from current research project relate active healthy ageing using caring service robot where involve problem be also relevant many application scenario require creation dynamic evolution knowledge+base such as automatic news summarisation solve this problem require solve sub- task task only study individually so+far propose holistic approach handle grammar%23NPR graph relate different input text output knowledge graph represent reconciled knowledge
semantic reconciliation knowledge integration machine read paper present novel method integrate knowledge extract from multiple natural+language grammar%23NPR source into integrated formal representation our tool rely on grammar%23NPR machine reading tool generate abstract representation text document integrate generated grammar%23NPR research lead result receive fund from european+union horizon grammar%23ENT framework programme research innovation grammar%23ENT grammar%23ENT grant agreement project grammar%23NPH managing active healthy aging use care service robot
wit.istc.cnr.it%2Fstlab-tools%2Ffred permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor
credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright hold by owner author s publication right licensed association+for+computing+machinery association+for+computing+machinery grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816945 knowledge by+means+of optimization technique graph alignment grammar%23NPH be tool extract knowledge from text represent by well-connected rdf graph formal semantic interpretation grammar%23NPH link knowledge existing linked data ontology so provide entity centric grounding unstructured data their relation as+well knowledge related different input text refer same concepts events name entity etc. reconcile grammar%23NPR knowledge graph need identify when entity events person organisation concept etc. present different text refer same one
problem refer as semantic reconciliation be relevant most application scenario require create update evolve knowledge+base from multiple and%2For dynamic grammar%23NPR source for+example support humanmachine dialogue context assistive robotics by collect user personal+memory personal+memory provide by grammar%23NPR input over time build integrated knowledge view e.g. summary about specific event by analyse news
active healthy ageing use care service grammar%23NPR challenge represent by lexical semantic communication interaction person robot need be able communicate human on natural+language basis as+well+as detect interpret express emotional expression react such interaction behaviour behaviour adapt evolve dependently on environment live+in robot learn evolve thanks+to specific human+relationship but also exploit general encyclopaedic background+knowledge from semantic+web moreover share learn other robot hence create knowledgesharing virtuous cycle cycle during time make more more cultivate at their starting phase as+well+as beyond do so problem that need address be related identification reconciliation entity when lie different part dialogue
that our machine reader fred be general-purpose semantic reconciliation build on top work not just robot human but any domain grammar%23NPR research project grammar%23NPH www
news social+network chat comment etc.
consider following news from two different source tony award curious incident big winner on broadway big night win tony award good musical ideal scenario goal be automatically produce integrated knowledge grammar%23NPR such as depict figure solve problem semantic reconciliation grammar%23NPR integrated formal representation result from knowledge acquire from two different news about link between text segment knowledge need preserve
semantic parsing multiple natural+language text transform formal representation identify common vs. different part in+order+to reason over integrated knowledge graph associate textual provenance on+the+one+hand problem very challenge consider that natural+language use very heterogeneous form express similar knowledge on+the+other+hand solve be crucial in+order+to fully exploit knowledge produce by grammar%23NPH remainder paper organise as+follows section discuss relevant related research section present fred our machine reader reader transform input text into knowledge+representation
introduce method solve semantic reconciliation problem finally section be dedicated evaluation proposed approach section end paper
2. related work ontology learning
nice survey present identify seven system represent state+of+the+art area describe typical task address by ontology learning system as+well+as their functionality implement technique
ontology learning population system focus on derive schema level formal representation knowledge express by text source e.g. concept taxonomical relation axiom etc. while fact level knowledge extraction mainly address by ontology population tool tool require existing target ontology large size text corpus also need some manual intervention
paper refer rdf owl as knowledge+representation+language knowledge graph
work include define semantic language represent meaning thousand english sentence
implementation not provide yet
integration ontology matching
rich overview ontology match method provide by as knowledge+base integration relevant work include leverage interplay between schema instance matching similarly show simple greedy iterative algorithm align knowledge+base million entity fact approach characterise by preferred large size ontology dataset treat good performance rarely probably never derive from text source aim at handle knowledge graph derive from text source source model by use frame semantics based representation align such graph accordingly similarity measure exploit frame semantics feature combine graph matcher
resolution
task deal+with identify element text refer same entity relevant work address crossdocument coreference resolution include use spectral clustering graph partitioning base on bag word latent similarity clustering technique
main difference between our method be that identify coreference relations by analyse formal representation input text by exploit their formal semantic+feature when extract entity be events problem change resolution event coreference within document document author jointly model name entity event cluster entity event mention construct merge accordingly linear+regression model system handle nominal verbal events as+well+as entity joint formulation allow information from event coreference help entity coreference vice+versa author this method show that achieve 61.2%25 recall 75.9%25 precision 67.8%25 f1 score grammar%23NPR metric on grammar%23NPR corpus extension european+central+bank corpus both event entity coreference
knowledge extraction knowledge extraction perform by our tool grammar%23NPH fred convert text into knowledge graph knowledge graph be fully labeled multi digraph such as rdf abstract data+structure characterise by multiple semantic layer i.e. node edge represent schema entity data entity meta-data entity linguistic entity name sub- graph etc. nl construction recognise from parse text fragment but their formal semantics need represent as knowledge graph formalisation step
our approach start by parse formalise text into rdf owl mainly expect target relatively short text need represent concept relations factual knowledge less emphasis schema level axiom such as disjointness cardinality restriction etc.
grammar%23NPH graph classify two macrogroups individual class individual in+turn categorize into named entity skolemized entity entity entity have no proper+name events situation occur table influence automatic aggregation step on size cell dependency graph glacier hormone pollutant node edge node edge node edge original 504+1119+1760 separate aggr step remove copy 503+1117+1652 grammar%23ENT remove lookup aggregate formula 19+227+288 all agr step apply 18+211+272 table heuristics theme heuristic restore symmetry remove macro merged cell fill empty data cell restore symmetry quantity lists instance remove auxiliary calculation remove uncertainty evaluation internal value remove uncertainty evaluation internal calculation procedure remove derivation constant parameter value from empirical data remove unit conversion+table comparison node variable automated ground truth calculation workflow glacier hormone pollutant match recall precision auxiliary calculation have most influence on size cell dependency graph heuristics be applicable glacier case
comparison ground truth workflow compare labeled node variable from ground truth calculation workflow spreadsheet model both glacier hormone case be implementation numerically discretize differential+equation both case researcher explain their paper that variable spreadsheet model progressively evolve base on value preceding row column
make assumption that cell range include label node semiautomated graph compare discretion step that include variable name ground truth workflow
glacier pollutant case more than 60%25 variable from ground truth find semiautomated graph case+study more than 40%25 node from semiautomated graph match ground truth variable hormone case presision recall be low table
result partly explain by+the+way design algorithm aggregation cell dependency graph hormone case researcher use unsubscripted hormone x equation as representative five particular hormone instance variable from ground truth contain x therefore match node from semiautomated graph contain aggregation five instance however automated graph be only node relate individual hormone relate aggregation spreadsheet table five hormone indeed have identical formula but positioning spreadsheet table be such+that formula not aggregate by our algorithm
aggregation algorithm also cause redundant node both hormone pollutant case
node automated graph span cell range fall within range other node node refer same quantity but count as separate node also detection copy statement algoritm be incomplete
some node show aggregation cell be semantically unlogical as node refer multiple quantity at+the+same+time node be artefact but count anyway
explanation our result be existence discrepancy between ground truth spreadsheet implementation both hormone glacier case discretization step several ground truth variable indeed reflect cell range label node automated graph but a+number+of variable especially glacier case cell range discretization step not match table discretion step some variable from ground truth like grammar%23NPR dist.
grammar%23NPR yield stress represent interval be not clear how match cell range spreadsheet semiautomated graph glacier case contain additional node that use as starting+point for+example ice surface bed elev. as intermediate step for+example b c calculation process different stage calculation however not include ground truth semiautomated graph hormone case contain protein bound concentration three protein but ground truth only grammar%23NPR bound concentration mention
pollutant case researcher normalize value quantity individual pollutant by dividing mean+value individual by maximum value all pollutant however maximum value not include as separate quantity spreadsheet but hide normalization formula our aggregation procedure all influence application heuristics on size aggregated cell dependency graph glacier hormone pollutant node edge node edge node edge auto. aggregate separate heuristics restore symmetry 178+247+905+638 remove auxiliary calculation 118+199+275+196 remove conversion all heuristics apply 84+159+125 table comparison variable node name semiautomated ground truth calculation workflow glacier case+study ground truth workflow semi automatic graph p ice density ice density g acceleration g bi bed elev. bed elev. bed elev. bi bed elev. bed elev. f shape factor f hi ice surface ice surface ice surface ice surface hi ice surface ice surface hi glacier thickness h h grammar%23NPR dist. terminus dist. terminus dist. terminus grammar%23NPR yield stress shear stress shear stress b c step length tities aggregate over all pollutant because+of identical formula syntax as consequence maximum value present spreadsheet not be distinguished from value individual pollutant semiautomated graph
discussion automatic aggregation procedure be able reduce size cell dependency graph case+study 50%25 95%25 our assumption that existence multiple instance copy quantity cause redundancy cell dependency graph appear be correct grouping formula base on identical syntax be most effective aggregation step within procedure procedure indicate that most redundancy cause by repetion formula multiple studied object like hormone pollutant
identify three theme developed set heuristics be base on different type knowledge symmetry theme base on knowledge on layout spreadsheet auxiliary calculation theme base on dis composition computational procedure spreadsheet heuristic on unit conversion base on domain knowledge application heuristics result additional reduction aggregated cell dependency graph by 40%25 85%25 heuristics be applicable glacier case possible explanation be that computational model glacier case be small simple enough developer do without additional design decomposition
precision recall from our case+study show that application our aggregation algorithm combine heuristics seem approximate ground truth important explanation not have perfect match be way design our aggregation algorithm comparison ground truth show that deficiency algorithm leave redundancy semiautomated graph
deficiency be difficult solve as involve choice on conceptual level unexpected undesired consequence for+example choose not remove node overlapping cell range from aggregated graph result redundant node refer same quantity however sometimes type node be not redundant for+example glacier hormone case where represent quantity different discretization step furthermore design spreadsheet table be too complex our aggregation recognize aggregate repeating pattern
redundancy semiautomated graph indicate feasible opportunity improve on aggregation procedure consequently on precision our methodology most important also indicate that many redundant node graph be rather product imperfect aggregation than auxiliary quantity that miss by heuristics therefore believe that our assumption that part quantity computational model be auxiliary quantity be correct
base selection our case+study on sufficient description calculation model including mathematical equation however all case study researcher include description equation different level modeling same model section furthermore some variable relation from calculation model only textually describe not explicitly include equation
construction ground truth calculation workflow from written publication therefore appear not at all trivial involve many choice interpretation
conclusion future work goal this study be explore extent calculation workflow underlie set spreadsheet make explicit result this study show that cell dependency graph computational model mainly consist redundant node especially multiple instance quantity auxiliary quantity presented methodology be able reduce this redundancy by 95%25 more bring size within limit human visual comprehension
heuristics develop this study appear suitable distinguish relevant auxiliary quantity thegraph however despite substantial reduction cell dependency graph calculation model approximate ground truth calculation workflow terms content size but be not perfect match
future work plan test improve our automatic aggregation procedure by apply on more case+study another interesting direction future work be use domain knowledge form ontology as expect that facilitate our methodology several way ontology on unit measure quantity like om facilitate automatic recognition unit conversion ontology also support automatic recognition domain concept spreadsheet provide additional step aggregation procedure also facilitate automatic separation relevant auxiliary quantity
future work experiment different set up our methodology different set heuristics
promosing direction be approach colleague aggregate cell dependency graph base on data+flow furthermore this study choose bottom-up approach step+by+step reduce original cell dependency graph instead aggregation set+up more drastic way by apply top-down approach include for+example apply heuristics from very beginning classify select different part spreadsheet include aggregation
this publication support by dutch national program grammar%23NPR thank anonymous reviewer their useful comment on our manuscript
reference infer template from spreadsheet proceedings 28th international conference on software page 182+191

spreadsheet+program reconstruct surface profile former mountain glacier ice+cap computer geoscience 605+610

as public+enterprise case open data
grammar%23ENT
visual checking spreadsheet proceedings first page
toolkit scalable spreadsheet visualization proceedings grammar%23NPR grammar%23ENT conference page
j tool spreadsheet auditing


professional spreadsheet user by generate dataflow diagram proceedings 33rd international conference on software+engineering association+for+computing+machinery
focused maintenance proceedings 9th page
grammar%23NPH grammar%23NPH b. w yangtze+river fluid visualization spreadsheet structure proceedings institute+of+electrical+and+electronics+engineers symposium on visual grammar%23NPR nova+scotia

finding fixing spreadsheet error survey automated approach spreadsheet grammar%23NPR
page
end-user orient graph grammar%23NPR visualization spreadsheet proceeding 4th international workshop on end-user software+engineering page
n. mazer novel spreadsheet method calculate free serum concentration testosterone dihydrotestosterone estradiol estrone cortisol illustrative example from male female population steroid 512+519
how semantics improve engineering process case unit measure quantity advanced engineering informatics 276+287 grammar%23ENT
dependence tracing technique spreadsheet investigation software+engineering method spreadsheet page
chemical exposure scoring procedure uncertainty propagation scenario selection risk analysis chemosphere 330+338
modeling spreadsheet audit rigorous approach automatic visualization
computing grammar%23ENT
inter- cell dependency spreadsheet proceedings institute+of+electrical+and+electronics+engineers symposium on information visualization grammar%23NPH united+states+of+america
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
em bedding ontology annotation spreadsheet bioinformatics grammar%23ENT context driven concept search use chetanagavankar gmail.com yuanfang.li%40monash.edu ganesh%40cse.iitb.ac abstract concept ontology use many scenario including annotation online resource automatic ontology population document classification improve search result collectively ten million concepts define large+number ontology cover many overlapping domain scale duplication ambiguity make concept search challenging problem present novel concept search approach approach exploit structure present ontology construct context effectively filter noise concept search result three key component our approach be context each concept extract from relevant properties axiom base on context result ranking use learn rank algorithm evaluate our approach on large dataset from grammar%23NPR our comprehensive evaluation perform on 2%2C062%2C080 concept more than 2%2C000 query use two widely employ performance metrics our approach outperform grammar%23NPR significantly multitoken query make+up large percentage total query
ontology concept search indexing introduction ever increasing availability structured data on lead challenging problem search this data
linked open data project connect dataset ontology from different domain range from biomedical academic government social+media current breed semantic+web search+engine broadly group into three category that search ontology that search individual resource that search concept represent group individual
ontology be sometimes too coarse-grained because large ontology contain hundred thousand even permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816958 million concepts on+the+other+hand search individual resource be too fine-grained since many resource be relevant return individually not be most useful approach search concept form ideal middle+ground be useful wide variety application such as annotation online resource ontology population document classification search majority application require search concept large overlapping ontology certain domain such as life+science be many overlapping domain ontology ontology primarily contain concept property property describe link concepts best our knowledge existing concept search approach primarily divide into two type base on nature input query grammar%23NPR query at one end keyword query at use grammar%23NPR query as precise input query lead exact result however require user be technically good at write grammar%23NPR query have knowledge schema ontology query in+particular often schema not know user user write query
keyword query based approach typically use standard information+retrieval technique including tf-idf base search ranking popularity based grammar%23NPR algorithm
approach not capture context necessary interpret query multiple keyword
this paper present novel context driven approach search concept ontology use keyword query
axiom class property use as contextual feature improve accuracy assist disambiguate user query primary technical contribution our approach be three+fold context extraction each concept base on relevant properties axiom base on context ranking search result use learn rank algorithm
related work semantic search+engine such as grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR falcon enable keyword based search ontology individual resource within current semantic search system search ontology document term on semantic+web whereas propose search concept ontology search system use standard information+retrieval approach that base on tf-idf grammar%23NPR algorithm our context drive approach use lexical co-occurrence measure direct indirect relation extraction query disambiguation our approach optimize multiple token query context play important role
be stream based approach tool realtime indexing schema extraction data recent work area semantic+web resource ranking largely base on adapt modify grammar%23NPR algorithm grammar%23NPR adapt grammar%23NPR hit algorithm semantic+web data grammar%23NPR rank ontology base on how well cover specified search term search system rank result base on term popularity dataset ecosystem paper by butt et.+al. present comparison eight ranking algorithm search resource within ontology evaluate performance traditional ir based ranking algorithm use semantic+web search current system adapt indexing pagerank based ranking semantic search sytems make use rich ontology structure indexing use learn rank approach ranking search result
work on top- k exploration query candidate on rdf graph data propose intermediate step convert keyword query structured query user need select correct grammar%23NPR query finally retrieve search result
our indexing technique appear be similar their approach require user input select appropriate grammar%23NPR query while our method internally interpret keyword query without need explicitly generate candidate grammar%23NPR query translation keyword query
problem definition give multi- word keyword query on search space diverse goal be retrieve relevant concepts high+quality top- k ranking use context each concept class
query information need represent as query q
q consist m keyword q grammar%23NPR grammar%23NPR km
space let ontology be o grammar%23NPR on name concept given ontology grammar%23NPR represent by ci grammar%23NPR grammar%23NPR grammar%23NPR
name annotation give ontology o entity be named concept named property declare ontology give axiom o logical annotation let entity represent set entity appear entity e let function name e represent name e let function annotation e represent value annotation axiom on e. annotation axiom include rdf label rdf comment rdf grammar%23NPR etc.
concept context concept define as set annotation value concept entity relevant axiom ontology
i grammar%23NPR grammar%23NPR axiom grammar%23NPR i object data property axiom context grammar%23NPR name grammar%23NPR annotation grammar%23NPR name e grammar%23NPR name e grammar%23NPR grammar%23NPR grammar%23NPR be set class property axiom relevant grammar%23NPR respectively where grammar%23NPR include domain range axiom context grammar%23NPR be set name entity be relevant grammar%23NPR together+with name annotation value grammar%23NPR
concept search framework our concept search approach base on understanding that token user query interrelate query interpretation use relations generate contextdriven approach user intention search+query interpret two way either generate implicit query interpretation such as generative language model generate explicit query interpretation clearly interpretable search result our ontology search setting take+advantage rich structure generate explicit interpretation use context properties axiom relate concept
context based search context based search index context information concept via axiom properties ontology context information index along co-occurrence information among keyword use query interpretation evaluate co-occurrence among keyword query use lexical co-occurrence measure explain in+detail section association among keyword use discuss section evaluate direct indirect relation among keyword use context concept obtain search result feature vector fv build search result use rank search result use learn rank algorithm present outline our search framework figure
search process log data concept grammar%23NPR grammar%23NPR km k concept figure our search framework evaluate co-occurrence among keyword use grammar%23NPR grammar%23ENT measure reject null+hypothesis that two words be independent 95%25 confidence if grammar%23NPR grammar%23ENT
chi+square grammar%23NPR grammar%23ENT test compare observed grammar%23NPR j frequency expect grammar%23NPR j independence if difference between observed expect be large than reject null+hypothesis independence statistic sum difference between observed expected+value all square scale by magnitude expected frequency as+follows j grammar%23NPR j grammar%23NPR j table result cluster algorithm combine one variant all system
precision recall precision recall precision recall precision recall grammar%23ENT sil grammar%23ENT sse min grammar%23ENT sse max grammar%23NPR precision recall precision recall precision recall grammar%23ENT sil grammar%23ENT sse min grammar%23ENT sse max grammar%23NPR table number percentage truncated comparison one variant each system number percentage truncated mapping suggestion
sambo full 9%2C026%2C626 grammar%23ENT sil grammar%23ENT 2.9%25 35%25 36%25 35%25 37%25 39%25 43%25 28%25 km-sse-min grammar%23ENT 2.2%25 27%25 29%25 28%25 28%25 30%25 34%25 22%25 km-sse-max 163%2C963 1.8%25 17%25 17%25 17%25 17%25 18%25 18%25 15%25 grammar%23NPR 258%2C308 2.8%25 26%25 28%25 26%25 28%25 28%25 30%25 19%25 conclusion introduce approach reduce search space oa using clustering topic identification by partition ontology generate mappable part computation validation time much reduce while maintain good quality alignment parts need other technique align other part ontology however use proposed technique obtain initial alignment alignment use as basis pa base session base approach complete alignment issue future work be address current limitation i.e. want investigate technique automate topic identification step
cluster technique investigate
thank grammar%23NPR grammar%23NPR european+union grammar%23NPR project grammar%23NPR grammar%23NPR grammar%23NPR financial+support
reference et+al. reduce search space ontology alignment using clustering technique topic identification technical report www.ida.liu.se grammar%23NPR publication grammar%23NPR
h. h do match large schema approach evaluation information+systems
et+al. result ontology alignment evaluation initiative grammar%23ENT international workshop on ontology matching page 61+104
et+al. density based algorithm discover cluster large spatial database noise 2nd grammar%23NPR page 226+231
ontology matching

et+al. ontology alignment evaluation initiative six years experience j data semantics xv 158+192
grammar%23NPH et+al. grammar%23NPR result grammar%23NPR grammar%23ENT 7th grammar%23NPR w on ontology matching page 133+140
rock robust clustering algorithm categorical attribute
information+systems 345+366
grammar%23NPH grammar%23NPH partition based block match large class hierarchy first page
grammar%23NPH et+al. large-scale interactive ontology matching algorithm implementation 20th grammar%23NPR page 444+449
session based approach align large ontology 10th grammar%23NPR page
use partial reference alignment align ontology 6th grammar%23NPR page 188+202
sambo system align merge biomedical ontology j semantics 196+206
silhouette graphical aid interpretation validation cluster+analysis j comp grammar%23ENT
efficient scalable algorithm segmented alignment ontology arbitrary size j semantics 344+356
ontology matching state+of+the+art future challenge institute+of+electrical+and+electronics+engineers grammar%23NPR 158+176
match large ontology base on reduction anchor 22nd grammar%23NPR page grammar%23ENT temporal pattern periodicity entity dynamics cloud economics chni%40informatik.uni kiel.de grammar%23NPH economics a.scherp%40zbw.eu abstract present initial result find temporal pattern entity dynamics on cloud
analysis use dataset three year observation use k mean cluster euclidean distance reveal temporal pattern entity dynamics in+addition conduct first investigation periodicity entity dynamics on cloud while large portion entity be static certain number entity have temporal pattern substantial change observe different periodicity with+respect+to temporal pattern entity dynamics
about temporal pattern their periodicity be important application depend on fresh data cache indices distributed cloud concentrate crawl refresh part cloud be know have change in+the+past b currently have their high periodical change rate
data cloud be global information space represent connect data cloud store information about different real-world object concepts commonly refer as entity relations between cloud expand continuously cover wide range domain because+of this evolution be important understand change behavior cloud over time many application involve data caching indexing distributed data+source
since most popular type search+query contain entity recent search+engine allow user explore entity e g. be important understand temporal dynamics entity in+order+to keep information entity up date be only few investigation temporal dynamics entity cloud however no work so+far look into temporal pattern dynamics periodicity how entity change on cloud
make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816948 this paper take initial step investigate temporal pattern entity dynamics on cloud discover dominant temporal pattern like yang et+al. do tweet blog news+article as dataset analyze dataset collection weekly snapshot large set document period three year
first quantify change entity between two successive snapshot use change metric based on grammar%23NPR distance like grammar%23NPR et+al. temporal dynamics each entity represent as temporal sequence where each element be change value between two successive snapshot compute by change metric subsequently cluster obtained temporal sequence use k mean cluster algorithm employ euclidean distance distance know be efficient high accuracy optimize number cluster observe eight temporal pattern entity dynamics most entity have no change over time have slight change on+the+other+hand certain number entity have temporal pattern substantial change
related work temporal change dynamics grammar%23NPR et+al. provide comprehensive analysis change base on monitor document week report that data change frequently but found that schema be generally static contrast grammar%23NPR et+al. analysis on cloud grammar%23NPR et+al. attempt grasp temporal dynamics context
report that use schema describe entity change quite lot thus while schema be stable entity signature be dynamic
et+al. form labeled directed graph base on where node be entity analyzed entity dynamics apply k mean cluster group entity similar dynamics however not take+into+account amount change entity consider only whether be change not in+addition not optimize number cluster not provide insight into any temporal pattern periodicity pattern
temporal pattern contrast our analysis on yang et+al. investigate pattern temporal variation social+media use grammar%23NPR cluster their analysis find six temporal pattern attention online content tweet blog news+article in+addition focus on cluster only sequence social+media activity have steep rise fall although be lot clustering algorithm develop temporal sequence such as grammar%23NPR grammar%23NPR symbol notation t point+in+time grammar%23NPR set quad capture at t snapshot at t g set all grammar%23NPR our dataset u set all possible grammar%23NPR b set all possible blank node l set all possible literal s subject s u b p predicate p u o object o u b l c context triple s p o c u s p o c quad k mean cluster euclidean distance as grammar%23NPH et+al. report as scalable high accuracy leave use more sophisticated algorithm future work
problem definition let grammar%23NPR be snapshot e. set all quad capture at certain point+in+time t. set all snapshot be g grammar%23NPR grammar%23NPR grammar%23NPR where n denote number snapshot quad s p o c grammar%23NPR consist rdf triple where s p o correspond subject predicate object context c e. uri on where rdf triple retrieve formally consider set all possible blank node b literal l. quad s p o c subject s u b be uri blank node predicate p u uri object o u grammar%23NPR uri blank node literal context c u uri
entity be real-world object concept consider all unique subject grammar%23NPR s appear grammar%23NPR as set entity observe at point+in+time t. entity s at point+in+time t describe by set quad grammar%23NPR s s p o c s p o c grammar%23NPR thus entity s consist quad describe by outgoing properties including p rdf type table summarize symbol notation
following aim at find temporal pattern entity dynamics by analyze how grammar%23NPR s vary over time
methodology first define function compute entity dynamics subsequently describe how find pattern periodicity entity dynamics
entity dynamics define function quantify degree dynamics entity between two point+in+time output value between following grammar%23NPR et+al. first measure degree change entity s between two point+in+time grammar%23NPR grammar%23NPR as show
s grammar%23NPR grammar%23NPR grammar%23NPR s grammar%23NPR s grammar%23NPR s grammar%23NPR s table show small example snapshot change entity db grammar%23NPR between grammar%23NPR grammar%23NPR compute as db grammar%23NPR grammar%23NPR grammar%23NPR
represent entity dynamics cf define temporal sequence s entity change as show
s s grammar%23NPR grammar%23NPR s grammar%23NPR grammar%23NPR s grammar%23NPR grammar%23NPR for+example temporal sequence entity db grammar%23NPH define as db grammar%23NPH
example snapshot dataset at time grammar%23NPR db green village rdf type db city db green village db population dataset at time grammar%23NPR db grammar%23NPH rdf type foaf person db grammar%23NPH db location db green village db grammar%23NPH db work db db green village rdf type db city db green village db population dataset at time grammar%23NPR db grammar%23NPH rdf type foaf person db grammar%23NPH db location db green village db grammar%23NPH db work db db green village rdf type db city db green village db population temporal pattern entity dynamics cluster aim at find temporal pattern entity dynamics cluster temporal sequence s entity s as define this end use kmeans cluster algorithm euclidean distance as this combination be good trade-off between efficiency accuracy result interpret centroid cluster as dominant temporal pattern entity dynamics
number cluster find optimal number cluster k by apply adopted cluster quality score from grammar%23NPR et+al. score consider two factor intra- cluster inter cluster sparseness
cluster quality score cq define as
comp cl iso c where comp cl compute compactness cluster define below comp cl max dist si sj si sj cl compute distance arbitrary pair temporal sequence entity si sj cluster cl return maximum distance between two entity observe cl employ euclidean distance as do cluster furthermore iso c compute minimum distance any two entity from different cluster c. define below iso c min dist si sj si cl sj cm cl cm ideally every cluster contain similar element e. low comp distance between element from different cluster be high e. high iso thus low cq indicate better cluster
detection from temporal pattern periodicity detection be task discover pattern at temporal sequence be periodic for+instance temporal sequence have periodicity respectively computing periodicity over observed entity dynamics ensure generalizability observed temporal pattern employ grammar%23NPR statistics dataset bottom row provide average number quad snapshot standard+deviation
snapshot context 2%2C230 quad snapshot 610%2C824.5 14%2C745.1 algorithm propose by grammar%23NPH et+al. algorithm output periodicity candidate confidence score
experiment result first describe dataset use experiment
show temporal pattern follow methodology describe above discuss result
dataset use grammar%23NPR dataset create monitor fixed set context dataset compose weekly snapshot over three years grammar%23ENT grammar%23ENT more detailed information about dataset refer sake consistency scalability first identify subset context appear all snapshot as show table context originate from pay level domain pld pld be sub- domain public top-level+domain user usually pay extract from data use guava tool from context observe over all snapshot extract on average quad south+dakota snapshot use subset original snapshot our experiment
reason readability continue use term snapshot following when refer subset original snapshot
result discussion entity find total entity from all snapshot i. e. form union all entity over all snapshot entity 5.96%25 appear all snapshot sake consistency scalability cluster focus on analyze temporal dynamics entity entity come from pld i. e. many entity span multiple pld
pattern entity dynamics from entity appear all snapshot observe that entity 77.29%25 not change once over entire period+of+time thus concentrate on cluster remaining entity run k mean cluster vary number cluster k from set number iteration 000 000 use cluster quality score define find that k be optimal number cluster figure plot centroid seven cluster interpretation clustering output be such+that each cluster represent characteristic temporal pattern
show how many entity belong cluster table cs denote set entity have no change over all snapshot in+addition top three candidate grammar%23NPR swse.deri.org%2Fdyldo last access on grammar%23ENT grammar%23NPR github.com%2Fgoogle%2Fguava%2Fwiki%2Frelease19 last access on grammar%23ENT figure temporal pattern entity dynamics
periodicity give ratio compare confidence score top candidate represent how different second third candidate be from top candidate terms confidence score observe that most entity belong where entity change periodically although top periodicity candidate be table confidence score second one be close top one confidence score indicate how likely certain periodicity be actual periodicity temporal sequence thus say that entity repeat change behavior at every week similarly second+largest cluster have periodical change but degree change be smaller than cluster cluster share similar duration terms their temporal pattern where entity continuously change over all snapshot however amplitude change be different between cluster small number entity belong cluster c4 here entity only have small amount change but change happen continuously at every week periodicity be one as show table however substantial change also observe at some point+in+time as depict figure cluster grammar%23NPR see as outlier since only contain one entity subject uri be legislation.gov.uk%2Fuksi%2F2010%2F1158%2Farticle%2F2%2Fmade
level domain entity cluster finally investigate resulting cluster cover provide dominant pld entity at first row large portion entity come from ontologydesignpatterns.org
provide dominant pld with+respect+to cluster ratio occupy by each pld cluster observe that majority more than 60%25 entity come+from up three different dominant pld most cluster even when number entity cluster be large e g. cs for+example see that ontologydesignpatterns.org grammar%23NPR be dominant cs ontology design pattern define ontology design pattern as generic solution recurring modeling problem be by+definition stable contain rdf rdf grammar%23NPR resulting cluster top three candidate periodicity temporal pattern
parenthesis ratio compare confidence score top candidate give represent how different second third candidate be from top candidate terms confidence score high value indicate that periodicity be as appropriate as top one
entity top three periodicity 12%2C982 c4 grammar%23NPR 1%2C541 cs 50%2C275 vocabulary hardly change too entity from legislation.gov.uk distribute different cluster dominate be official database statute+law united+kingdom each entity indicate law have different pattern depend on type law assume that type law be reason why have different dynamics thus distribute over cluster however investigate more detail require further analysis
conclusion present initial result analyze temporal pattern entity dynamics on cloud use kmeans cluster find seven temporal pattern entity dynamics on large datset over period three year in+addition conduct first investigation periodicity entity dynamics cloud insight provide on temporal pattern periodicity cloud dynamics be important application depend on timely update data cache index cloud conduct target crawl fresh data part cloud that know have frequent change b have peak their change rate accord periodicity in+the+future scale+up analysis use more elaborate cluster algorithm
reference k mean advantage careful seeding soda page

change dynamics dynamics analysis linked open data+source profile

structured knowledge open information page 267+277 world+wide+web+consortium

detection time+series database grammar%23NPR 875+887
ontology design pattern semantic+web content grammar%23NPR page 262+276 springer
pay level domain pld entity cluster pld show until sum ratio each cluster except exceed 60%25
level domain ratio all ontologydesignpatterns.org 40%25 legislation.gov.uk 11%25 grammar%23NPR 9%25 40%25 legislation.gov.uk 30%25 utexas.edu 28%25 data.gov.uk 20%25 22%25 openei.org 42%25 legislation.gov.uk 35%25 23%25 legislation.gov.uk 49%25 kit.edu 29%25 22%25 c4 legislation.gov.uk 100%25 grammar%23NPR legislation.gov.uk 100%25 legislation.gov.uk 23%25 bbc.co.uk 21%25 ivan-herman.net 11%25 ntu.ac.uk 6%25 reegle.info 5%25 34%25 vivoweb.org 13%25 kanzaki.com 10%25 fao.org 10%25 utexas.edu 10%25 qudt.org 10%25 buzzfeed.com 10%25 37%25 cs ontologydesignpatterns.org 52%25 grammar%23NPR 11%25 37%25 observe link data dynamics grammar%23NPR page 213+227 springer

linked data good practice different topical domain grammar%23NPR page 245+260 springer
towards understand changing mine dynamics linked data source entity grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH experimental comparison representation method distance measure time+series data data+mining knowledge discovery 275+309
pattern temporal variation online media grammar%23NPR page 177+186

building taxonomy search intent name entity query page association+for+computing+machinery grammar%23ENT figure formula+one measure grammar%23NPR grammar%23NPR table good result combination n t 25%25 french+open 15.9%25 be useful use automated approach difference between grammar%23NPR explain by size dataset larger dataset benefit wisdom crowd
conclusion future work this paper investigate uncertainty involve relation extraction process from highly noisy data+source grammar%23NPH application extract game result two grand+slam tennis+tournament use modular approach extraction propose different approach reduce uncertainty involve our proposed approach be use domain knowledge fact relation repetition wisdom crowd choose right semantic+web source proposed method lead improvement up 25%25 f1 result
future research more statistical+analysis do need check validity proposed method on other domain also want take+time constraint into account furthermore plan do some correlation analysis automatically find optimal number repetition corresponding characteristic trustworthy author also research more detail last more research on trustworthiness semantic+web dataset need cause different extraction result
reference
unexpected rule from twitter hashtags application sport event machine+learning application grammar%23NPR 13th on page 207+212 grammar%23ENT
spot ball detecting sports+event on twitter editor advance information+retrieval volume lecture note computer+science page 449+454
unsupervised method extraction propositional information from text proceedings national+academy+of+sciences suppl
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
vault scale approach probabilistic knowledge fusion proceedings 20th on knowledge discovery data+mining grammar%23NPR page 601+610 new+york new+york united+states+of+america association+for+computing+machinery
incorporate non-local information into information extraction system by gibbs sampling proceedings 43rd annual meeting on page 363+370 grammar%23NPR united+states+of+america
unsupervised improvement named entity extraction short informal context use disambiguation clue grammar%23NPR grammar%23ENT semantic+web information extraction volume proceeding page ceur-ws.org grammar%23ENT
concept extraction challenge university twente at grammar%23NPR proceedings concept extraction challenge at workshop on rio+de+janeiro volume proceeding page grammar%23ENT
grammar%23NPH
spatially temporally enhanced knowledge+base from artificial+intelligence
semi- structured resource
uncertainty reduction knowledge discovery information extraction on proceeding grammar%23NPR grammar%23ENT
various approach text representation named entity disambiguation
242+259
grammar%23NPH joint inference named entity recognition normalization tweet proceedings 50th annual meeting long paper volume page 526+535 grammar%23NPR united+states+of+america

grammar%23NPR named entity recognition tweet experimental study proceedings conference on empirical method natural+language+processing grammar%23NPR page grammar%23ENT grammar%23ENT grammar%23NPR united+states+of+america
grammar%23NPH about twitter
grammar%23NPH automatic extraction soccer game event from volume page
wisdom tweetonomies acquire latent conceptual structures from social awareness stream proceedings 3rd grammar%23NPR page grammar%23ENT new+york new+york united+states+of+america rule refinement technique grammar%23NPR united+kingdom 44+1224+272296 julius.cepukenas.10%40aberdeen.ac.uk united+kingdom 44+1224+272296 chenghua.lin%40abdn.ac.uk united+kingdom 44+1224+272288 d.sleeman%40abdn.ac.uk box classifier be able classify unseen instance once train on appropriate domain dataset such classifier have advantage be generally very efficient but disadvantage not be able explain their processes user reason over last decade so a+number+of algorithm develop be able extract rule set from classifier focus this project be re- implement state-of-the-art system grammar%23NPR then show that when rule refine by system grammar%23NPR that refinement+process virtually all case improve fidelity refined rule set when compare rule set extract by grammar%23NPR statistically significant difference between two approach demonstrate investigate classifier blackbox neural+network grammar%23NPR classifier decision+tree based whitebox domain so total classifier-dataset combination consider only case 6.25%25 be result slightly bad case 31.25%25 be same not improve remaining case 62.5%25 show significant improvement in+the+future intend use similar approach improve accuracy classification this study focus on fidelity category subject descriptor grammar%23NPR application general+term algorithm experimentation keyword expertize capture permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT palisade new+york+state united+states+of+america association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816950 introduction currently huge amount data be available analysis in+order+to find useful knowledge this amount data human use advanced statistical data+mining technique important subfield data+mining machine+learning be classification attempt classify instance as pre- defined set category many classification+system implement use range schema represent both training instance description category classifier implement be very efficient both learning from instance subsequent classification unclassified instance however lack explanatory facilities i.e. be unable explain their classification procedure describe description hold several class so+called grammar%23NPR classifier thus last decade so system develop when give trained classifier training set be able extract set rule capture classifier inherent category thus make information knowledge knowledge be initially implicit explicit available scrutiny by domain expert such knowledge then also use whole part other knowledgebased system review state+of+the+art system then re- implemented grammar%23NPR system additionally implement knowledge+base grammar%23NPR refinement system grammar%23NPR be development stalker focus this project be extract rule rule be as close as possible inherent classifier this reason use fidelity metric not accuracy achieve high+fidelity opt modular approach first phase use grammar%23NPR instead attempt enhance this algorithm choose use independent knowledge+base refinement system improve kb subsequently run a+number+of studies where compare fidelity values report by refined rule set rule set case although classifier base on classifier be like as opaque as grammar%23NPR classifier include study both as reference+point see whether stage approach use here make their inherent information more understandable user comparison understandability be between original classifier refined kb note understandability be not focus this project detailed discussion compare type classifier within same framework see literature review some introductory comment about this subfield already make introduction section simply summarize here system use studies namely orthogonal search grammar%23NPR algorithm grammar%23NPR base on formalism propose by grammar%23NPH grammar%23NPR successfully apply variety domain mainly biomedicine further grammar%23NPR use extract rule from any classification module as utilize pedagogical approach approach consider trained classification model as black+box instead inspect classifier internal structure create rule rule express relationship between input output classification module hence this approach say be generic assumption behind this approach be that classification model better represent data than original dataset be rule set be clean as be free apparent conflict significantly grammar%23NPR use extract rule from both module from grammar%23NPR module grammar%23NPR possess attractive characteristic reduce problem from exponential linear terms number input number description value instead perform exhaustive search grammar%23NPR perform grammar%23NPR code while search through training dataset finally resulting set rule then prune by minimize number conjunction use rule trim by remove overlapping redundant rule note however grammar%23NPR only work ordinal attribute detailed literature review on this topic associated reference see knowledge based refinement system kb system have great importance many research area development such system often involve extensive knowledge+acquisition deal inherent inconsistency kb several system including seek grammar%23NPR grammar%23NPH stalker develop give kb set labelled instance make systematic change kb attempt cover all instance assign category specify by expert this project use grammar%23NPR system grammar%23NPR develop by author be successor successful rule refinement tool stalker stalker introduce important innovation system namely efficient grammar%23NPR like mechanism greatly reduce computational cost evaluate potential refinement knowledge+base grammar%23NPR also incorporate this innovation both grammar%23NPR stalker use multiple refinement generation cycle however stalker focus on mis classified data instance whereas grammar%23NPR generate refinement each mis classification tuple data instance class pair first deal all fn mis classified tuple generalization phase before consider grammar%23NPR tuple specialization phase give that grammar%23NPR assume that each instance be unique only assign by expert single class be virtually always possible propose refinement kb distinguish instance use induction when necessary more detail grammar%23NPR literature review system see studies as mention section each classifier train use particular domain dataset each dataset contain series expert labelled instance so be inherent classification trained classifier be able classify same set instance but not necessarily identically as expert see figure have re- implemented grammar%23NPR algorithm algorithm run mode st use static threshold at use adaptive threshold this study use st mode threshold set at as original implementation as algorithm process rule from classifier call osre-st approach produce original set rule from classifier then refine by grammar%23NPR result further rule set kb refer as extract refinement approach note report fidelity metric between rule set produce by original classifier osre-st between original classifier extract refinement approach four dataset use this study namely breast+cancer grammar%23NPH discretize into section soya+bean dna promoter theory dataset in+addition four classifier use namely mlp grammar%23NPR na%C3%AFve grammar%23NPR figure extraction refinement rule from trained classifier where solid rectangle symbolise method approach where rectangle broken+line symbolise data+structure e.g. domain dataset rule set training instance domain d expert annotation empty classifier c training instance domain d classifier c annotation classifier c train domain d system grammar%23NPR rule extract grammar%23NPR fidelity define as percentage instance dataset classify exactly by rule as by classifier specificity true negative rate sensitivity true positive rate calculate refined extract ruleset use training dataset classifier as oracle provide measure rule set be shortcoming when compare classifier provide some indication whether rule generalization specialization require enhance performance rule set accuracy define as percentage instance dataset classify by classifier as by domain expert accuracy rule set define similarly general assumption dataset use train classifier be available use refinement system table comparison fidelity values extract refinement osre-st approach grammar%23NPR dataset classifier osre-st approach extract refinement approach fidelity spec. sens. fidelity spec. sens. grammar%23NPR na%C3%AFve grammar%23NPR grammar%23NPH mlp grammar%23NPH grammar%23NPR 78+100 100+100 grammar%23NPH na%C3%AFve grammar%23NPR grammar%23NPH grammar%23NPR dna mlp dna grammar%23NPR dna na%C3%AFve grammar%23NPR dna summary this study use osre-st system extract ruleset from several trained classifier subsequently grammar%23NPR refine kb 4. approach increase fidelity rule firstly classifier train full dataset rule extract calculate further ten+fold cross validation process apply in+order+to calculate fidelity this approach make usual distinction machine+learning between training test set thus believe that set metrics be compatible trend expect identify this study primarily investigate fidelity values report variety situation by osre-st extract refinement approach hypothesize that value fidelity report by extract refinement be consistently high than values report by osre-st approach without rule refinement additionally give relationship between set metrics note above also hypothesize that value report by extract refinement be consistently high than values report by osre-st approach this example be case where specificity and%2For sensitivity measure be equal both approach see grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR table however except grammar%23NPR represent special case where specificity sensitivity rule extract by osre-st be already 100%25 hence not improve see table detail note grammar%23NPR be special case as algorithm osre-st not extract any rule fidelity values report by rule produce by extract refinement approach be very encouraging as case values report by extract refinement approach be significantly great than osre-st approach additionally be case where fidelity rule produce by osre-st extract refinement approach be identical namely case grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR be just case where fidelity rule extract by extract refinement approach be worse than fidelity rulesproduced by osre-st approach case grammar%23NPR paired carry+out on set fidelity values support hypothesis give earlier this section at 95%25 confidence+level several reason last two observation identify firstly result table suggest that dna promoter be hard domain note be only domain extract refinement approach be unable extract rule rule have at+least slight increase fidelity when compare original rule extract by osre-st approach be true specificity sensitivity metrics in+fact only increase whitebox classifier see table secondly be worth noting that case where extract refinement approach be not capable increase specificity sensitivity metrics regard osre-st approach fidelity metrics be also not increased be clear that when specificity sensitivity value osre-st be both 100%25 grammar%23NPR have no opportunity enhance kb hence fidelity value not increase however when osre-st specificity be give grammar%23NPR potential use generalization operator revise kb hence enhance metrics similarly if sensitivity be then grammar%23NPR specialization operator use this suggestion confirm by comparison between domain odd dna domain exclude from this comparison average value fidelity osre-st approach domain be 58.6%25 average value fidelity metric domain extract refinement approach be 67.76%25 increase 9.16%25 be great than same entity domain average increase fidelity different classifier summarise table average increase fidelity different domain summarise table table average increase fidelity metric between approach focus on effect classifier use increase fidelity measure between refinement approach overall conclusion extract refinement approach work as expect produce consistently good fidelity value than osre-st approach except that improvement dna promoter theory be marginal average increase fidelity mlp calculate as+follows value fidelity extract refinement minus fidelity osre-st mlp all dataset grammar%23NPH dna add+together average then produce by divide by number dataset this case this calculation be comparable outline footnote above detail see table average increase fidelity metric between approach focus on effect dataset use increase fidelity measure grammar%23NPH dna between refinement approach further work investigate accuracy as oppose fidelity refined kb create from blackbox classifier so that when provide null kb some instance create initial kb from several instance investigate in+detail increase fidelity classifier 6. acknowledgments support author jc provide by development+fund dr grammar%23NPH provide comment on this project 7. reference grammar%23NPR t. p orthogonal search based grammar%23NPR trained neural+network practical efficient approach institute+of+electrical+and+electronics+engineers transactions on neural+network vol. 374+384 carbonara l. grammar%23NPR d effective efficient knowledge+base refinement machine+learning vol. 143+181 grammar%23NPR j rule refinement technique classifier master thesis computing science dept preparation grammar%23NPH grammar%23NPH grammar%23ENT extract rule from trained neural+network institute+of+electrical+and+electronics+engineers transactions on neural+network 377+389 marten d. grammar%23NPR j. grammar%23NPR r. grammar%23NPR j. grammar%23NPR b. grammar%23ENT from support vector machine overview issue application credit+scoring sci grammar%23NPR j. grammar%23NPR d. grammar%23ENT unified approach extraction rule from support vector machine advanced data+mining application springer breast+cancer dataset grammar%23ENT collect by grammar%23ENT grammar%23ENT ftp%3A%2F%2Fftp.ics.uci.edu%2Fpub%2Fmachine-learning-databases%2Fbreastcancer-w grammar%23NPH dataset grammar%23ENT ftp%3A%2F%2Fftp.ics.uci.edu%2Fpub%2Fmachine-learningdatabases%2Firis grammar%23NPR r. learn by learning from example experimental comparison two method knowledge+acquisition context develop expert+system soybean disease diagnosis vol. no. 126+161 grammar%23NPR grammar%23NPH refinement approximate domain theory by knowledge grammar%23NPR grammar%23NPR computer homeostasis through syslog analysis use autonomous epistemic agent grammar%23NPH laboratory grammar%23ENT
united+states+of+america cameronhughes%40acm.org grammar%23ENT
united+states+of+america ctestlabs%40ctestlabs.org grammar%23NPH asc construction grammar%23ENT
united+states+of+america james.dittrich%40gmail.com grammar%23NPH grammar%23ENT united+states+of+america twatkin6%40kent.edu proliferation mobile+computing internet thing host service cloud+computing increase burden computer log file analysis system+administrator network analyst security+analyst large server host organization be due voluminous amount log entry now produce by technology since log file analysis use monitor control overall health computer+system behind technology become increasingly important spike number log entry make real-time log analysis by human effort untenable automated real-time log analysis essential log analysis process often require human insight judgment before diagnosis information synthesis become apparent so while automated log analysis method be essential also be knowledge base be effective this paper describe knowledge based approach partial computer self-regulation use autonomous epistemic agent analyze diagnose syslog entry real-time use priori posteriori knowledge log file analysis within hybrid deductiveabductive first order logic model epistemic agent use be priori knowledge based computer+system conjunction posteriori knowledge extract from log file entry uncover negative positive scenario take+advantage opportunity regulate computer+system homeostasis category subject descriptor grammar%23NPR artificial+intelligence knowledge+representation formalism method modal+logic predicate logic semantic+network temporal logic
algorithm reliability theory verification
knowledge retrieval knowledge+representation conceptual graph abductive epistemic+logic text analysis autonomous agent event calculus
introduction grammar%23ENT internet+engineering+task+force standardize syslog grammar%23NPR syslog protocol provide transport allow machine send event notification message ip network event message collector be standard use globally message log security auditing system management computer fault analysis syslog be also part portable operating+system interface define international+standards+organization grammar%23NPR international standard benefit syslog server be ability collect comprehensive log data from variety source device quickly
problem problem be threefold voluminous log data analysis that requires human insight judgment grammar%23NPR culture syslog server have ability collect more data more quickly than reasonably analyze by human effort be not uncommon single server generate 500%2C000 log entry day both have over 1%2C000%2C000 server provide proxy host often require external time consume data+mining analytics as only feasible course analysis effective log file analysis be not exact+science log analysis process often require human insight judgment before diagnosis information synthesis become apparent so while automated log analysis method be essential also be knowledge base be effective further since log file permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816953analysis use identify in- progress security breach data+loss computer failure after+the+fact automated analysis be often late avert lessen reverse stop kind issue many organization be reactive culture with+respect+to log file reactive culture log analysis only become concern when system be state+of+emergency this paper describe grammar%23NPR approach partial computer self-regulation use autonomous epistemic agent analyze diagnose syslog entry real-time use priori posteriori knowledge log file analysis within hybrid deductive-abductive first order logic model epistemic agent use be priori knowledge based computer+system conjunction posteriori knowledge extract from log file entry uncover negative positive scenario once uncover take+advantage opportunity regulate computer+system homeostasis 2. grammar%23NPR describe standard syslog messaging subsystem here focus on three aspect grammar%23NPR use agent priori knowledge facility severity level log message format
syslog epistemic agent syslog epistemic agent be autonomous agent have epistemic structure e commission use grammar%23NPR grammar%23NPR grammar%23NPR computer homeostasis base on syslog analysis where grammar%23NPR define by national+institute+of+standards+and+technology standard grammar%23NPR grammar%23ENT es define by structure e g2+army+intelligence grammar%23NPR j f v e be conceptual hypergraph consist priori propositional knowledge about grammar%23NPR negative positive server scenario model theoretic semantic representation environment
v e be conceptual hypergraph consist posteriori propositional knowledge extract from syslog entry some interval t grammar%23NPR j be poset abductive justification proposition one more negative scenario j define by j p p g2+army+intelligence p where p be most epistemic plausible explanation some conceptual graph g where g g2+army+intelligence .vc be set vector that extent practical selected specifically practicality prune search space diagnosis during abductive inference
epistemic agent level epistemic commitment j define by grammar%23NPR c c g2+army+intelligence p p j f be knowledge+management function over es maintain logical consistency epistemic plausibility
3. epistemic agent be grammar%23NPR knowledge our epistemic agent have basic general common+sense knowledge about compliant operating+system environment table show area where our epistemic agent have priori knowledge basic knowledge from area along set negative positive server scenario make+up content from small domain application vocabulary identify each area grammar%23NPR model m use implement their respective domain application ontology where m d f d be domain take from area table f be hybrid interpretation combine deductive abductive method establish semantics formula m. specific vocabulary relationship property individual individual form basis domain application ontology painstakingly take from knowledge source list also table
negative positive scenario over time point let t be time grammar%23NPR define by range grammar%23NPR where be starting+time grammar%23NPR be ending time let any negative scenario positive scenario be fluent f then our epistemic agent have relation table epistemic agent source priori knowledge knowledge source five area agent priori knowledge
area knowledge source ontology type apply file+system user acct. maintenance memory+management system application process maintenance commonly use peripheral syslog file grammar%23NPR grammar%23NPR vol definition syslog man+pages administration guide application grammar%23NPR domain although j contain p most epistemic plausibility p still not be good+enough our epistemic agent use grammar%23NPR as standard grammar%23NPR description set o acronym
acronym description grammar%23NPR single occurrence single segment ii som single occurrence multiple segment iii moss multiple occurrence single segment iv mom multiple occurrence multiple segment figure simplified view grammar%23NPR log message format
grammar%23NPR f t p f p j grammar%23NPR p say agent have good reason believe that particular negative positive scenario f be in+effect over time+period specify by t. positive scenario base on tuple facility severity area any log message present at be among facilities severity be level select area be from agent for+instance if be no syslog message over t given facility table then assume positive scenario that facility over t. if be message given facility over t severity level then assume only somewhat positive scenario that facility over simplified overview syslog format show facility severity as define component each log entry negative scenario base on tuple facility severity area quadrant analysis q le t all le grammar%23NPR where q le t o o grammar%23NPR som moss mom semantic analysis grammar%23NPR from agent posteriori knowledge g2+army+intelligence table show description set le t analysis place every le quadrant so that grammar%23NPR occur only once grammar%23NPR segment not consider at same severity characterize same way as grammar%23NPR have multiple occurrence multiple segment mom 4. epistemic agent represent agent know about particular computer syslog at particular t. proposition g2+army+intelligence be conceptual graph be result semantic analysis message content see figure every grammar%23NPR over t. message content parse tag accord our domain application model m then present our grammar%23NPR because deductive abductive interpretation grammar%23NPR formula use
f generate semantic meaning message primary tag be concept arity grammar%23NPR arity grammar%23NPR arity grammar%23NPR arity tagged message pass hybrid interpreter see section return appropriate interpretation as clausal form horn clause be implementation grammar%23NPR conceptual graph store g2+army+intelligence as part agent posteriori knowledge for+example grammar%23NPR content session open user by uid get tag as concept session grammar%23NPR session user grammar%23NPR status open grammar%23NPR user concept grammar%23NPR then give by f semantic interpretation open session user grammar%23NPR this clause be semantic analysis grammar%23NPR use by agent conjunction grammar%23NPR tuple facility severity area q le t as evidence some negative positive scenario possibility
partial computer homeostasis grammar%23NPR history be no requirement that system application+program report error exception syslog facility consequently syslog file be history contain entry from only program elect record error exception only figure grammar%23ENT high level flowchart error exception program deem necessary log mean that syslog s.o. system operational status history not count+on complete picture any selfregulation do as result diagnosis syslog entry base on partial knowledge be epistemic agent construct s.o. history system by analyze syslog if any problem potential problem identify so history still reflect current state system will base on grammar%23NPR permission make necessary adjustment contribute system homeostasis table list tool use develop knowledge+base figure grammar%23NPR contain highlevel flowchart computer homeostasis including epistemic agent functioning as control mechanism figure grammar%23NPR show algorithm use regulate system base on anomaly error exception identify syslog table lists describe primary function
grammar%23NPR algorithm
tool use develop knowledge+base
use purpose knowledge type grammar%23NPR conceptual graph ontology development priori protege knowledge+base development priori grammar%23NPR deductive inference+engine grammar%23NPR conclusion algorithm identify computer issue issue normally filter ignore by system+administrator because look+at interrelationship between message multiple occurrence message interval that human system+administrator ordinarily miss our correct have permission correct report have no permission correct but identify as require attention
primary function epistemic agent control algorithm primary function description grammar%23NPR take tokenized form log entry return grammar%23NPR formula hybrid deductive abductive model model generate process create e
produce set candidate hypothesis associate grammar%23NPR negative positive scenario
produce set negative scenario verify from hypothesis
produce set positive scenario verify from hypothesis
determines what level action agent have authority take base on scenario type
performs necessary change system contribute system homeostasis
report any action take analysis diagnosis grammar%23NPR accord configuration constraint originally setup syslog entry on system
reference rfc network working+group grammar%23NPH syslog protocol category standard track
grammar%23NPR grammar%23NPR information technology-portable operating+system interface
grammar%23NPR t. grammar%23NPR c. grammar%23NPH a. grammar%23ENT epistemic structured representation legal transcript analysis advance computer information+science engineering springer 101+107
grammar%23NPR t. grammar%23NPR c. grammar%23ENT building grammar%23NPR knowledge space from interrogative domain grammar%23ENT semantic computing conference publishing service los+alamos 309+312
grammar%23NPR s. grammar%23NPR g. grammar%23NPR a. grammar%23ENT practical internet+security grammar%23NPR edition
grammar%23NPR a. grammar%23ENT 3rd edition define grammar%23NPH open+university united+kingdom open.ac.uk abstract this paper propose ontology design pattern concept explanation motivation behind this work come from our research research focus on automatically identify explanation data pattern if want produce explanation from data agnostically from application domain first need formal definition explanation be i.e. be component their role their interaction analyse survey work from discipline group name cognitive+science aim identify difference commonality way their researcher intend concept explanation
then produce not only ontology design pattern model but also instantiation ana lysed discipline besides contribution paper present how proposed ontology design pattern use analyse validity explanation produce by our other framework
subject descriptor grammar%23NPR artificial+intelligence knowledge+representation formalism method ontology design pattern keyword explanation ontology design pattern knowledge discovery+1 introduction word explanation occur so continually hold so important place philosophy that little time spend fix meaning profitably employ grammar%23NPH system logic
this paper present ontology design pattern support formal representation explanation motivation behind our work come from our research aim be automatically find explanation data pattern use background+knowledge from data
that data pattern intend as subset data permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT same regular way our general research problem be that find explanation pattern be intensive task still rely on human expert role be provide explanation refine result use their own background+knowledge while cross domain machine-accessible knowledge data use facilitate automatisation explain data pattern
challenge face here be that if wish give explanation pattern more specifically process data in+order+to produce first need formal way define be explanation define e be complicated epistemic matter upon common agreement never reach although extensively discuss through time discipline rather than enter this discussion our proposition be identify ontology design pattern pattern formally support representation explanation including component their role interaction in+order+to provide abstract description description be applicable any context where system automatically produce explanation
methodology use derive such design pattern be explore how explanation define area most deal organisation understanding knowledge usually group define as cognitive+science review main literature discipline embrace by cognitive+science in+order+to see how their researcher see explanation by identify difference and%2For commonality among different area such as aspect explanation matter how e define aim at abstract our own definition e
finally propose be formal definition e encode as ontology design pattern odp present instantiation into ontological representation explanation considered discipline cognitive+science constitute along cross domain survey on definition explanation major contribution our paper finally also show how use pattern assess validity framework produce explanation real-world scenario
motivation our previous work present grammar%23NPR as framework explain group item use background+knowledge extract from linked grammar%23NPR grammar%23NPR assumption be that grammar%23NPR refer e further on intend concept explanation
www.w3.org%2Fstandards%2Fsemanticweb%2Fdataneural network on artificial+intelligence grammar%23NPR p.d. grammar%23NPR t neural+network ii be why be so interested now institute+of+electrical+and+electronics+engineers expert grammar%23ENT volume grammar%23NPR improvement grammar%23NPR grammar%23NPR algorithm grammar%23NPR classifier design neural computation vol. no. grammar%23NPH proceedings on uncertainty artificial+intelligence san+francisco grammar%23NPR r. grammar%23ENT grammar%23NPR program machine+learning san+mateo grammar%23NPH grammar%23NPR publisher evaluate competency first-order ontology basque+country grammar%23NPR grammar%23NPR javier.alvez%40ehu.eus basque+country grammar%23NPR grammar%23NPR paqui.lucio%40ehu.eus basque+country grammar%23NPR grammar%23NPR german.rigau%40ehu.eus abstract report on result evaluate competency first-order ontology use automated theorem provers evaluation follow adaptation methodology base on competency question cq framework first-order logic present apply grammar%23NPR sumo set cq use this evaluation automatically generate from small set semantic pattern mapping wordnet sumo analyse result conclude that be feasible use work grammar%23NPR grammar%23NPR enable resolution goal by+means+of perform trivial inference
subject descriptor grammar%23NPR artificial+intelligence knowledge+representation formalism method general+term experimentation introduction ontology evaluate by consider their use application when perform correct prediction on inferencing in+order+to enable good reasoning capability inferencing process be able deduce from ontology as+much correct implicit knowledge as possible author propose use set competency question cq evaluate ontology ontology be goal that ontology expect answer once set cq define evaluation consist check whether cq entail by ontology not
firstorder logic grammar%23NPR like vampire e be highly sophisticated system prove provide advanced reasoning support expressive ontology consequently permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright thirdparty component this work honor all use contact owner author copyright hold by owner author s
grammar%23ENT grammar%23ENT grammar%23ENT new+york grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816946
some figure about clause general clause departamento+del+meta knowledge top level sumo 2%2C034%2C650 mid level sumo 2%2C601%2C975 grammar%23NPR transformation 1%2C152 total 4%2C635 2%2C785 tool use automatize evaluation firstorder grammar%23NPR ontology as propose more concretely author adapt above method evaluate grammar%23NPR ontology use also obtain set cq from mapping from wordnet grammar%23NPR sumo
this paper successfully apply method describe evaluate grammar%23NPR sumo grammar%23NPR be grammar%23NPR ontology obtain by transform sumo use proposed set cq our result prove that be feasible work grammar%23NPR ontology perform trivial inference inference be very useful wide range knowledge intensive application for+instance help validate consistency associated semantic resource like wordnet derive new explicit knowledge from
next two section introduce grammar%23NPR sumo method use section evaluate ontology last section provide some concluding remark
grammar%23NPR sumo grammar%23NPR have origin ninety when group engineer from push formal ontology standard their goal be develop standard upper ontology promote data interoperability information+search retrieval automated inference natural+language+processing
express suo-kif be dialect both suo-kif use write grammar%23NPR formula but syntax go beyond fol. consequently sumo not directly use by without suitable transformation grammar%23NPR sumo be result transform around axiom from top middle level sumo into grammar%23NPR formula translation base on small set axiom provide axiomatization grammar%23NPR www.ontologyportal.org60 120+300+600+400+600+800 1%2C000 1%2C200 execution time+limit second n u m b er o f p ro o f all test truth test falsity test figure number solved goal 60+120+300+600+400+600+800 1%2C000 execution time+limit second n u m b er o f x io m s all test truth test falsity test figure number different axiom use proof meta predicate that require define knowledge sumo as grammar%23NPR formula meta- predicate be instance subclass disjoint partition axiomatization not directly inherit from sumo transformation also add new axiom suitable characterization sumo type variable arity relation holdsk predicate simulate use variable predicate grammar%23NPR formula table provide some figure about content grammar%23NPR sumo more specifically number atomic formula unit clause atomic formula general clause use each part grammar%23NPR sumo
speak atomic formula simply atom make explicit knowledge ontology whereas atomic formula contain connective and%2For quantifier define implicit knowledge be worth note that axiomatization meta- predicate meta- knowledge axiom require transformation into grammar%23NPR formula grammar%23NPR transformation not include any atomic formula sum+up provide 4%2C635 unit clause 2%2C785 general clause plus conjecture as source association+of+tennis+professionals each cq
execution time+limit second v er g e n u m b er o f x io m s all test truth test falsity test figure average number axiom use proof evaluation method author propose evaluate expressiveness ontology by prove completeness theorem set cq proof completeness theorem require check whether given cq entail by ontology not author adapt above method use as tool automatically check whether cq entail not in+particular this adaptation base on use that work by refutation within given execution time+limit follow this proposal set cq partition into two class truth test falsity test depend on whether expect conjecture entail by ontology not if association+of+tennis+professionals be able find prove given conjecture then know for+sure that corresponding cq entail by ontology however if association+of+tennis+professionals not find proof not know if conjecture not entail by ontology although conjecture entail association+of+tennis+professionals not be able find proof within provided execution time+limit consequently accord proposal truth test classify as pass if association+of+tennis+professionals find proof since conjecture expect entail whereas falsity test classify as ii pass if proof found because conjecture expect not entail otherwise if no proof found both truth test falsity test classify as iii unknown this classification provide effective method evaluate ontology given set cq be worth remark that evaluation result strongly depend on execution time+limit association+of+tennis+professionals since cq classify as unknown within given executiontime limit classify as pass pass within longer execution time+limit
provide complete evaluation framework author also propose set cq as benchmark
proposed cq obtain from mapping from wordnet sumo information morphosemantic database contain semantic+relation between morphologically related noun verb
grammar%23NPR at wordnetcode.princeton.edu standoff file morphosemantic-links.xls.60 execution time+limit second n u m b er o f x io m s b y ty p e top level uc gc all test uc gc truth test uc gc falsity test execution time+limit second n u m b er o f x io m s b y ty p e middle level uc gc all test uc gc truth test uc gc falsity test figure number axiom from top middle level more concretely author generate 3%2C556 truth test that obtain from pattern define on basis on information about antonym process by negate conjecture resulting truth test author also propose 3%2C556 falsity test consequently proposed benchmark consist 7%2C112 cq this benchmark use evaluate grammar%23NPR sumo grammar%23NPR result describe next section
evaluation result this section report result evaluation grammar%23NPR sumo grammar%23NPR use framework introduce previous section this evaluation use association+of+tennis+professionals vampire standard 64-bit grammar%23NPR cpu grammar%23ENT desktop machine grammar%23ENT ram consider different execution+time limit grammar%23ENT grammar%23ENT ontology grammar%23NPR sumo grammar%23NPR set cq execution report be freely figure sum+up number solved goal within different execution+time limit be worth mention that solve goal include both passing truth test nonpassing falsity test as expect association+of+tennis+professionals solve more problem within execution time+limit grammar%23ENT 1%2C281 proof 7%2C112 cq from proof correspond passing truth test 3%2C556 truth test proof correspond pass falsity test 3%2C556 falsity test number problem solve within grammar%23ENT grammar%23ENT grammar%23ENT be respectively however most outstanding result be that number solved goal correspond falsity test be pass falsity test not significantly increase execution time+limit pass falsity test within grammar%23ENT grammar%23ENT respectively imply that association+of+tennis+professionals solve more more complex truth test as execution time+limit become long whereas proof pass grammar%23NPR www.vprover.org grammar%23NPR adimen.si.ehu.es%2Fweb%2Fadimensumo test be very simple detect within really short execution time+limit this conclusion confirm by result report figures number used axiom figure sum+up number different axiom use all proof conjecture figure sum+up average number different axiom conjecture use proof as before number different axiom use pass falsity test within grammar%23ENT grammar%23ENT be similar different axiom respectively by+contrast different axiom use in+passing truth test within grammar%23ENT whereas different axiom use within grammar%23ENT grammar%23ENT respectively same way more than different axiom conjecture use proof pass truth test within grammar%23ENT whereas around different axiom conjecture use all remaining case
sum+up seem that solve goal correspond pass falsity test require less knowledge than solve goal correspond passing truth test number proved truth test increase along execution time+limit whereas number proved falsity test do not thus be clear that grammar%23NPR sumo grammar%23NPR provide knowledge solve goal correspond complex truth test but state-of-the-art require longer execution time+limit
happen that either goal remaining falsity test be not provable b ontology not contain knowledge solve additional falsity test goal c state-of-the-art require much longer execution time+limit solve falsity test goal
type axiom knowledge use proof figure sum+up number axiom from top level middle sumo use proof distinguish between atomic formula atomic formula around 2%2C034 general clause general clause from top level sumo use proof pass falsitytests within all considered execution+time limit grammar%23NPR 2%2C034 general clause general clause use passing truth test within execution time+limit grammar%23ENT middle level sumo around 2%2C601 general clause general clause use proof nonpassing falsity test within all considered execution+time limit 2%2C601 general clause general clause use passing truth test within execution time+limit grammar%23ENT sum+up top level axiom 2%2C654 axiom middle level axiom 3%2C576 axiom already use proof passing truth test since cq construct consider that ratio between number used axiom number proof correspond passing truth test be really high
number different top level axiom use proof be high than number different middle level axiom but difference be even big when consider average number different axiom from top middle level use proof pass truth test around general clause from top level general clause from middle level this result be not surprising since knowledge in+the+middle level sumo define on basis knowledge top level
concluding remark our experimental result prove that application methodology propose grammar%23NPR ontology successfully automatize by use in+particular encourage that state-of-the-art be able construct complex proof solve cq within relatively small execution+time limit
result also enable evaluate usefulness axiom by check use when solve cq proof obtain within execution time+limit grammar%23ENT different axiom use 7%2C420 axiom from are 4%2C635 are general clause 2%2C785 general clause consequently 6%2C449 axiom total not use any proof among axiom use some proof axiom axiom use more proof axiom axiom use proof more goal correspond truth test imply that set axiom be really useful pass truthtests seem be correct further since remaining axiom axiom use less+than proof also conclude that quality proposed set cq be high because proof involve different set axiom
future work plan improve coverage set cq knowledge ontology in+particular knowledge in+the+middle level sumo by develop new method obtain cq semi-automatic way
acknowledgment be grateful anonymous reviewer their insightful comment this work partially fund by spanish project skater grammar%23NPR comma grammar%23NPR basque project grammar%23NPR grammar%23NPR grant grammar%23NPR grammar%23NPR
reference grammar%23NPR reengineering ontology first-order reasoning
j. semantic+web inf. syst. 80+116
improve competency first-order ontology j. grammar%23NPH
rez editor grammar%23NPR 8th int. conf. on knowledge capture grammar%23NPR grammar%23ENT association+for+computing+machinery
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH v. grammar%23NPR grammar%23NPH version reference+manual grammar%23NPR stanford+university computer+science department logic group
methodology design evaluation ontology grammar%23NPR workshop on basic ontological issue knowledge sharing grammar%23NPR grammar%23ENT
towards standard upper ontology et+al. editor grammar%23NPR 2nd int. conf. on formal ontology information+systems grammar%23NPR grammar%23ENT page association+for+computing+machinery
link lexicon ontology mapping wordnet editor grammar%23NPR institute+of+electrical+and+electronics+engineers int. conf. on inf. knowledge grammar%23NPR grammar%23NPH grammar%23ENT volume page 412+416
standard grammar%23ENT from sigmakee.cvs.sourceforge.net%2Fsigmakee sigma
task based approach ontology evaluation grammar%23NPR workshop on ontology learning population grammar%23NPR grammar%23ENT
design implementation vampire ai communications 91+110
e brainiac theorem prover ai communications 111+126 grammar%23ENT item appear same group share underlying commonality beside dimensions use their grouping commonality detect graph linked data form chain rdf property value share by large part item group explain call chain candidate explanation
link data traverse use uri dereferencing grammar%23NPH iteratively expand graph use strategy try find new more accurate chain evaluate ad-hoc f-measure finally linked data pathfinding approach use establish relation between candidate explanation criteria criteria group item in+order+to identify most plausible explanation while remove noisy result
very short example want understand why people search actor grammar%23NPH only at specific grammar%23NPR explanation if be familiar context identify as people search grammar%23NPH when grammar%23NPH movie release while human use own background+knowledge derive this therefore explain phenomenon grammar%23NPH use from linked data problem here be that if human be not familiar context grammar%23NPH not find above statement as plausible explanation statistically computing explanation on linked data graph have also drawback especially because some coincidence emerge set candidate explanation e.g. people search grammar%23NPH when be on
example above understand that explanation not only mean put two event correlation x happen because+of y but also define causal dependence base on context occur x happen because+of y some context c question be then whether be enough define explanation if not else need e be complete broad sense this work aim at identify be important actor explanation i.e. element interaction need let declare that give pattern derived observation raise explanatory level not only pure correlation our challenge consist then design ontology pattern formally represent in+order+to do so explore how concept explanation define within discipline embrace cognitive+science section find their commonality difference view explanation finally extract apply section ontology design pattern define e
define explanation survey cognitive+science firstly define by as science understand nature human mind this work date cognitive+science present as field involve series discipline interact such way represent as cognitive hexagon figure next section provide summary how main trend discipline try define explanation be worth mention that be grammar%23NPR search peak show by tool at www.google.com%2Ftrends%2Fexplore%23q%3Ddaniel%2520radcliffe grammar%23NPR release some grammar%23NPH movie more than one indeed happen at+the+same+time
cognitive hexagon as dotted+line be weak connection between discipline
choice our methodology but work especially contemporary span over multiple discipline additionally choose consider broad more common field computer+science sociology rather than original artificial+intelligence anthropology common nomenclature also adopt be define explanandum nda that explain fact event explain explanans tia that do some premise explanandum
explanation philosophy work provide interesting survey on how explanation define by intellectual from ancient times first attempt discussion already appear among greek intellectual contemporary day
history peloponnesian+war define explanation as process where fact indisputable data observe evaluate base on our knowledge human+nature then compare in+order+to reach generalised principle why some event occur grammar%23NPH grammar%23NPR grammar%23NPR explanation be expression knowledge use logos compose by form consist abstraction entity know unchanging unseen world fact i.e. occurrence state+of+affairs state+of+affairs be changing world use see grammar%23NPH posterior analytics present explanation as deduction process find+out cause why happen according+to grammar%23NPH case doctrine cause be either thing matter form end change initiator modern age determinism causality become prominent explanation know cause event mean know why happen turn mean understand universe determine by natural+law grammar%23NPH grammar%23NPH describe explanation as process demonstrate mechanical interaction between god primary efficient+cause all thing world thing secondary cause law+of+nature some initial condition grammar%23NPH reject god component put law+of+nature as play central role explanation natural+phenomenon be deductive process find most general principle fundamental law+of+nature that account empiricist then reject explanation explanation not trace directly experience according+to grammar%23NPR all causal knowledge stem from experience causation be just regular succession fact discover by experience grammar%23NPH reintroduce metaphysical component into explanation believe that knowledge start experience but not arise from shape by category understanding form pure intuition space time grammar%23NPH reject this view define e as only based on law+of+nature on deduction fact explain by deduce cause cause be by state law be instance grammar%23NPH revisit extend grammar%23NPH work start contemporary discussion around explanation explain event be show how this event expect happen take+into+account law law govern occurrence as+well+as certain initial condition formally speak singular explanandum e explain if only if description e be conclusion valid deductive argument explanantia involve essentially set c initial antecedent condition law like statement l l be either statistical inductive statistical explanation law+of+nature deductive-nomological explanation salmon further extend inductive statistical explanation model propose statistical relevance model where explanation capture dependency between explanandum explanans statistically causal model model see explanation as mechanical processes interact component interactive view be also interventionist where explanation be process manipulate explanandum within space alternative possibility see if relation hold when various other condition change grammar%23NPR approach grammar%23NPH grammar%23NPH affirm that explain phenomenon be see as instance broad pattern similar phenomenon according+to grammar%23NPH explanation base on both scientific understanding world but also on some assumption i.e. regularity nature be basic unifier similarly grammar%23NPH believe that a+number+of explanatory pattern schema need be unified in+order+to explain fact
explanation psychology explanation psychology generally mean regulate behaviour process some information i.e. be attempt understand phenomena relate intelligent behaviour work psychology criticise traditional philosophical scientific approach explanation since be too+much focused on causality subsumption law deduction instead psychological explanation accept over determinism dynamism i.e. antecedent condition condition explain phenomenon be one change over time explanantia be not law+of+nature but human capacity that psychological methodology discover confirm literature psychological explanation define as model provide law by+means+of be possible describe phenomenon while relate other similar phenomenon possibly allow prediction intend as future occurrence phenomenon
highlight how mechanical aspect within explanation process play central role into recent psychology work phenomenon often explain by decompose into operation localise part component mechanism mechanism have spatial temporal organisation organisation explain how entity organise into level carry+out their activity in+other+words explain phenomenon means reveal internal structure by define organisational level level responsible produce then identify how level relate other level finally build model explain similar phenomenon this idea interactive level found major contemporary psychology trend i.e. connectionist bottom-up approach that first specify system component then produce explanation model symbolic top-down one that first identify phenomenon explain then describe accord external symbol syntacticosemantic rule mathematical law manipulate by human
explanation neuroscience neuroscience explanation reject concept level as+well+as synthetic priori framework mentality be central pivot between interpretation rationality neuroscientist aim at understanding mind require assume that belief be true rational be contrast many philosophical view explain neuroscience mean fit mental phenomenon into broadly rational pattern explanation be answer whatif question describe happen some variable effect when manipulate intervene on some cause be upper level explanation when cause be macroscopic variable environmental+factor low level explanation if cause be more fine-grained neural genetic biochemical mechanism also explanation not require any law sufficient condition be simply generalisation describe relationship stable some appropriate range intervention therefore typical approach explanation consist detect how variable respond hypothetical experiment some intervention occur then establish if other variable same intervention respond same way relevance explanation assess by their stability i.e.
relationship be more stable than some intervention some background condition
explanation computer+science artificial+intelligence ai explanation consider term inference reasoning program mean decipher connection between events in+order+to predict other event represent some order universe process explanation fit two reasoning pattern abduction induction that grammar%23NPR describe lecture on pragmatism according+to grammar%23NPR induction be inference rule major+premise start from case minor+premise result conclusion while abduction be inference case from rule result model often put same umbrella term deductive posteriori inference same grammar%23NPR later period see abduction as only one phase process hypothesis idea generate but then need evaluate by induction deduction move+on from such discussion approach generate explanation see extensive survey show that two inference similarly model so that one body initial knowledge divide into observation prior knowledge map another new knowledge constraint certain criteria reveal their connection confirm hypothesis explanation process then consist first analyse observation then define tentative hypothesis finally prove empirically similarly explanation psychology artificial+intelligence explanation not assume that truth antecedently accept prior knowledge in+order+to produce new knowledge need be scientifically empirically evaluate plausibility usefulness desirability consider too subjective replace by some formal constraint that process respect namely efficiency how easily solve problem deducibility introduce new knowledge consistency be consistent already know emptiness introduce new knowledge work attempt merge view from philosophy artificial+intelligence survey explanation also play central role other area computer+science such as knowledge+management
system use intelligent tutoring decision support explanation provide insight into how their reasoning accomplish user present solution explanation problem solve rely on some domain knowledge see reference work this domain knowledge relate fact procedure events define as tacit because be already individual mind inflow new stimulus then trigger cognitive+process cognitive+process produce explanation explanation be explicit knowledge communicate symbolic form
intend as give meaning interpretation make understandable
explanation weber grammar%23NPR founder major two trend sociology explain be act give meaning justify social fact where intend as observable regularity behaviour member society their perspective explanation be rational empirical observation social behaviour more contemporary work extend this view by define explanation as comparison events as form reasoning take account social world social behaviour constantly evolve in+time space generally agreement be that explanation base on social fact have more pragmatic requirement with+respect+to scientific explanation formal constraint such as mathematical formalisation empirical falsifiability leave space description approximation complex structure social world explanation be weak strong pending on set social impact that rule express for+example be proverb everyday rule express know social theorem rule sum+up human experience paradox have also distinguished between social anthropological religious explanation where religious mean give sense phenomenon use symbol tradition explanation declare be valid if phenomenon also appear due other circumstances logically exhaustive if be generalisable logically possible combination condition encounter
explanation linguistics linguistic explanation be description focus on find what rule language explain linguistic fact
not aim at describe processes but rather at explain how language acquisition be possible by identify grammatical pattern pattern occur human language useful survey on explanation linguistics be found central assumption linguistic theory be that language be set element word morpheme phoneme meet set wellformedness condition call language universal consist languagespecific characteristic form grammar language major approach generative linguistics following grammar%23NPR theory functional linguistics base on grammar%23NPR school have different view on role function component linguistic explanation former case only generalisation explanation regard as interesting since show that linguistic construction be derivable from general regularity language descriptive adequacy but regularity be innate accept priori explanatory adequacy generativist see act explain as define new constraint on framework framework be able describe language this claim innateness view explanation as constrain description not accept by functional linguistics external element reality be essential understand linguistic fact functionalist be more interested language regularity than descriptive framework rejection priori language universal not mean that language structure be arbitrary but rather than be worth attempting explain wherever possible language fact use external linguistic factor
summary consideration from section above briefly summarise following conclusion
philosophy be discipline ask question check answer explanation philosophy be process where set initial element event some initial condition describe deduce put into relation output phenomenon accord set empirical metaphysical law
psychology try understand human animal cognitive+process here e intend as model component entity activity mechanically interact produce regular change from start condition termination condition
neuroscience give understanding brain explanation see as generalisation describe relationship between input output variable variable be stable some appropriate range intervention without any law sufficient condition
computer+science deal computer+program as medium perform human operation explanation be generalisation start knowledge some fact some prior knowledge map another new knowledge constraint certain criteria
sociology focus on link between human cognitive+process sociocultural world grammar%23NPR explanation ontology design pattern
as act give meaning some social regularity occur because+of some social behaviour
linguistics provide theory deal nature human language explanation be process derive regularity language grammar from set linguistic specific fact respect some wellformedness constraint
premises be possible abstract common model e. linguistic formalisation found particularly suitable our purpose be present hereafter when x happen then due given set circumstances c y occur because+of given law l
next section focus on design ontology pattern represent e as+well+as instantiation various presented discipline
explanation design pattern ontology design pattern be small motivated ontology exploit as building+block ontology design
purpose be use as design component problem assumption that ontology model accord pattern appropriate dependency between plus some necessary design expansion base on specific situation needs follow this idea build ontology design pattern pattern represent explanation reuse existing design pattern ontology then extend class properties accord our needs resulting model that call grammar%23NPR show figure described grammar%23NPR
ontology design pattern shortly define grammar%23NPR as ontology characterise be explanation event include various component component make process plausible for+instance initial condition explanation happen outcome event and+so+forth
main class explanation represent e aim identify differently instantiate accord discipline present class have few constraint represent as owl class restriction in+order+to be complete explanation needs at+least one antecedent event express as grammar%23NPR some part event ii require posterior event grammar%23NPR thanks+to grammar%23NPH help design presented pattern
express as grammar%23NPR some part event iii happen context relate two event express as grammar%23NPR some situation situation
b explanation have then antecedent event explanans posterior event explanandum see how call not only event but also phenomena fact variable observation knowledge etc. choose reuse part event class from enable represent any binary relation between object event thanks+to our pattern be easily integrated participation one by extend part event individual accord structure participation odp
c two event be related context otherwise be not possible distinguish their correlation from coincidence represent condition constraint two event occur use situation situation class from precisely design represent context situation thing thing have common associate see documentation owl axiom blue box above object property grammar%23NPR be syntax axiom show infer about situation explanation happen if there exist situation explanation contextualise through grammar%23NPR then both explanans explanandum share this same situation through situation grammar%23NPR therefore be same context
d last main component explanation represent use dul theory class from grammar%23NPR according+to documentation theory be description represent set assumption describe usually general scientific philosophical common-sense theory include here use term theory idea that best embrace concept define cognitive+science law human capacity human experience universal and+so+on consist have binding force on some events analysis note that theory be specialisation description class from grammar%23NPR ontology this view our theory act as description classify explanation corresponding grammar%23NPR ontologydesignpatterns.org%2Fcp%2Fowl%2Fparticipation.owl grammar%23NPR www.ontologydesignpatterns.org%2Fcp%2Fowl%2Fsituation.owl grammar%23NPR www.ontologydesignpatterns.org%2Font%2Fdul%2Fdul.owlfigure instance pattern philosophy leave psychology right
build upon context event explain possible explain event
e also include dolce class dul agent represent produce explanation since be no agreement on how call act explain e.g. deduce induce infer describe use literature then keep generic object property label as dul grammar%23NPR
instance create instance grammar%23NPR accord our finding survey section present ontological model each discipline as+well+as practical example example give idea how process work
example use notation e p c t where stands antecedent event explanans p posterior event explanandum c situational context happen t theory govern events purpose avoid redundancy class dul agent omit note that all presented pattern submit grammar%23NPR central grammar%23NPR but be also available online high grammar%23NPR
pattern e as see philosophy present figure leave t be law also subspecified as metaphysical empirical try sketch general view area roughly pre- postempirical view both p be phenomenon class while c be necessary sufficient condition class be worth mention that be mere generalisation how explanation think philosophy that be in+turn specialised into more specific pattern accord for+instance different time trend for+example represent mill explanation model as grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR while grammar%23NPH model as grammar%23NPR grammar%23NPR grammar%23NPR circumstance form
pattern e as intend psychology present figure right show section how psychological explanation more focus on their mechanical aspect therefore i grammar%23NPR class subclass be class entity activity context c be human grammar%23NPR correlate p. theory consist grammar%23NPR not law+of+nature practical example simple event p as slip on banana explain by fact that person do not pay+attention path psychological explanation attempt relate grammar%23NPR ontologydesignpatterns.org grammar%23NPR people.kmi.open.ac.uk%2Filaria%2Fexperiments%2Fexplain odp two events c mental+state feeling person slip as for+instance anger tiredness if this condition not meet anymore e.g. be calm careful explanation not hold human capacity capacity govern example be ruling human mind so that human+action change depend on feeling
remaining pattern show figure clockwise order
idea explanation neuroscience be similar psychology difference that event consider as grammar%23NPR grammar%23NPR p see how neuroscientist reach explanation by experiment s t rather than law that necessary+condition c be rather called intervention s two event occur if change nullify explanation for+example explain why human do math calculation p by fact that some particular neuron respond actively quantity
neuroimaging prove that t some area on parietal+lobe use provide pre- verbal representation
context c explanation hold be numerical+calculation
model explanation computer+science easily represent as communications+security+establishment observation grammar%23NPR constraint grammar%23NPR note that new knowledge be grammar%23NPR if explanation reach by induction grammar%23NPR if reach by abduction use here example give by caveman roast lizard c on end pointed stick caveman watch by amazed crowd caveman caveman use years only their bare hand caveman explain that painless roasting p be effect use long rigid sharp object simply by observe equivalent use prior knowledge t how stick support lizard while keep+away hand away from fire
sociology reintroduce human component into idea explanation represent as se grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR social trend p italian live their parents until later age explain by constantly decreasing job+opportunity job+opportunity make impossible living on c theory behind be rule govern socio- economical behaviour
explanation linguistics be model focus on function nature language namely series grammar%23NPR s specific language analyse on basis some grammar%23NPR s model news consumption be graph structure try represent entire context news+story where node name entity edge represent relationship among according+to hypothesis state national+security+service model follow schema concentric entity layer national+security+service aim exploit harmonize figure concentricity news+item fugitive grammar%23NPH apply asylum single conceptual model different semantic relationship establish between news entity this model base on finding make make explicit duality entity via two main layer namely core crust
former compose small number key entity be essential identify story entity have high potential better summarize main fact behind news+story frequently mention related document therefore spottable via frequencybased function figure core compose entity grammar%23NPH united+states+of+america grammar%23NPR airport where action take+place be seed good understanding story on+the+other+hand crust compose entity express particular details around news+item mention some specific related document but be not always spottable via frequency based measure their relevancy instead ground on existence relation such as popularity serendipity among entity core figure some entity such as grammar%23NPH grammar%23NPH lawyer be not so prominent at first glance but definitely play role story contribute good understanding fact semantic context news+item therefore build by combine core crust into single data+structure grammar%23NPR grammar%23NPR in+order+to go deep formalization national+security+service focus on some+other aspect this conceptual model be important news consumption scenario
relevancy dimensions concept relevancy be extremely wide complex depend on several variable that two different person person ought judge relevancy entity rarely agree on however layer based representation use nss better support complex multi dimensional relevancy relation establish among entity involve news+item allow formalize potential reason reason link core crust become then place host different relevancy dimensions dimensions bring diversity story description entity denote opinion informativeness serendipity popularity interestingness unexpectedness
predicate entity relation discuss importance discover explicitly establish relation among entity inside national+security+service propose now step further by consider not only unlabeled relation but also explicit predicate characterize entity link find such property name formalize entity dependency be still open challenge
co-occurrence entity document collect from reveal how tight be relations context story further analysis document help provide additional information enable label predicate
nss be graph based structure this information straightforwardly incorporate into model predicate establish between element inside become labeled link thanks+to flexible nature model
exploit such kind annotation in+order+to make user aware reason make entity relevant
story over time case different fact fact shape story plot happen chronological order imply that entity predicate involve such fact be especially relevant during particular period+of+time within time+span story once+more flexible graph based nss model support edge annotate temporal reference in+order+to reflect when particular entity play important role plot story hold specific relationship track evolution relationship in+time open room timeline based summarization prototype highlight milestone characterize story evolution
news consumption paradigm potentially be numerous way consume news+story grammar%23NPR each tool application display information about news+item follow different philosophy target different audience present main fact from different angle despite this variety alternative this section propose model classify news consumption approach reference time user actually consume news document dnews describe story grammar%23NPR identify three main phase before during phase user behavior be different be evolution understanding story consequently information requirement application present story
before user this phase normally not consume main news+item yet so their understanding about story be limited times require quick easy way interpret main fact so get+to+know glance whatthe news talk about other case additional content display in+order+to illustrate news context this type recommendation look content be very similar dnews leave diversity aside special application category this phase include advanced summary summary aim fully tell story without consume original content
during correspond time user watch main document illustrate story dnews normally imply passive information activity where user be pretty much focused task consume document without engage any other action during this phase user knowledge grow from background information provide before phase most detailed understanding news good example prototype this category be second screen application aim illustrate say on news minimal user interaction
user become fully aware basics story want go deep into details switch active mode browse description entity categorize into dimension ultimately jump other related story
level interaction drastically increase since user become more engage move by curiosity discover more detail main document dnews enrich additional content focus on diversity detail some specific fact grammar%23NPR other application fall same consumption phase advance interactive summary browse capability
grammar%23NPR formulate as hypothesis that national+security+service news+item be knowledge+representation model effectively capture context story support different existing news application this graph layer based structure help populate very diverse prototype aim support user interpret news sake illustrate our hypothesis without claim provide exhaustive plot base on quantitative data figure show how duality between core crust concentric model better satisfy evolution user consumption needs different consumption phase as result change aspect like viewer knowledge about story user engagement content diversity
define formula+one section entity core usually express most general upper-level concept concept drive story behind news+item even if entity be present almost every stage news consumption have strong decisive role before phase leave side figure decrease importance as dnews consume visualization come+into+play
during be special phase phase require both core crust entity middle part figure specially when mention document dnews in+particular core entity start less demand since often consume during before while crust entity start bring added+value reveal obvious fact around story plot
last phase consumption process so-called figure core crust usage along different consumption phase user have already fair understanding news+story entity core be highly present during previous phase become often too obvious be therefore not so critical use instead entity crust bring particular detail user want consume attempt move from general understanding news+item explore specific story detail right side figure since crust consider different relevancy dimensions application easily move along bring diversity desire at latest stage consumption process in+addition application display timeline based summary propose additional hypothesis state that core remain stable in+time have less interest while crust contain entity entity bring stand-out information particular period+of+time need display
ecosystem news application this section review existing application prototype consume news classify accord different consumption phase identify section analyze how benefit from graph representation model like in+order+to make first qualitative evaluation our hypothesis
before consume news+item
present approach get quick overview video content enable user decide if user be interested not story automatically select some fragment within video call hotspot hotspot contain annotation high frequency score this notion representativeness clearly align definition core entity inside this layer straightforwardly use hotspot creation
similar occur where some small video fragment be hyperlinked base on some visual topical similarity even if such task tackle use multimodal analysis technique application at early+stages news consumption focus mostly find content as similar as possible original therefore entity core be suitable trigger search on documentindexes where that additional content find
during consumption news+item present second screen application implement slideshow slideshow give user access factual information about person type location-type grammar%23NPR entity entity relate news+story display main screen entity still show when mention video illustrate story as+a+whole but be increasing use other entity clarify more specific fact video as display entity give absence national+security+service fee such prototype develop first implementation entity expansion algorithm in+order+to recreate very basic version crust
consume news+item
find example application illustrate story consume news+item active mode demo target idea user user want further dig into detail story via some additional content propose along different dimensions additional content facet easily match layer envision crust definition like opinion from expert align opinion other source align informativeness reveal importance multi layer philosophy inside
also include application application offer advanced interactive visualization summarize entire context story extremely challenging task even when perform by expert domain representation model offer by national+security+service consider relation between entity help implement conceptual diagram where entity relate each via particular connection visualize like
third example this category be time based representation break+down story relevant fact chronologically represent over x+axis analyze story italian election grammar%23ENT during one week voting process main difficulty encounter during implementation be filter+out entity entity buzz during entire week so become obvious like promote instead entity entity peak relevance during certain moment week particular reason like grammar%23NPR have meeting italian president on grammar%23ENT this phenomena reinforce our hypothesis about core entity remaining stable in+time becoming useless such timeline prototype while one crust bring interesting fact need show
prototype in+addition+to previous prototype be also other application target big portion news consumption phase spectrum good example find online demo grammar%23NPR offer innovative way consume grammar%23NPH talk by support user not only at pre- consumption stage via hotspot calculation but also during viewing through entity highlighting period just by link similar course other related grammar%23NPH chapter
conclusion different application consume news benefit from existence graph based model able capture linkedtv.eurecom.fr%2Fhyperted entire context story propose model call ground on existence semantic+relation between entity describe news+story this model have concentric nature consider two main layer core core include most representative frequent entity crust crust compose additional entity become relevant because+of certain relationship happen between core in+addition identify three phase news consumption process before during national+security+service support application fall inside category as support underlined hypothesis review our past prototype identify challenge face when capture story information explain how use national+security+service feed their needs lead good implementation
future work plan experiment new technique automatically populate model aim use existing domain specific knowledge+base further contextualize relationship present national+security+service exhaustively evaluate be main relevancy layer inside crust consider ensure user get+the+best consume experience possible
this work partially support by grammar%23NPH within grant number grammar%23NPR by european+union 7th framework programme via project grammar%23NPR ga
reference et.+al.
fine-grained hyperlinking video within closed collection use scene segmentation 22nd on multimedia united+states+of+america
concentric nature news semantic snapshot 8th international conference on knowledge capture grammar%23NPR
v. grammar%23NPR track analyze grammar%23ENT italian election 10th demo track page 258+262
grammar%23NPH l. augment tv newscast via entity expansion
11th demo track page 472+476
grammar%23NPH l. grammar%23NPH generate semantic snapshot newscast use entity expansion 15th
grammar%23NPH find share hot+spot video 13th international semantic+web conference grammar%23NPR demo track
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
what fresh media be looking retrieve media item from multiple social+network first international workshop on socially aware multimedia grammar%23NPH page
t. grammar%23NPR grammar%23NPH a. m grammar%23NPR automatic selection social+media response news 19th on knowledge discovery data+mining grammar%23NPR page grammar%23ENT identify prominent life+event on open university university united+kingdom grammar%23NPH university united+kingdom open university united+kingdom united+kingdom united+kingdom abstract social+media be common place people post share digital reflection their life+event including major event such as get+married have child graduate etc.
creation such post be straightforward identification event on online media remain challenge
research recent year focus on extract major event from such as earthquake storm flood this paper however target automatic detection personal life+event focus on five event psychologist find be most prominent people live define variety feature user content semantic interaction capture characteristic life+event present result several classification method automatically identify event our proposed classification method obtain result between f1 measure different type life+event novel contribution this work also lie new corpus tweet annotate by use crowdsourcing constitute best our knowledge first publicly available dataset automatic identification personal life+event from
subject descriptor grammar%23NPR information+systems application miscellaneous grammar%23NPR database+management database application data+mining general+term social+media life+event introduction billion social+media user nowadays post about their daily life friend follower while wide+body research social+media focus on event detection around permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815845
events such as news+story earthquake music gig work focus on detection common prominent life+event automatic detection prominent life+event social+media be still relatively new research topic main body+of+work focus on classifying tweet about one two different type life+event however benefit this line research be numerous besides more obvious area such as profile marketing product recommendation more novel use case start emerge recent year introduction brief automated biography grammar%23NPH such as me while still their infancy regard how select content tool be example way user recollect their digital life log data
paper aim provide step+forward this direction by study automatic identification prominent life+event while other work look+at identify taxonomy life+event automatically base our research on previous work from psychology literature where identify set common life+event out event select top five five have child beginning school marriage parent death falling in+love our research here aim at automatically identify top life+event perform this study make following contribution demonstrate how collection user content semantic+feature semantic+feature commonly use social+media analysis apply identification prominent life+event study novel set interaction feature feature consider historical interaction among user in+order+to determine whether unusual pattern contribution towards post indicate that post refer specific life+event test machine+learning approach automatically identify five prominent life+event from psychology evaluate role different subset feature on characterise event contrast our finding existing studies on life+event social+media generate make publicly available new corpus 2%2C241 tweet manually through grammar%23NPR www.intel.com%2Fmuseumofme%2Fr%2Findex.htm grammar%23NPR www.facebook.com%2Flookbackflower.3 best our knowledge be first publicly available dataset life+event identification state+of+the+art event detection use social+media be not new research area work go+into extract event from social stream use various methodology use twitter identify earthquake by treat tweeter as sensor tweet as reading identify live news+event use twitter by identify reliable user user tweet about such topic grammar%23NPH develop system call grammar%23NPR build signal individual words then use wavelet analysis modularitybased graph partitioning clustering signal together look+at identify gig by consider frequency image post grammar%23NPR within bounding+box around know gig venue
close relation our work at extract life+event less research do grammar%23NPH look+at identify two type life+event marriage employment their methodology focus on utilise only unigrams test over variety different classification algorithm also consider feature like retweets but find little improvement over unigrams more recently grammar%23NPH tackle three major challenge life+event detection taxonomy life+event noiseness data lack training data approach by seed initial dataset tweet tweet have common key reply like congratulation then use grammar%23NPR human identification method construct their training set
then look+at use grammar%23NPR classifier compare bag words grammar%23NPH pos feature our paper extend state+of+the+art by investigate six life+event psychologist identify as most common amongst people
data+collection annotation mention earlier our selected life+event be common five identified by child grammar%23NPR starting school south+carolina death parent dop
twitter data+collection in+order+to seed our initial dataset decide construct several query our life+event one particular challenge be reduce bias selection query word for+example get+married use only query word wedding marriage church omit large+number tweet that use different tense synonym our approach look+at split each event into minimum number concept then use wordnet find related term in+addition+to also extract slang phrase use grammar%23NPR different tense each verb grammar%23NPR then create set word root concept permutate grammar%23NPR www.crowdflower.com grammar%23NPR onlineslangdictionary.com%2Fthesaurus grammar%23NPR www.verbix.com each generate our final query each query generate also suffix en help select only english write tweet
life+event root concept life+event concept child child birth get+married marriage death parent death parent starting school start school falling in+love love collect data our study take+advantage recent indexing infrastructure upgrade by write scraper capable parse twitter front-end search allow search tweet no limit on date
then use twitter lookup api extract all standard meta+data associate tweet
set extraction limit 1%2C000%2C000 tweet life+event split this limit evenly amongst total query available event grammar%23NPR use store initial dataset
data annotation annotate our final dataset use grammar%23NPR as our annotation tool grammar%23NPR be online crowd source annotating platform where upload dataset accompany by question crowd make judgement on
goal be label tweet two piece information be tweet about event be tweet about our proposed theme category e.g. getting married by use this divided approach our dataset then have dual purpose as both general event classifier topic classifier future work intersect two set answer where be yes consider this list be tweet about our selected life+event
help obtain good result run several small trial annotate around tweet at time fine-tune our question main issue find both question be subjectivity case classifying tweet about event one annotator consider that go shop be event while confuse that be just mundane action give our very general definition that event be happen at some time first user be correct however be easy see why many user argue be not case answer if tweet be about particular topic theme find that many user only answer yes if be only explicit for+example tweet that be about get+married from wedding photographer perspective incorrectly classify as not be about get+married some case people also get+confused about whether not tweet need be event as+well+as related theme avoid problem problem appear during annotation trial refine our final grammar%23NPR question as grammar%23NPR be tweet related particular topic theme grammar%23NPR be tweet about important life+event case grammar%23NPR provide list example event take from work by give this extra guidance user found eliminate lot subjectivity previously experience example grammar%23NPR www.crowdflower.com%2Fin reminisce post such as event
summary while be still extensive room future work our experiment result show that different feature use accurately identify personal life+event social+media help therefore manage our digital personhood hope that presented study serve as basis future work within community enable further research into automatic identification management our digital identity
conclusion although much research do on identify global event from social+media very+little attention give prominent life+event
paper be first tackle this issue by develop method automatically identify six type life+event found be amongst most common highly memorable events by psychologist event begin school falling in+love marriage have child parent death
study this paper how different user content semantic interaction feature use characterise identify personal life+event apply machine+learning approach automatic event identification
train proposed feature obtain result between f1 measure
contrast role different feature conclude that content feature be most discriminative feature identify life+event negative polarity high interaction ratio be characteristic death parent event positive sentiment reference other user be characteristic have child event post semantic concept around decorative+element be characteristic get+married post common simple vocabulary be characteristic start school post about falling in+love tend show positive sentiment additionally identify set n gram be most discriminative each type event also find almost no effect interaction feature on life+event detection when apply
additional contribution this work lie generation dataset personal event detection this dataset make publicly available community
reference grammar%23NPH grammar%23NPH how important life+event disclose on relationship likelihood sharing privacy
behavior social+networking
grammar%23NPH
measure user influence twitter million follower fallacy grammar%23NPR int. on weblog social+media grammar%23NPR united+states+of+america
personal life+event detection from social+media workshop on social personalisation hypertext
detect life+event feed from grammar%23NPR on semantic computing grammar%23NPR united+states+of+america grammar%23ENT institute+of+electrical+and+electronics+engineers

live news+event use twitter grammar%23NPR 3rd on location-based social+network grammar%23NPR page new+york new+york united+states+of+america association+for+computing+machinery
age effect cultural life script apply cognitive+psychology 291+298
grammar%23NPH grammar%23NPH major life+event extraction from twitter base on congratulation condolence speech+act grammar%23NPR grammar%23ENT grammar%23NPR on grammar%23NPR
use social+media identify event 3rd workshop on social+media grammar%23NPR association+for+computing+machinery multimedia grammar%23NPR
mining comparing engagement dynamics multiple social+media platform grammar%23NPR grammar%23ENT association+for+computing+machinery grammar%23NPR on science united+states+of+america association+for+computing+machinery
on stopwords filtering data sparsity sentiment analysis twitter grammar%23NPR 9th int. grammar%23NPR on language resource evaluation grammar%23NPR grammar%23ENT
earthquake shake twitter user real-time event detection by social sensor grammar%23NPR 19th on new+york united+states+of+america
want be retweeted large scale analytics on factor impact re tweet twitter network institute+of+electrical+and+electronics+engineers 2nd int. grammar%23NPR on social computing grammar%23NPR united+states+of+america

topic detection track english grammar%23NPR 5th int. workshop on on information+retrieval asian language grammar%23NPR page 165+172 hong+kong association+for+computing+machinery
b. s grammar%23NPH event detection twitter grammar%23NPR int. on weblog social+media grammar%23NPR grammar%23ENT propagation policy grammar%23NPH data+flow grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH open+university united+kingdom open.ac.uk universit%C3%A9 grammar%23NPR grammar%23NPR univ-paris13.fr cnr.it abstract govern life+cycle data on be challenging issue organisation user data distribute certain policy determine action allow circumstance assess policy propagate output process be one crucial problem description policy data+flow step imply huge number propagation rule be specified number policy time number action this paper provide method obtain abstraction abstraction allow reduce number rule significantly use grammar%23NPR ontology hierarchical organisation possible relation between data object compact knowledge+base set more abstract rule give definition policy propagation rule show methodology abstract policy propagation rule base on ontology how effective this methodology be when use grammar%23NPR ontology how this ontology evolve in+order+to better represent behaviour policy propagation rule
concept information+system semantic+web description language social professional topic computing technology policy computing methodology semantic+network applied computing law keyword formal concept analysis rwanda+defense+force license grammar%23NPR introduction govern life+cycle data on be challenging issue organisation user data distribute certain policy determine action allow circumstance smart city data hub information gather several different way span from loading static file very dynamic data like permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
isbn grammar%23ENT grammar%23ENT by sensor store retailer load frequency customer entering shop place car+park monitor by sensor register whether space be busy not etc. consequently information own by different user fix constraint on usage data provide specific terms licence in+addition smart city data hub play role mediator between data provider data consumer where consumer be company establish processes processes integrate manipulate analyse data in+order+to build service end-user way term licence propagate through operation therefore need manage
chief technology officer busy time want decide term use apply data deliver by their service perform estimatation busyness i.e. crowd density area given future time+frame hope sell this data company transport advertisement domain in+order+to do that access reuse set data+source among one be suitable use commercial purpose according+to licence assign by data provider company want limit customer reuse data for+example prohibit re- sell third party while do need be sure not invalidate policy that inherit from original data be still valid output process this context assess policy propagate output process be important difficult problem
processes represent as ontology within semantic+web framework be emerging information model support exchange policy on
grammar%23NPR grammar%23NPR design enable formally describe how application use data consist taxonomy ontological relation between data object
grammar%23NPR processes represent as network data object connect by semantic+relation description propagation rule establish in+order+to capture how policy represent propagate rich data+flow rule then tell be given circumstances set policy apply output complex data+processing task however have description policy data+flow steps imply huge number propagation rule be specified number policy time number possible+action
aim provide abstraction abstraction allow reduce grammar%23NPR www.w3.org%2Fcommunity%2Fodrl grammar%23NPR purl.org%2Fdatanode%2Fns%2Fnumber rule significantly
this paper show methodology abstract base on ontology how effective this methodology be when use grammar%23NPR ontology how this ontology evolve in+order+to better represent behaviour
next section describe background related work our research section definition give before introduce approach propose provide overview on methodology
describe how methodology apply use grammar%23NPR ontology section finally derive some conclusion last section
background governance be at centre current effort master information+flow modern smart city world link open data licence be important in+order+to enable conscious exploitation resource develop technology technology allow policy negotiate enforce be emerging information model support exchange policy on work at development set specification enable interoperability transparent communication policy associate software service data policy express model include deontic aspect odrl duty odrl permission odrl prohibition associate set odrl action rdf license database be notable effort towards establishment common database licence description base on rdf ontology provide by among
based representation reasoning on policy require in+order+to enable secure data access usage distributed environment particularly semantic+web
describe semantic+web use provenance ontology grammar%23NPH grammar%23NPH describe workflow execution terms agent action assets involve grammar%23NPR ontology design describe semantic+web application by means relation between data involve their process ontology be taxonomy possible relation occur between data object object be part process execution such as+one represent use therefore use further qualify implication action perform such process grammar%23NPR describe process implication data oriented way as network data object rdf license database grammar%23NPR be primary resource present work
propagation rule be horn rule reasoning on horn rule be one way deal policy particularly because allow treatable defeasable reasoning
have capability classify collection object depend on their feature
use ontology alignment for+instance our case use detect common behavior grammar%23NPR www.w3.org%2Fcommunity%2Fodrl relation terms policy propagation then test refine grammar%23NPR ontology farm+credit+administration apply as part method classify licence base on common feature reduce cost licence selection refer reader description grammar%23NPR tool implement farm+credit+administration as+well+as other functionality evolve concept lattice semantic+web ontology also part approach present here
approach describe this paper clearly relate principle method knowledge engineering knowledge+acquisition consider as iterative process model refinement be exactly how decide tackle our problem here more recently problem+solving method study in+relation+to task understanding process execution
policy process execution represent aim do here be investigate propagation policy data+flow
approach intent present work be verify extend be possible compact knowledge+base propagation rule use ontology ontology organize data+flow steps hierarchy this section outline methodology design this task while next section apply concretely
go into detail methodology be worth introduce concept as define be horn clause following form have x p propagate p r relation r x y have y p where x y be data object p be policy r relation between two data object for+example use represent fact fact download file f distribute attribution requirement result local copy d that also need use accord attribution requirement therefore above abstract rule instantiate as+follows have f attribution propagate attribution grammar%23NPR relation grammar%23NPR f d have d attribution in+fact reduce more compact form i.e. binary association between policy relation propagate policy relation as other part generate automatically from above representation
this definition obtain method generate knowledge+base rule from binary matrix relations policy be first assumption our methodology
second assumption be ontology be available organise data+flow steps hierarchy by means semantics for+example this ontology tell that relation grammar%23NPR be kind grammar%23NPR
methodology compose following phase a1 acquisition initial task be setup knowledge+base grammar%23NPR grammar%23NPR methodology
farm+credit+administration algorithm perform on knowledge+base grammar%23NPR output process be set concept cluster policy propagate same set relation order lattice
abstraction this phase search match between ontology farm+credit+administration lattice when match occur subtract rule rule be abstracted through ontology taxonomy
check extent hierarchical organization relation match cluster produce by farm+credit+administration developing measure this step evaluate how+much ontology compress knowledge+base i.e. compression factor detect partial match observe measure produce previous phase particularly about partial match this phase perform operation fix inaccuracy evolve ontology improve compression factor
above methodology consitutes iterative process as show figure
acquisition initial task be setup knowledge+base as illustrate at beginning this section conceive as association between policy relation between two data object
knowledge+base be then binary matrix where each row represent possible relation between two data object each column policy cell matrix be depend on whether policy propagate not given relation for+example matrix contain cell grammar%23NPR grammar%23NPR grammar%23NPR
each positive cell matrix generate populate set rule r
analysis objective second phase be detect common behavior relations with+respect+to policy propagation achieve by apply farm+credit+administration implement grammar%23NPR tool output farm+credit+administration algorithm be ordered set concept term each concept+group set object concept extent map set attribute concept intent farm+credit+administration concept group set object have given set attribute vice+versa our case each concept map group relation propagate group policy concepts organize hierarchically by farm+credit+administration lattice from top concept t include all object potentially no attribute bottom concept b including all attribute potentially empty extent set object
other concept order from top bottom
first layer concept right below t include large group object have few attribute common layer below have more attribute less object until bottom b reach our setting t include all relations no policy while bottom concept b include all policy but no relation
concept identify by farm+credit+administration collect relation have common behavior our knowledge+base r propagate same policy however not have explanation cluster in+other+words not know why do suitable abstraction not only have operational effectiveness but also be meaningful make hypothesis that ontology relation organize hierarchy by means their semantics contribute enlighten meaning concept
abstraction abstraction process base on apply ontology ontology organize relations hierarchy for+instance relation grammar%23NPR be sub- relation grammar%23NPR a+number+of policy propagate by grammar%23NPR also propagate by grammar%23NPR sub- relation that branch hierarchy by grouping all relations below grammar%23NPR transitive closure obtain cluster relation similar one farm+credit+administration concept call grammar%23NPR branch for+example expect branch ontology reflect cluster relation obtain by farm+credit+administration thus search match between ontology farm+credit+administration lattice when match occur subtract rule rule be abstracted
abstraction algorithm
rule c grammar%23NPR h grammar%23NPR grammar%23NPR c h c h p r match c h when p grammar%23NPR r policy c branch h process summarize list describe as+follows concept from result farm+credit+administration algorithm obtain set concept c including relation r extent concept propagate set common policy p intent concept c grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR rn pn ii branch each relation ontology compute transitive closure sub- relation obtain set mapping h grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR tn bn where t be relation ontology b related branch i.e. all sub- relation matching search partial overlap between branch h cluster c. measure possible match each branch each cluster evaluate precision recall case branch be fully cluster say match have high precision inversely recall indicate how+much branch cover cluster cluster including only relation branch result match maximum recall
compression when match have full precision use top relation as abstraction sub- relation all policy intent concept in+other+words remove all rule refer relation as imply by more abstract one
assessment result abstraction phase represent as set measure between concept portion ontology measure interest assessment phase be number relation concept size cluster
number relation branch be also present extent concept
branch size bs number relation branch
precision pre calculate as be bs mean degree matching branch extent concept
recall grammar%23NPR recall calculate as grammar%23NPR be i.e.
extent concept cover by branch
f measure formula+one f measure well-known fitness score calculate from precision recall as formula+one pre rec pre rec
measure now consider quantify qualify way ontology align propagation rule precision recall indicate how+much relation be close be suitable abstraction policy propagation table show example take from experiment see section
be worth mention some general consideration make by inspect measure
excerpt from table measure compute by abstraction algorithm list
be b pre rec formula+one branch 52+115 grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR 4+115 grammar%23NPR c concept id be bs branch size pre precision rec recall formula+one f measure
grammar%23NPR whole extent concept be branch branch also include other relation relation not propagate policy include concept
pre perform subtraction rule as list low recall indicate that high number exception still need keep rule set also reflect high from deduce low number policy concept as consequence inspect partial match high precision low recall highlight problem problem be easy fix as number relation policy compare be low for+example row relate relation bs be so need only check whether relation grammar%23NPR branch also propagate policy concept
perfect match between concept branch ontology be f1 however when not happen try improve approximation
general estimation effectiveness approach give by calculate cf as number abstracted rule divide by total number rule cf r r set rule set rule subtract depend on choose whether perform any adjustment knowledge+base any refinement ontology at+the+same+time indicate how+much ontology be coherent common behavior relation knowledge+base policy propagation rule
adjustment this phase try make adjustment in+order+to improve define set operation target fix error initial knowledge+base r as+well+as evolve ontology
fill sometimes branch be not completely cluster inspect missing relation realize that be diagnosis this operation illustrate by figure
ticked circle represent relations ontology be cluster given concept grammar%23NPR circle miss meaning not propagate policy concept fill operation make branch b be fully cluster concept c attempt push+up pre up achieve by add r all rule generate from association between policy concept c relation branch b. this change affect grammar%23NPR knowledge+base r increase number rule
fill operation diagram on left show diagnosis issue while diagram on right show+the+way repair figure instance pattern clockwise neuroscience computer+science sociology linguistics
grammar for+example explain that english c not allow expression my book p know that my are determiner that t only one determiner accept
pattern application on derive pattern e return our original motivating scenario analyse how+much grammar%23NPR framework fit into process derive explanation same approach of+course apply any other framework automatically produce explanation scenario already present section be following grammar%23NPR want explain fact p that group user search information about grammar%23NPH only at some very specific times need identify be constitute t c
first component grammar%23NPR use background+knowledge derive some plausible explanantia type+a event p. process consist find fact statistically highly correlate explanandum for+example both people search grammar%23NPH because grammar%23NPH movie release people search grammar%23NPH because be on correlate grammar%23NPH accord grammar%23NPR if no connection between events p identify however be no+way distinguish whether p happen together by coincidence as second case also explanation be incomplete impossible if have no knowledge context c be relation between grammar%23NPH grammar%23NPH
second component be therefore find this relationship form path between two entity represent fact for+instance grammar%23NPH grammar%23NPH grammar%23NPR test this scenario see linkedu.eu%2Fdedalo%2Fdemo provide more explicit information user for+example there exist path such as grammar%23NPH portray grammar%23NPH but not between grammar%23NPH this path explicits context c two fact happen without two event be but spurious+correlation
bring our conclusion point obtained explanation summarise as people search grammar%23NPH because grammar%23NPH movie release since grammar%23NPH be main character familiar reader recognise here law t govern explanation actor play movie therefore make+sense that people interest grammar%23NPH if be play grammar%23NPH movie grammar%23NPR be however not yet able provide same information user explanation present leave that grammar%23NPH be actor that grammar%23NPH be movie equally that actor play movie as general principle
conclusion this work be attempt formally define intend as explanation present such formalisation as ontology design pattern modelling achieve by analyse how different discipline cognitive+science perceive explanation i.e. element interaction need explanation be place proposed ontology pattern then instantiate accord discipline finally show how apply pattern on custom framework framework automatically produce explanation real-world scenario by show how assess whether system fit proposed pattern
identify as important future work introduction our based framework third component role be explicit law underlie relationship between events so that explanation pre-sented user consider complete according proposed grammar%23NPR another direction explore be possibility refine pattern by introduce quality be necessary explanation e.g. validity truth and+so+on by explore new perspective e.g. other discipline
evaluation other existing framework on basis our ontology design pattern be also possible
reference knowledge+management system issue challenge benefit
ais grammar%23NPR
be psychological explanation grammar%23NPR companion philosophy psychology page 113+130
grammar%23NPH sociology computational approach sociological explanation
computer review 223+235
causation psychiatry philosophical issue psychiatry page 196+215
description be not explanation methodology comparison method theory study religion 133+148
explanation from physics philosophy religion international journal philosophy religion 89+108
simple syntax
university+press oxford+university
grammar%23NPH how work versus be law two conception psychological explanation
cognition page 117+144
grammar%23NPR theory
page
structure content truth journal page 279+328
grammar%23NPH rule ofthe sociological method
grammar%23NPR theory abduction springer science business media
explanation scientific understanding page
understand semantic+web through description situation on+the+move grammar%23NPR doa grammar%23NPR page 689+706 springer
ontology design pattern handbook on ontology page 221+243

mind new science history cognitive revolution basic book
poverty deductivism constructive realist model sociological explanation
methodology
grammar%23NPH h. j grammar%23NPR world atlas language structure volume oxford+university university+press oxford+university
explain language universal

study logic explanation philosophy+of+science page 135+175
grammar%23NPH on explanation psychology physics philosophy+of+science
grammar%23NPH on explanation linguistics article forum discussion this issue www
de energeia grammar%23NPR
explanatory unification philosophy+of+science page 507+531
grammar%23NPH mode explanation from rule emergence grammar%23NPR grammar%23NPR grammar%23NPR
computational investigation into human representation processing visual information wh san+francisco freeman company
function level mechanism explanation cognitive+science problem theory psychology page
artificial explanation epistemological interpretation explanation ai grammar%23NPR 131+146
past contemporary perspective on explanation general philosophy+of+science focal issue page 97+173
induction inference process
d. h grammar%23NPH explain explanation
grammar%23NPH grammar%23NPR modern approach artificial+intelligence
grammar%23NPR cliff
scientific explanation causation unification grammar%23NPR page
grammar%23NPH pattern abduction grammar%23NPR 201+234
on proper treatment connectionism grammar%23NPR brain+science
explain reasoning overview explanation knowledge based system knowledge engineering review
scientific explanation encyclopedia philosophy second edition dm grammar%23NPR ed.
grammar%23NPR reference united+states+of+america
grammar%23NPR looking cluster explanation labyrinth linked data
semantic+web trend challenge page 333+348 springer
sociological writing
make things happen theory causal explanation
cause explanation psychiatry
issue psychiatry explanation phenomenology nosology page 132+184
explanation component software+system crossroads grammar%23ENT automatic extraction data from park pa alzaidy%40psu.edu park pa giles%40ist.psu.edu abstract scientific chart be effective tool visualize numerical data trend appear wide range context from experimental result scientific paper statistical+analysis business report abundance scientific chart make inevitable search+engine include as index content however query base on only textual data use tag image limit query result many study exist address extraction data from scientific diagram in+order+to improve search result our approach achieve this goal attempt enhance label chart by use original data+value that chart design represent this paper describe method extract data+value from specific class chart bar+chart extraction process be fully automated using image+processing text recognition technique combine various heuristics derive from graphical property bar+chart information use enrich indexing content bar+chart improve search result evaluate effectiveness our method on bar+chart draw from as+well+as chart embed digital document
information extraction scientific chart understanding search introduction scientific chart have wide presence not only as image but also as embed figure pdf document
search+engine nowadays include figure search result however index content chart document contain rely mainly on metadata textual tag
not+including actual information chart represent query process search+engine overlook many valuable query result thus enrich indexing content both document image base on chart content provide additional dimension search permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT
dx.doi.org%2F10.1145%2F2815833.2816956
many studies aim extract various type information from scientific chart including bar+chart automatically extract data+value from bar+chart effectively enhance their semantic label this additional information chart allow various analysis search tool additionally further assist application information+retrieval knowledge discovery technique
this paper describe system system target bar+chart specifically method automatically extract text graphical component from chart combine result infer original data+value chart
related work problem understanding scientific chart address various study describe method automatically annotate each text chart semantic role e.g. axis label caption etc. chart along their semantics use build index their web-based diagram search+engine describe although annotator extract textual graphical component role label method not provide further processing include original data+value chart as+well other approach data+value extract recover but only black+and+white grayscale charts as study relevant our work be where propose system analyze redesign chart their approach use method infer original chart data+value upon extract graphical textual component graphical component extract automatically however user require specify region chart where textual component locate another similar study study automate extraction data+recovery process be their approach extract data+value chart base on mapping between text graphic component each chart then represent as xml file contain data+value text information chart
method apply image find apply model based approach detect chart type extract component however bar+chart data extraction method describe further detail handle only single series bar+chart where all bar assume be same color
method our method follow pipeline show figure system comprise three main module graphical component extraction module text extraction module data inference module system take 2-dimensional grammar%23NPR data extraction system overview chart image format as input regenerate chart as output following assumption make charts chart be 2-dimensional chart
fill color bar be solid color rather than pattern
y-axis follow linear scale rather than logarithmic
y-axis alignment be left x-axis alignment be bottom chart
assumption system module extract bar data automatically system initially apply some pre processing step chart image such as color normalization noise filtering algorithm text recognition graphics typically apply noise removal filter image first scientific chart however be uncommon have highly noisy image as compare chart image additionally noise removal filter e.g. bilateral filter while computationally costly provide no significant improvement when apply chart extract from pdf document thus not apply any noise filtering step only pre processing step our method change color space prior pass chart extraction module text extraction module require grayscale version chart graphical component extraction image convert lab color space
be due fact that our method expect apply colored chart just+as effectively as black+and+white image pdf document recover raster format where each pixel specify by grammar%23NPR value process chart rgb+color scale propose following challenge single bar have very light color color be close grammar%23NPR value background make less distinguishable as distinct graphical component chart
multiple series bar+chart where bar share adjacent edge adjacent bar mistake be same graphical component if their rgb+color not highly contrast
challenge be far less present when recognition component apply image lab color space
lab color space be more perceptually uniform than grammar%23NPR space imply that amount change color value correspond similar amount change human perception make effective extract information information be distinguishable based on visual+effects such as chart component in+order+to extract meaningful information from chart two basic task perform
extraction graphical component i.e. bar ax other task be retrieval text label graphical component additional processing information require obtain data+value chart method propose achieve task describe remainder this section
graphical component extraction once bar convert lab space graphical component extraction module be responsible identify main chart component bar accurate data extraction bar extraction method must first correctly identify each bar distinctly secondly have high recall i.e. identify as+much bar as possible preferably
recover bar our algorithm follow similar approach present perform connected component label image however our method use lab color space distinguish component connected component method neighboring pixel differ color label as different component compare color lab space use delta e distance equation
experimentation threshold find suitable
connected component recover identify bar component by use heuristics derive from graphical property bar following properties use distinguish bar component bar fill bounding+box ratio greater than 90%25
color any pixel inside bar be different than color all pixel within 2+pixels 3+pixels distance outside bar edge
bar define by location height pixel in+order+to identify x-axis use conjecture that xaxis be horizontal+line that all bar have edge on use histogram location base bar recover common horizontal+coordinate among horizontal edge all bar define as x-axis
text component extraction text associate graphical component chart be key element recover original value represent by chart component label x yaxes bar in+addition+to numerical+value mark scale y-axis this section describe our proposed method automatically extract piece valuable information method involve two basic step as follow method first step be automatically locate text region image next step be apply ocr recognition text component
identification text region chart identify text region chart follow similar step method propose image be alreadybinarized step above remove component area size be great than typical character size
in+order+to identify letter represent single word letter subject isotropic dilation small window size close small gap between pixel be close another just enough be adjacent then apply connected component label dilated image label label entire word as one component maintain image quality ocr once location text region specify text region block pass ocr step be from image before dilation rather than dilated if text region width be smaller than height be vertical most likely be name y-axis
text grammar%23NPR value tesseract ocr use recognize nominal numerical+value each text region result filter+out any text region produce empty space only punctuation+mark for+instance some stray pixel be identify as text return character finally recovered text represent as their nominal numerical+value along their location
data inference algorithm extraction chart numerical+value ax name
x-axis y-axis b t output data+value data.name y name x name if grammar%23NPR below x-axis then x grammar%23NPR end assign element x x name x label base on horizontal alignment if grammar%23NPR left y-axis then y grammar%23NPR end assign element y y name y label base on vertical alignment grammar%23NPR y label do grammar%23NPR grammar%23NPR y grammar%23NPR grammar%23NPR value grammar%23NPR end median scale set grammar%23NPR set datai.name x labelsi where x be closest grammar%23NPR return set data pair data.name chart data inference this step extract chart data by apply inference process name bar be text identify bar bar typically locate bar value bar be y-axis value correspond high point bar extract this data pair follow step similar three main value recover accurately obtain bar value be y-axis scale+value ratio bar name i.e. x value algorithm show step inference method as input require location x-axis y-axis previously locate set bar b text string t. y-axis percentage bar extract pdf embed bar+chart 87%25 atlas charts 95%25 image 77%25 table extraction accuracy determine be area leave leftmost bar x-axis location extract previous graphical component extraction step each graphical component grammar%23NPR contain value x y coordinate height bar denote grammar%23NPR grammar%23NPR grammar%23NPR respectively text string t also contain value x y coordinate text region value string noted grammar%23NPR grammar%23NPR grammar%23NPR respectively first value infer y scale rely on both correct location text describe scale result ocr specify text text correspond value apply two heuristics y-axis label be left leftmost bar are vertically align text text satisfy assumption sort use next step next step identify ratio
calculate difference between y label pixel divide by difference numerical+value y label text bar value be thus their heights pixel multiply by ratio nominal+value each bar be value text locate below bar identify value extract text region region locate below x-axis horizontally align
experiment in+order+to evaluate effectiveness our method test accuracy on bar+chart found chart embed pdf document examine result system on total bar+chart are extract from pdf document are from data+set image provide by their revision tool also experiment bar+chart bar+chart generate from scientific chart generation tool such as grammar%23NPR by
show percentage correctly recovered bar bar+chart method recover on+average 87%25 bar first+class image pdf embed chart however due inaccuracy ocr tool y scaling almost 20%25 charts not calculate correctly even+though bar identify locate high accuracy be bar+chart obtain from atlas database be due image quality relative simplicity bar mostly bar be very small size similar previous class y scale evaluate correctly most chart except when ocr result be incorrect bar extraction obtain good result image however due quality image ocr not detect y-axis label value over 80%25 charts this type image more enhancement text region require obtain final value chart data figure show sample input chart compare resulting chart thats reconstruct base on value
limitation this approach note since not parse legend legend account+for miss information nominal+value bar however since color bar distinguish high accuracy parsing legend be feasible extension this work addition-of annotation see grammar%23NPH tweet annotate by at+least three worker confident score automatically compute by grammar%23NPR return aggregated result annotation base on response greatest generated dataset from our dataset 14%2C000 tweet 23%25 where about events while 38%25 be related given event theme give total tweet where found intersection between be about event their target theme grammar%23NPH show number tweet annotate as life+event their respective answer
event category have same amount tweet although falling love have far fewer cause by breadth our initial root concept as love cover wide variety different topic this dataset make publicly available http reellives.net%2Frl-data%2Fuploads%2F2015%2F06%2Fa692044.csv grammar%23NPH number tweet annotate as life+event grammar%23NPR be this grammar%23NPR theme event parent 116+509+625 falling in+love 64+114+178 get+married 22+709+731 child 43+489+532 51+420+471 total 296+2241+2537 feature engineering understanding how life+event describe on social require exploration various factor characterise event before identify factor be actually important inform our understanding how life+event generate explore affect various feature be variety work examine different feature social+media characterise social content dynamics however not feature help represent life+event this section list three different category feature individual feature each category that select including user feature content feature semantic+feature in+addition this work propose novel set feature interaction feature base on historical interaction user within platform
user feature user feature describe author post as+well+as standing participation on social+media platform for+instance by measure user social connectivity frequency participation hypothesis when use set feature be that certain type user e.g. be more popular follow by more user within network be more prone share life+event
content feature content feature define vocabulary post that being share i.e. word grammar%23NPR success.crowdflower.com%2Fhc%2Fen-us article grammar%23NPR compose as+well+as quality measure text such as metrics calculate readability post post sentiment hypothesis behind use feature be that post related life+event write different way than more general post e.g. express strong sentiment positive negative write more formal colloquial way etc. semantic+feature semantic+feature represent entity concepts person organisation location etc. appear within post hypothesis use feature be that prominent life+event semantically associate certain entity concept type
interaction feature interaction feature be novel set feature look+at network user user interact particular tweet rather than just consider number retweets favourite reply consider be user perform action their interaction pattern towards author post hypothesis use feature be that if user user not generally interact author post suddenly show interest post contain information special+interest
list individual feature consider as part every feature set specify table
life+event classification once set feature select our goal be assess how post about life+event differ terms selected factor do so utilise two stage approach that function as+follows identify life+event first detect post post talk about life+event vs. post post do not
identify particular type life+event second step detect post talk about particular event type
experimental setup explain following section
experimental setup uncover factor factor characterise life+event as+well+as particular type life+event first derive set post constitute instance train machine+learning classifier as describe section grammar%23NPR annotation result large class imbalance between post refer particular type event versus
post do not all occasion be more post post refer events specific type i.e. instance positive class than instance negative class in+order+to ensure that have balanced class distribution each dataset randomly sample tweet categorise as events even dominant class our case positive class from each respective dataset result split between post refer life+event type versus
post do not resultant number instance each dataset show table
balance dataset positive vs. negative class distribution then construct each post instance feature use feature describe section result vector representation each post more than 15%2C000 table example answer event category get+married tweet patriot obvi win because grammar%23NPR get+married tomorrow want be good mood yes yes not my idea marriage yes no this superstar fianc sing flood while be giving+birth no yes be sorry rant but feel strongly about wish celebrity like stand+up espouse this cause no no table user feature degree author each post this feature measure number incoming connection user very popular user be more keen on share their prominent life+event than user small network out degree this feature measure number outgoing connection from user user user follow big network tend listen speak i.e. post therefore be less keen on share their own life+event post count measure number post that user make during life social+networking platform
user post i.e. share more be more keen on share prominent life+event
age measure length time that user be member community longer user participate platform high probability user user experience important life+event during this period therefore user user be part community longer time share more life+event post rate measure number post make by user day hypothesis be that user user share more frequently also feel more comfortable share prominent life+event than post less frequency country this feature describe user country as state by user social+media profile note purpose this investigation seven english speaking country consider united+states+of+america united+kingdom hypothesis behind consider this feature be that culture play role on how keen user be on share life+event table instance class instance count event event event death parent 509+509 child 489+489 get+married 709+709 420+420 falling in+love 114+114 element content semantic each post also map created instance class label extract from grammar%23NPR annotation process section negative class event case event vs. event classifier event particular type case event type classifier positive class
dataset use as input three different classifier grammar%23NPH grammar%23NPH grammar%23NPH train each classifier use all permutation feature set e.g. only content feature content feature semantic+feature content feature semantic+feature user feature etc. use 10-fold cross validation evaluate created machine+learning classifier use standard classification performance measure precision recall f1 measure assess performance report result obtain each dataset grammar%23NPH classifier section leave+out result obtain clarity since be classifier obtain bad performance dataset result report each feature set individually as+well+as optimal combination feature last row each table
result begin by examine performance different feature set on identify post about life+event grammar%23NPH present performance grammar%23NPH classifier achieve when train on isolated feature set as+well+as good performance combination feature note that isolated feature set content feature be good performing feature obtain f1 measure among content feature most discriminative be n gram
top classifier be grammar%23NPH use semantic n- gram feature obtain f1 measure by perform attribute selection over content feature use information gain identify most discriminative n gram including knot tie refer metaphorical+expression tie+the+knot pass birth birth refer start end life terms such as love loving love indicate affection among most discriminative semantic+feature find en.wikipedia.org%2Fwiki%2Fknot en
en.wikipedia.org wiki pillow en.wikipedia.org%2Fwiki%2Fhessian_ cloth refer dressing code decoration usually use wedding en.wikipedia.org wiki funeral en.wikipedia.org%2Fwiki%2Fcancer associate death parent event
rest table show result identification particular event type as see table content feature be good performing individual feature all case sometimes use sometimes use grammar%23NPH classifier among content feature most discriminative be n gram follow by polarity interaction feature as oppose our initial hypothesis be not very useful discriminating life+event event type be due fact that not be grammar%23NPR content feature represent vocabulary use tweet tweet help discriminate different type prominent life+event case this work consider unigrams unique term total unigrams identify collected corpus reduce sparsity vocabulary follow approach grammar%23NPH remove infrequent terms appear only once remaining unigrams
length number words post although tweet allow maximum character long short message associate different type o life+event complexity measure cumulative entropy term within post gauge concentration language dispersion different terms let n be number unique term within post p fi be frequency term t within p therefore complexity give by n fi logn log fi this feature aim study whether post describe life+event contain many term term not repeat often rather repeat term from limited vocabulary fog index use i. e. more syllable this feature gauge how hard post be parse by human
count count number hyperlink within post i.e. link additional information user provide external+reference when come describe their life+event time day number minutes through day that post make this feature use identify possible key time within day associate life+event
novelty post term with+respect+to other post derive this measure use measure post about prominent life+event contain unique term with+respect+to platform vocabulary rather terms be familiar platform user p %C3%97 idft polarity assess average polarity post positive negative neutral use grammar%23NPR post about important life+event associate strong positive negative sentiment
count number mention other user within post when announce particular life+event author post mention relevant people involve
number time post share re- post by other user post about prominent life+event be less retweeted since other user friend feel cautious about sharing author personal information table binary all r f1 interaction user content semantic sem ng choice people post about personal life+event twitter+inc. allow user post character message tweet follow message other user on their fee mainly use communicate other individual similar interests regardless+of whether user know another follow update from celebrity company etc.
platform like mainly use by individual individual wish stay connect reconnect people know offline this sense be more likely that user share their life+event on platform like
while information be mostly public obtain information from require explicit consent every single user from information collect
constitute strong barrier collect data from this platform currently work+on develop application application allow explore this platform information as part our future line+of+work
death parent r f1 interaction user content semantic int ng grammar%23NPH show result obtain when try automatically identify post about death parent as see by result most powerful individual grammar%23NPR semantic+feature entity this feature set represent entity entity appear collected corpus total unique entity identify by use entity extractor example include en.wikipedia.org%2Fwiki%2Fmastercard en.wikipedia.org%2Fwiki%2Fbreastfeeding http wiki grammar%23NPR concepts this feature set represent concept entity type extract from collected corpus unique concept identify by look+up each entity rdf type grammar%23NPR ontology record concepts list each post example include dbpedia.org%2Fontology%2Fwork dbpedia.org%2Fontology%2Ffood dbpedia.org%2Fontology%2Fhospital table interaction feature naive interaction total number user u interact retweeted reply tweet
ratio let be u be set user user interact retweeted contribute conversation tweet author tweet interaction ratio consider number time that user ui u mention author post previous conversation m ui with+respect+to mention other user m ui
1%2C000%2C000 ui m ui length conversation this feature count number day conversation last conversation conversation last several day be indicator event for+example a+number+of people say congratulations post about getting married be content feature within n gram obtain up f1 measure new+brunswick classifier by do attribute selection use information gain top most discriminative terms about this event be pass pass expire refer act death father mother dad dad identify person loving sad refer feeling involve this type life+event negative sentiment associate post be also discriminative feature this type event this particular event type combination n gram interaction feature obtain good result f1 measure provide slightly good performance that content feature alone our hypothesis be that context death user user not tend interact be more sympathetic explicitly express their condolence
child r f1 interaction user content semantic co sem grammar%23NPH show result obtain recognise life+event have child again this case content feature be good performing individual set feature obtain up f1 measure new+brunswick classifier
most discriminative feature be n gram among birth give born deliver refer act have child baby child school school refer child growing up love love love express feeling experience this type life+event other discriminative feature include positive polarity high number referral count reference other user user user generate post tend mention partner post top combination feature be content semantic+feature but improvement be only marginal among most discriminative semantic+feature highlight en.wikipedia.org%2Fwiki%2Finfant en.wikipedia.org%2Fwiki%2Fchildbirth http wiki wife grammar%23NPH get+married r f1 interaction user content semantic ng grammar%23NPH show result identification event get+married this event content feature be again top individual set feature top identified n gram gram characterise this life+event be knot tie tie tie tie refer expression tie+the+knot wedding ceremony wed marriage married refer act get+married n gram alone obtain f1 measure add other subset feature not boost this performance
show result identification starting school event content feature be top individual feature obtain f1 score among most discriminative n gram this life+event be school stare start start tomorrow beginning begin year year refer event start school love grammar%23NPR r f1 interaction user content semantic ng refer love first day at school informativeness complexity be also relevant content feature indicate that post about starting school tend always use same vocabulary vocabulary be well+known terms platform user
falling love r f1 interaction user content semantic sem ng grammar%23NPH show result identification starting school event content feature be top individual performing feature set obtain f1 measure this result improve up f1 measure by including semantic user feature key n gram identify this event be love happy loved amazing friend start girlfriend other relevant content feature include polarity polarity tend be positive top semantic+feature include en.wikipedia.org%2Fwiki%2Fgirlfriend en.wikipedia.org%2Fwiki%2Fmusic_download
discussion most fascinating dimensions social+media be way allow create our digital lifelogs help maintain autobiographical memory
make possible identify retrieve relive part important life+event many social+media platform develop tool this effect this paper focus on automatic identification prominent life+event from data consider event from psychology research identify as most important event that people experience be their lifetime
study focus on how user share life+event on however as+one expect obtained result vary different social platform indeed observe from our result that interaction feature be not particularly discriminative within platform be most likely due fact that ego network on tend expand well beyond close friend family thus social interaction pattern not appear change much around event specific post on+the+other+hand recent study suggest that generally use disclose important life+event therefore aim explore as part our feature work where expect see great impact interaction feature on life+event detection
selection content semantic user feature inspire by literature consist feature found relevant one aspect another identify life+event however acknowledge fact that be other feature feature better characterise our selected five event individual platform platform
our proposed set interaction feature note note at+the+moment not allow collection historical chain favourite reply retweets towards user therefore generate our interaction feature base on mention however be desirable test proposed interaction feature above interaction type
generated dataset also argue that despite use wordnet expand list life+event root concept tweet select base on keyword rather than randomly therefore n gram have advantage as individual feature identify event random collection tweet however not ensure gathering tweet about life+event aim study require far more data annotation in+order+to obtain similar sample thus significantly increase annotation cost
mention paper generate gold+standard dataset life+event annotation use grammar%23NPR now believe that annotation quality improve by provide additional context annotator instead only show individual post be fairly difficult task even human annotator decide whether tweet be about event not without any context for+example tweet grammar%23NPR forever memory convey little information deduce that talk about get+married at casino call grammar%23NPR las+vegas in+addition+to our selection method extract any tweet tweet find include our original query word possibly cause problem where tweet take+out context from original
note paper experiment various question provide grammar%23NPR annotator next plan manually evaluate grammar%23NPR annotation reassess their quality identify where difficulties confusion emerge enable further improve manual annotation ensure high quality gold+standard result
regard life+event study this paper although base our selection event on psychology research argue that event be difficult discover social+media for+example event falling in+love find many tweet our seed dataset however when annotate very be actually about falling in+love be because our initial query word be inadequate but also be due lack sufficient information single tweet determine whether fall+in+love another person
paper only study top five event event deem be most popular by grammar%23NPH grammar%23NPR interest expand our study more all specific events list that research however event be difficult find on social+media such as whereas only be grammar%23NPR branch abstract by relation be too general so that sub- relations actually propagate policy c but top relation can not figure illustrate operation as result new relation wedge between top relation direct subproperties new branch allow perform fill operation next iteration
wedge operation
merge observe concept match by two branch max precision be possible that two top relation abstract by new common relation if be case perform this operation create new relation as show figure new branch allow perform fill operation next iteration
figure merge operation
group figure illustrate group operation set relation be all+together extent concept but belong different branch want create common parent relation again new branch enable fill operation execute next iteration
remove grammar%23NPR remove relation as subproperty another possibly cut sub- branch this operation remove single subsumption relation ontology relation be not cluster concept branch be almost realize that relation not really subsume by basically detail semantic relation remove from branch relocate elsewhere grammar%23NPR operation
group operation
from fill operation perform on ontology change ontology normally fill operation perform on newly created branch in+order+to populate rule base new rule as show figure adjustment phase restart new iteration process repeat until reasonably good compression factor reach no more meaningful change be possible
application complex environment such as city data hub data have very diverse life+cycle understanding how policy propagate between data object be crucial aspect data governance this section describe how apply methodology compress knowledge+base policy propagation rule use this task as describe previous section knowledge+base consider as binary matrix associate set policy set relation between data object result from workflow execution de-facto situation datahub
acquisition in+order+to setup knowledge+base policy propagation rule rely on rwanda+defense+force license database extract possible policy each policy be association one deontic element permission prohibition duty one action see listing example
example policy
ion odr l d e r v e p r oh b t o n l d r e x t r c t on+duty odr l t grammar%23NPR l cy generate policy by combine any grammar%23NPR action any deontic component however lead large+number meaningless policy duty odrl use adoption rwanda+defense+force license database permit obtain list meaningful policy only use extract list possible relation between data object
phase require manual supervision all association between policy relations in+order+to enstablish initial set propagation rule two collection use together grammar%23NPR tool generate farm+credit+administration formal context i.e. binary matrix cell
this stage manual supervision require inspect all cell decide whether enstablish rule not grammar%23NPR tool allow incrementally inspect different portion context rely on filter capability bulk operation be able check un check collectionsof cell similar status keep+track on cell cell be still supervise thanks+to grammar%23NPR be possible edit manually formal context reasonable effort list display sample binary relations be true false formal context
example cell binary matrix associate policy rule farm+credit+administration formal context dn grammar%23NPR permiss ion odr l copy dn d e n t f e r so f p r oh ib t on cc grammar%23NPR dn grammar%23NPR permiss ion odr l dn grammar%23NPR permiss ion cc reproduction dn grammar%23NPR duty odr l grammar%23NPR ike dn grammar%23NPR p r oh ib t on odr l use dn metadata p r oh ib t on odr l transform at end this process matrix have cell mark as true initial knowledge+base then compose policy propagation rule listing reader deduce that large part include relation relation not propagate any policy for+example top relation dn grammar%23NPR but also dn grammar%23NPR dn about among
example policy propagation rule
dn grammar%23NPR permiss ion odr l copy propagate dn d e n t f e r so f p r oh ib t on cc grammar%23NPR propagate dn grammar%23NPR permiss ion cc reproduction propagate dn grammar%23NPR duty odr l grammar%23NPR ike propagate dn grammar%23NPR p r oh ib t on odr l use propagate dn metadata p r oh ib t on odr l transform analysis follow method describe section apply farm+credit+administration algorithm obtain concepts represent cluster relation propagate same set policy list show one exemple all relation extent this concept propagate policy intent
example concept
extent intent dn t t r bu t e grammar%23NPR duty cc grammar%23NPR r ibut ion dn c l grammar%23NPR duty cc grammar%23NPR f t dn grammar%23NPR duty cc notice dn grammar%23NPR duty cc grammar%23NPR dn s c r p r so f duty odr l t grammar%23NPR l dup l c t e duty odr l grammar%23NPR dn grammar%23NPR duty odr l t t r bu t e dn grammar%23NPR r ibutes dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR ion dn h s d e n t f e r s dn ha s t e rp r e t t on dn grammar%23NPR dn grammar%23NPR ion dn ha grammar%23NPR f c t on dn grammar%23NPR dn grammar%23NPR ion dn ha grammar%23NPR l e c t on dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn d e n t f e r so f dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn sport grammar%23NPR dn grammar%23NPR dn grammar%23NPR c t grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn opt grammar%23NPR dn proce s s ed into dn r e f c t o r ed t o dn r e l t on so f dn grammar%23NPR dn grammar%23NPR abstraction abstraction phase search match between cluster relation obtain by farm+credit+administration concept extent branch grammar%23NPR hierarchy list show output abstraction algorithm return all branch branch be partially match by given concept
extent be cluster relation match several branch grammar%23NPR different way at begninning be top property grammar%23NPR dn grammar%23NPR branch size b be relation constitute whole hierarchy ontology clearly size intersection be same+as size extent as all relations cluster be branch
precision pre matching be pretty low this branch obviously match whole extent full recall rec like any other concept
more interesting case be branch associate dn grammar%23NPR intersection concept make all sub- relation in+fact look+at intent list sound reasonable that all part given data object inherit all grammar%23NPR full precision enable rule reduction process similar case be relation dn grammar%23NPR
example match between concept branch grammar%23NPR hierarchy
e s b pre rec grammar%23NPR branch 43+115 dn re grammar%23NPR dn dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR assessment by only consider branch match full precision by each concept rule compression factor at this stage make following consideration size matrix manually supervise be large be possible that error make at that stage process
not design purpose represent common behavior relation terms propagation policy be possible refine ontology in+order+to make cover this use case good reduce number rule even more
adjustment section describe method catch possible error matrix base on identification partial match high precision low recall such case highlight branch branch be close fully include extent concept as example pick branch dn grammar%23NPR from list list show detail about how concept match this branch happen that all relation except dn grammar%23NPR be part this concept other 4they inherit many other policy as+well consider by other concept low layer farm+credit+administration propagate policy list intent listing however be mistake happen acqusition phase as dn grammar%23NPR behave similar way dn grammar%23NPR
example match between concept branch grammar%23NPR hierarchy
e s b pre rec grammar%23NPR branch dn grammar%23NPR dn grammar%23NPR dn s s e l e c t o no f dn grammar%23NPR dn grammar%23NPR c t grammar%23NPR dn d e n t f e r so f dn sport grammar%23NPR dn grammar%23NPR decide then perform fill operation add all necessary rule make branch dn grammar%23NPR fully cover intent
branch similar score be dn
show that only missing relation be top one
grammar%23NPR dn define as relation between two object have same vocabulary same population contain actually same data
be possible that two object have same data without have same policy for+example dataset like lists city postcode import from different source have different policy while contain same data this case opt add new relation grammar%23NPR abstract all branch more focused semantic dn dn grammar%23NPR try capture exactly fact that two data object share same origin same population same vocabulary operation perform add this relation be wedge as new property inject between dn direct sub- relation
example match between concept branch grammar%23NPR hierarchy
e s b pre rec grammar%23NPR branch dn dn dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn dup l c t e each operation run our process again from analysis phase assessment in+order+to evaluate how+much change affect compression factor
list change perform statistics about impact on number rule number concept generate by farm+credit+administration number rule abstracted rule compute compression factor
show how compression rate increase number adjustment operation do
result obtain rule total concepts rule abstracted rule remaining compression factor
this methodology be able fix many error initial data refine by clarify semantics many properties add new version ontology at beginning this work find at purl.org%2Fdatanode%2F0.3%2Fns current version ontology be at purl.org%2Fdatanode%2F0.4 ns
list change perform
c r r first column identify change perform start from initial state
number concept farm+credit+administration lattice r number grammar%23NPR before process number rule abstracted subtract r size rule set without abstracted rule compact+flash compression factor adjustment c o m p re s io n f ct o r figure progress compact+flash
conclusion this paper present method abstract policy propagation rule approach rely on hierarchical organization possible relation between data object demonstrate that apply grammar%23NPR this task allow reduce number rule factor moreover be able analyse ontology in+relation+to this task enhance have as result not only further reduction rule but also good ontology in+addition apply ontology method be able find correct error rule
similar approach apply set policy by rely on grammar%23NPR ontology contain subsumption relation between action however hierarchy action grammar%23NPR be quite small grammar%23NPR
interesting problem be related change occur initial knowledge+base representation licence rwanda+defense+force grammar%23NPR be ongoing effort evolution initial knowledge+base contribute further improve expect that new policy contribute expand improve even more
future work extend assessment phase methodology also include coherency check between hierarchy farm+credit+administration lattice ontology build additional measure support adjustment phase currently while grammar%23NPR analysis phase perform use grammar%23NPR assessment adjustment phase implement dedicated script investigate if make+sense integrate measure operation methodology grammar%23NPR tool apply approach other use case
reference rule defeasible reasoning on semantic+web grammar%23NPR grammar%23NPH grammar%23NPH editor rule rule markup+language semantic+web volume lecture note computer+science page 111+120
grammar%23NPH
grammar%23NPH grammar%23NPH grammar%23NPH j. j grammar%23NPH norm organization semantics knowledge engineering review 107+116
rule based policy representation reasoning semantic+web proceedings conference on reasoning grammar%23NPR page 240+268 grammar%23NPH grammar%23NPH springer grammar%23NPR
bottom-up approach licence classification selection editor grammar%23NPR international workshop on legal domain semantic+web application grammar%23NPR hold during 12th grammar%23ENT page association+for+computing+machinery

semantic web+application through relation between data node grammar%23NPR open+university milton+keynes
deal diversity smart city datahub editor proceeding fifth workshop on semantics smart city workshop at 13th grammar%23ENT grammar%23ENT
ceur-ws.org
grammar%23NPH grammar%23NPH no registration need how use declarative policy negotiation access sensitive resource on semantic+web semantic+web research application page 342+356 springer
rez problem+solving method understanding process execution
science engineering
grammar%23NPH grammar%23NPH design application rule based access+control policy grammar%23NPR semantic+web page
grammar%23NPR grammar%23NPR ontology world+wide+web+consortium recommendation world+wide+web+consortium apr.


knowledge+acquisition as process model refinement

v. rodr_iguez-doncel rez
dataset rwanda+defense+force license editor legal knowledge information+systems grammar%23NPR grammar%23ENT twenty-seventh annual conference
v. grammar%23NPH
engineering principle method data knowledge engineering 161+197
grammar%23NPR merge bottom-up merging ontology grammar%23NPR volume page 225+230 grammar%23ENT through mesh classification information+technology monash.edu information+technology monash.edu monash.edu abstract large research institution broad research discipline such as life+science be highly important very challenging task capture each researcher expertise match researcher by expertise assist identify inter disciplinary collaboration opportunity make informed policy decision challenge be multi dimensional stem from needs provide thorough coverage breadth depth disciplinary area b develop accurate representation researcher expertise c process large volume data efficiently medical subject+heading mesh comprehensive taxonomy life+science widely use indexing publication this paper present novel framework matching research expertise base on knowledge encode mesh specifically design novel effective hybrid mesh classification algorithm by combine state-of-the-art method use mesh term aggregate from researcher publication design researcher match algorithm base on semantic similarity similarity take into consideration structure mesh taxonomy
subject descriptor grammar%23NPR information+systems application content analysis indexing grammar%23NPR artificial+intelligence learning knowledge+acquisition general+term algorithm design keyword mesh researcher profile matching introduction large research institution there be thousand researcher work diverse array discipline work permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor
credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america
association+for+computing+machinery isbn grammar%23ENT grammar%23ENT
dx.doi.org%2F10.1145%2F2815833.2815837
different but relevant sub- disciplinary area sound understanding researcher expertise mechanism systematically represent such expertise have significant impact on a+number+of issue such as identify individual institutional collaboration partner decision+making
disciplinary collaboration can arise organically
able identify research expertise identify complementary research expertise further facilitate collaboration within disciplinary institutional boundary
identification individual researcher expertise be foundation understanding evaluate research strength organisational unit such as lab department faculty institution this knowledge in+turn better inform research planning strategic decision+making
challenge need tackle achieve above task firstly research topic subject need describe comprehensively interested branch study
researcher expertise need represent accurately thirdly large volume data primarily form publication need process efficiently
challenge manifest life+science encompass study very large+number branch science including biology medicine as+well+as inter disciplinary area such as bioinformatics largest database life+science now provide reference more than 21%2C000%2C000 biomedical article number article continue rapidly increase at about 700%2C000 article per+year
breadth depth field as+well+as large volume publication data make life+science especially challenging domain represent researcher expertise
taxonomy provide comprehensive common vocabulary describe research topic relevant life+science contain 26k term main heading current iteration mesh vocabulary extensively use classify index annotate biomedical article believe broad coverage wide adoption make mesh ideal candidate as basis representation researcher expertise
task classifying article use mesh term often refer as mesh classification be very challenging problem give large size mesh complex hierarchical+structure mesh as result despite recent development automatic method mesh classification still perform this process require extensive domain knowledge be expensive timeconsuming error further text enhancement technique apply increase accuracy result ocr tool improve accuracy y scale extraction ax name extraction
node o p grammar%23NPR figure sample bar+chart extract from random pdf file same chart reconstruct use data+value
conclusion although many studies address problem understanding scientific chart very few specifically address bar+chart class this paper apply image+processing technique extract data from bar+chart embed digital document enhance that be useful semantic label document image describe system extract graphical text component from bar+chart reproduce original data+value chart experimental evaluation demonstrate effectiveness our method bar recovery accuracy up 87%25 pdf embed bar+chart value further use future studies development domain specific knowledge discovery application query snippet generation
acknowledgement author like thank many insightful comment
reference search statistical diagram frontier engineering national+academy engineering page
grammar%23NPR search+engine data driven diagram proceedings 24th on companion page 183+186 international conference steering+committee
get computer see information graphics so user not have foundation intelligent+systems page 660+668

robust algorithm text string separation from mixed text graphics image pattern analysis machine+intelligence institute+of+electrical+and+electronics+engineers transactions on 910+918
model based chart image recognition graphics recognition
advance perspective page

associate text graphics scientific chart understanding document analysis recognition
on page 580+584 institute+of+electrical+and+electronics+engineers
grammar%23NPH automatic extraction table metadata from digital document
proceedings 6th association+for+computing+machinery institute+of+electrical+and+electronics+engineers cs joint conference on digital+library page 339+340 association+for+computing+machinery
quartz atlas by quartz atlas.qz.com
grammar%23NPH grammar%23NPH revision automated classification analysis redesign chart image proceedings 24th annual association+for+computing+machinery symposium on user+interface software technology page 393+402

text detection chart image pattern+recognition image analysis 139+144 grammar%23ENT extraction classification chemical+compound from vnj2101%40columbia.edu as4669%40columbia.edu kk3004%40columbia.edu am4155%40columbia.edu pg2450%40columbia.edu up2127%40columbia.edu compound provide key information text document material science domain extraction classification chemical+compound text present novel kernel incorporate periodic+table property chemical+element evaluate performance perform experiment on corpus 14%2C656 journal paper abstract field publish during grammar%23ENT grammar%23ENT grammar%23NPR be device typically contain four layer compose different material
abstract treat as bag words word classify into five class four class correspond four layer grammar%23NPR one class categorize unrelated word not belong layer cask find outperform string subsequence kernel other kernel such as radial basis function polynomial linear
keyword information extraction chemical+compound string kernel unstructured data introduction amount digital information various domain grow exponentially increase research thus be need efficiently index past research work key information quick retrieval term recognition identification relevant word text term classification base on ontology be important subtasks information extraction information extraction technique apply finance computational biology biochemistry etc.
extract domain relevant information from unstructured text document document domain such as materials+science chemistry chemical+compound capture key information
make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816954 due enormous variation representation equivalent material acronym word phrase chemical+formula be impractical develop comprehensive database all kind representation material present corpus necessitate use machine+learning algorithm automate extraction classification chemical+compound text document input+data not readily describe by explicit numerical feature vector string kernel method provide effective alternative explicit feature+extraction kernel method when combine classification technique help grouping related word by classify into same class
this study present novel kernel use extract classify chemical+compound represent as chemical+formula from text document evaluate performance utility with+respect+to other kernel perform experiment corpus journal paper abstract domain grammar%23NPR be device typically comprise four layer substrate window absorber compose different material metal contact metal contact be mostly limited material photovoltaics field ag al sometimes mo hence be not part our study this study use multi class grammar%23NPR extract classify chemical+compound into four layer each abstract
background learn separating hyperplane between two set point so as maximize margin separation between different class key statistical property maximal margin solution be that performance not depend on dimensionality space separation take+place make possible work+in very high dimensional space such as induce by kernel kernel hyperplane feature space correspond nonlinear decision boundary input space
x sign x b as show grammar%23NPR data+point only appear inside dot+product other data+point thus kernel thelearning take+place high dimensional feature space
kernel function compute this inner+product high dimensional space by implicitly map data+point feature space grammar%23NPR formulation constrained optimization problem grammar%23NPR decision function f x give by grammar%23NPR where xi li represent set training data x grammar%23NPR x xi be nonlinear function map input space into high dimensional space k x xi x grammar%23NPR xi be multiplier multi class classification this study one-againstone method use grammar%23NPR class this method construct grammar%23NPR grammar%23NPR binary grammar%23NPR classifier class decision base on majority voting scheme ijth classifier optimization formulation be as+follows min grammar%23NPR grammar%23NPR t ij t yt w t ij xt bij ijt if xt grammar%23NPR yt w t ij xt bij ijt if xt grammar%23NPR ijt t l where be measure violation constraint c be weighting factor use penalize error w be normal hyperplane
introduce by grammar%23NPH et+al. computing similarity between string report perform better than standard word feature space kernel classification text document this study each word corpus consider as sequence character be kernel feature space generate by subsequence length k subsequence be any ordered sequence not necessarily contiguous k character present word each subsequence correspond one dimension feature space value that dimension depend on how frequently how contiguously such substring be present word if subsequence be not present in+a+word value that particular dimension be if subsequence be present value that dimension be sum over occurrence subsequence give by decaying factor length occurrence thus high weight give subsequence be more contiguous compute pairwise similarity between word compute inner+product between feature vector more subsequence two word corpus have common high value their inner+product be parameter k select through cross validation grammar%23NPR present function computing similarity between string s t kn s t u n u s u t u n u s l j u t j l j u n u s j u t j l l j feature vector dimension ia alkali+metal grammar%23NPR alkaline+earth grammar%23NPR alkali+metal grammar%23NPR grammar%23NPR grammar%23NPR alkaline+earth+metal grammar%23NPR grammar%23NPR intelligence+bureau coinage+metal grammar%23NPR boron group poor metal iva carbon group transition+metal grammar%23NPR chalcogen noble+gas grammar%23NPR noble+gas metalloid table feature vector dimension where represent finite+set english alphabet n represent all finite string length n u be subsequence string s l represent length subsequence term position character word subsequence subsequence be more contiguous have small subsequence length thereby high weight as dimension feature space grow exponentially k grammar%23NPH et+al. present efficient dynamic programming technique recursive computation subsequence kernel
this study efficient dynamic programming formulation implement
improve performance corpus material science abstract augmentation domain specific knowledge on chemical+compound propose this study motivation this approach be that string represent chemical+compound even if differ by only few character have widely different property thus focus on subsequence similarity string result incorrect classification word this study propose chemically augmented string kernel kernel compute new gram matrix by add gram matrix positive weight word contain elemental symbol
key step this approach be devise scheme incorporate domain knowledge computing similarity ensure that modified kernel be valid grammar%23NPR kernel
approach use dimensional feature vector take+into+account group periodic+table additional class group as show table each group be column periodic+table series span over multiple column periodic+table task compute similarity matrix subdivide into four parts
each word possible chemical+element extract use regular expression match second every element dimensional feature vector compute indicate presence absence that element each category periodic+table third feature vector all element present word add fourth this feature vector normalize eliminate any bias words great number element
similarity between two word compute use theinner product between periodic+table based feature vector feature space this inner+product value augment similarity value compute by combine value be similarity value between two word with+respect+to since inner+product two vector feature space be valid kernel addition two valid kernel be valid kernel addition remain grammar%23NPR kernel performance relative other kernel utility corpus grammar%23NPR abstract present below
result implementation evaluate cask use corpus 14%2C656 journal paper abstract grammar%23NPR domain publish during preprocess rule apply ensure standardization word phrase abstract rule include converting name chemical+element compound their respective chemical+formula e.g.
germanium grammar%23NPR ii converting commonly used acronym their compound+name e.g. azo al zno
stopwords remove from corpus abstract treat as bag words each word classify into five class window layer tco layer absorber layer substrate layer grammar%23NPR layer training data consist labeled words result average over 4-fold stratified crossvalidation overall f1 score classifier report following section obtain by average formula+one score five class use as metric compare performance classifier
performance since corpus consist words explicit numerical feature vector not exist other kernel such as linear polynomial thus design appropriate feature vector be important step compare performance other kernel
four main domain component component include feature design be indicator function specific element like cu ga grammar%23NPR grammar%23NPR etc. ii lexical characteristic compound like presence integer bracket etc iii class specific characteristic tco layer presence oxygen symbol word iv presence element periodic+table category
feature vector consist dimension capture relevant domain knowledge pertain type element present word prevent inclusion irrelevant feature feature feature have variance below adjustable threshold eliminate result feature vector dimension
present average formula+one score one standard+deviation different kernel second degree polynomial kernel kernel variance parameter as grammar%23NPR kernel as with+respect+to various c value be evident that linear kernel c high formula+one score use as baseline performance evaluate performance improvement
experiment perform by vary hyperparameters k figure present top c value grammar%23ENT s c o r e s linear grammar%23NPR figure formula+one score versus c value grammar%23NPR other kernel c value grammar%23ENT s c o r e s lambda k lambda k lambda k lambda k figure formula+one score versus c value grammar%23NPR subsequence string kernel four combination k value as see figure be evident that performance classifier be low than baseline linear kernel
value grammar%23ENT s c o r e s lambda lambda lambda lambda figure formula+one score versus c value grammar%23NPR similar figure figure present result k as illustrate figure k provide good formula+one score significantly outperform baseline performance be also evident increase value k drastically improve performance given corpus where element mostly consist one two character cu s k not be appropriate estimate similarity by subsequence match as be possible that individual element length grammar%23ENT be present compound belong different layer thus k solve this problem by matching compound combination at+least element account material property thorough cross validation various c value grammar%23NPR abstract level classification result publication year grammar%23ENT o u n t 80+100+120 grammar%23NPR a- si frequency count versus publication year various absorber layer material k perform formula+one score found be primarily sensitive c value range
optimal hyperparameters then use generate result following section
utility present abstract level classification result few abstract as illustrate figure classification algorithm correctly identify material correspond layer additionally entire corpus abstract roughly 96%25 word classify as grammar%23NPR category also be in+agreement expectation expectation most word corpus not belong any layer grammar%23NPR
further explore collection abstract count abstract each unique material grammar%23NPR layer be recorded figure present count abstract with+respect+to year publication top three material absorber layer similar figure obtain material other layer additionally as classification result ordered collection material each abstract figure word correspond material concatenate form single+string single+string represent material content each abstract then also use determine pairwise similarity between abstract purpose cluster abstract on basis material+information demonstrate utility abstract level cluster abstract out pool abstract contain material four layer select word correspond material four layer each abstract concatenate then use compute pairwise similarity between abstract resultant gram matrix serve as weight matrix cluster abstract into four cluster by+means+of spectral clustering following be set four material vector obtain from one such cluster grammar%23NPR f grammar%23NPR cd glass grammar%23NPR grammar%23NPR cd glass grammar%23NPR grammar%23NPR cd glass grammar%23NPR a- si tco glass
set suggest that clustering gram matrix as weight matrix result reasonable grouping abstract present another means organize retrieve information contain corpus
discussion material science domain researcher primarily explore various combination element compound pursuit developing novel material consider as extension string kernel specific purpose extract classify chemical+compound from text document as show grammar%23NPR corpus such method use index document list compound be present aid efficient retrieval document be similar terms material+information additionally generation time plot be useful entrant field obtain sense evolution discussion key material
additional author grammar%23NPH columbia+university email de2300%40columbia.edu columbia+university email venkat%40columbia.edu
reference string kernel implementation code

grammar%23ENT
grammar%23NPH pattern+recognition machine+learning springer grammar%23NPH
tutorial on support vector machine pattern+recognition data+mining
thin-film solar+cell overview progress photovoltaics research application
genia project ninth conference on european chapter page 271+272
single layer learning grammar%23NPR north+atlantic+treaty+organization
term identification biomedical literature 512+526
grammar%23NPH text classification use string kernel
computational design hierarchically structured material science grammar%23ENT
tutorial on spectral clustering
computing grammar%23ENT figure system architecture syntactic rule by combine grammar%23NPR show example take from legal domain compound expression constituent expression expression comprise category generalize over several token thus form higher+order recurring pattern corpus extraction pattern use as unit translation enable better capture structure semantics domain
present approach system system build on idea extract meaningful pattern from domain corpus gather human feedback on their translation learn rulebased translation system use grammar%23NPR formalism system be human centric heavily rely on manually curated linguistic resource while machine continuously prompt human on translate this interactive human machine dialog produce translation system that aim achieve high precision in- domain translation find application several technical domain including medical education legal etc. system be available demo at mtdemo.hostzi.com
system architecture our system grammar%23NPR figure follow iterative pipeline+architecture where every component be modular system be interactive take human feedback on translation
feedback use build linguistic resource incorporate into underlying translation model translation model express use grammatical framework formalism formalism base on functional+programming type theory this expressiveness abstraction make model easily programmable by human
pattern extraction this module aim capture potential translation unit present corpus take as input in- domain source+language corpus monolingual typed dictionary produce frequently occur pattern as output lebanese+free+will+movement algorithm appendix explains our frequent pattern mining algorithm pattern comprise variable length gap represent by x grammar%23NPR act as placeholder generalized entity
pattern selection grammar%23NPR www.cse.iitb.ac.in vishwajeet pattern extraction section mine large+number redundant pattern as potential translation unit since get manual translation candidate translation unit be costly operation identify subset pattern be both diverse maximally cover in- domain source+language corpus pattern selection algorithm appendix provides detail on this selection subset good pattern where goodness subset measure terms corpus coverage figure provide example where first column contain sample text from corpus other column show pattern pattern bold selection step
sample pattern pattern selection pattern translator translator module involve human provide translation pattern select by pattern selection module
use build bilingual lexicon along monolingual dictionary use train rule based translation engine subsequent iteration trained translator provide multiple translation option that user choose from override section offer further detail on user+interface
generalization translation unit this module help generalize translation unit by cluster together further aid reduce number rule subsequent grammar generation where each cluster identifier act as terminal all production involve cluster member
see various sentence that phrase internal reordering be same also have same external reorder when translate other language so cluster translation unit have same internal reordering into one cluster use reorder distortion score between translation two translation unit as measure cluster translation unit since cluster represent all translation unit present that group also represent their translation behavior be their external internal reordering be same same external reordering help make possible write single translation rule all member translation unit cluster
rule grammar%23NPR learner once translation unit extract select translate store language resource database next job+in+hand annotate sentence translation unit in+other+words represent sentence form sequence translation unit if coverage sentence be 100%25 whole sentence represent only help translation unit once sentence represent canonical+form parse linearize use grammatical framework rule
be extension logical framework component call concrete syntax reorder rule rule handle gender number andperson information while do look-up write use formalism main purpose behind use grammatical framework be functional nature also grammatical framework have concept call abstract+syntax abstract+syntax provide interlingua representation interlingua representation help linearize different language very easily just by write concrete grammar that language
system user interface our system have interactive user+interface human translate pattern n gram also have provision expert user configure pattern length frequency threshold pattern extraction different user+interface feature show at http www.cse.iitb.ac.in vishwajeet figure on link depict feature provide expert user user upload new corpus use upload input+file option mark label figure upload dictionary option label enable user upload bilingual+dictionary system perform lookup provide translation suggestion user either choose run system extract pattern on default configuration label manually configure pattern length frequency label
pattern extract filter validate by system user use web-based system show figure provide translation feedback human translator show current sentence grammar%23NPR label figure along previous next sentence as context information pattern display below column label as fragment grammar%23NPR label figure on hover over pattern untranslated n gram span cover that pattern n- gram sentence get highlight refer figure pattern contain generalized terminal label translator view all instance label figure by hover over nt initially translation pattern untranslated n gram label figure suggest by system using pattern database glossary lookup smt translator even configure source get suggestion choose get translation suggestion from smt system by click on smt button label figure b choose get translation suggestion from database by click on glossary button label figure translator edit translation suggestion label figure give by system correct also reorder composed translation sentence by click on reorder button label figure present simple drag+and+drop interface user refer figure finally if user wish edit composed translation do that by click on final editing button depict by label figure final editing user save translation by click on save button label user also download translation by click on download button label in+order+to get translation suggestion particular word phrase user enter text suggestion panel on right get multiple translation suggestion particular word phrase
feature system once translator translate pattern pattern instance n- gram system auto translate next time appear sentence
if pattern pattern instance n- gram translate differently different sentence system list as choice user decide enter new translation
system also have integrated suggestion component component fetch translation suggestion from various source user use this get translation suggestion word phrase choose good translation from choice
evaluation evaluate system terms quality pattern grammar system efficiency evaluation do on five public dataset viz. constitution grammar%23NPR spoken grammar%23NPR income+tax grammar%23NPR dataset belong domain government document technical tutorial academic book where high+quality translation be imperative table show corpus statistic terms number sentence dataset
number frequent pattern corpus coverage number frequent pattern increase size corpus corpus coverage depend on number pattern extract from corpus adhere specified pattern length frequency table depict information about number pattern extract coverage on five different corpus
dataset corpus coverage by pattern domain sentence frequent pattern frequent instance coverage constitution spoken tutorial biology income+tax act physic effect varying pattern length frequency threshold pattern extraction criterion assess quality individual pattern be whether not appear unseen data thereby cover sentence that data set such pattern then consider be good if collectively offer high coverage on unseen data split dataset into test where mine split use extract pattern their coverage terms number word cover evaluate on test split perform three+fold cross validation vary both pattern length frequency threshold from report coverage on grammar%23NPR indiacode.nic.in%2Fcoiweb%2Fwelcome.html grammar%23NPR spoken-tutorial.org grammar%23NPR www.ncert.nic.in%2Fncerts%2Ftextbook%2Ftextbook
grammar%23NPR grammar%23NPR www.incometaxindia.gov.in%2Fpages%2Facts grammar%23NPR www.ncert.nic.in%2Fncerts%2Ftextbook%2Ftextbook
grammar%23NPR coverage vs. pattern length on mining data b coverage vs. pattern length on test+data c pattern length versus number pattern fixed threshold figure corpus coverage varying pattern length frequency threshold mine test set figure capture trade-off between pattern length frequency threshold coverage fixed threshold as pattern length increase coverage on both mine test set progressively decrease
observation apply when fix pattern length increase frequency threshold also observe that gap coverage be much smaller varying frequency threshold at small length this gap progressively widen as pattern length increase
effect varying dictionary size on corpus coverage our pattern selection algorithm constrain cardinality set pattern while maximize corpus coverage
correspond limit size bilingual+dictionary be desirable as size bilingual+dictionary be proportional human effort translation corpus coverage increase increase size dictionary however this increase be not linear but rather flatten increase size dictionary figure capture this relationship between coverage fraction pattern select sub setting different dataset
coverage versus number pattern select pattern selection induced grammar once user provide translation pattern their instance uncover n gram sentence reorder different chunk grammatical framework rule induce firstly abstract+syntax induce define meaning express grammar then concrete english concrete hindi syntax induce provide map from meaning string english hindi language figure on grammar%23NPR illustrate sample induced grammar new sentence extract translate pattern give as input grammar if match found then sentence reorder use mapping from concrete syntax
conclusion present interactive machine+translation approach high+quality translation technical domain corpus
in- domain source corpus our system mine minimal number frequent pattern maximally cover corpus leverage human their high+quality translation continuously rebuild rule based translation engine engine realize use formalism
reference v. grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al. grammar%23NPR computer-assisted translation workbench proceedings 14th conference european chapter page
grammar%23NPH s. grammar%23NPH r. l. m. m. g. grammar%23NPH grammar%23NPH patent translation within molto project grammar%23ENT
grammar%23NPR web-based translation research suite grammar%23NPR grammar%23ENT workshop on post- editing technology page
machine+translation view from lexicon massachusetts+institute+of+technology press
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al. matecat tool proceeding grammar%23NPR page 129+132
www.cse.iitb.ac.in vishwajeet grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
grammar%23NPR free open-source platform rule based machine+translation machine+translation 127+144
principle semantic compositionality grammar%23NPR
grammatical framework programming multilingual grammar grammar%23NPR publication center study language information
accept post editing environment flexible customisable online tool perform analyse machine+translation post editing proceeding xiv workshop on post editing technology practice page 119+128
toward memory based translation proceedings 13th conference on computational linguistics volume page 247+252

review+article example based machine+translation machine+translation 113+157
grammar%23NPH
machine+translation multimodal interactive pattern+recognition application page 135+152 springer
a. algorithm algorithm lebanese+free+will+movement algorithm data corpus c pattern length l frequency threshold t maximum consecutive gap token g result set f frequent pattern maintain dictionary structure grammar%23NPR where key be pattern value be list span each sentence s c maintain array list slist size s such slist store all+one length pattern along span sentence start from si use slist create 2d array list smatrix size s grammar%23NPR such smatrix j store all pattern along span s start from si pattern length j filter pattern from smatrix span be syntactically incomplete grammar%23NPR pattern grammar%23NPR end dictionary l do valid mask v length l do pattern p length grammar%23NPR apply v on pattern p create new pattern if be present grammar%23NPR then update spanlist by do union span list p else add grammar%23NPR spanlist as spanlist p end end remove pattern length gap position accord mask have span count less+than t end end remove pattern from grammar%23NPR number span be below t output grammar%23NPR grammar%23NPR algorithm pattern selection data dictionary pattern p spanslist number word corpus n max size selected set k result set f diverse high coverage terms word pattern f n do false end k do grammar%23NPR grammar%23NPR grammar%23NPR each pattern p p f do coverp count false bit be spanlist p if coverp grammar%23NPR then grammar%23NPR p grammar%23NPR coverp end end if grammar%23NPR then f true if be spanlist grammar%23NPR else break end end output retell thousand way grammar%23NPH redondo%40eurecom.fr grammar%23NPH giuseppe.rizzo%40eurecom.fr grammar%23NPH raphael.troncy%40eurecom.fr abstract live constantly evolving world where news+story relevant fact happen every moment story numerous news+article post social+media reaction create offer multitude viewpoint about happen around many application try deal this complexity from very different angle target particular needs reconstruct certain part story exploit certain visualization paradigm
this paper identify challenge study how adequate news+story representation effectively support different phase news consumption process propose innovative model call grammar%23NPH grammar%23NPH design capture entire context news+item this model feed very different application assist user before during news+story consumption formalize duality news annotation distinguish between representative entity relevant entity consider different relevancy dimensions that incorporate into model form concentric layer finally analyze impact this national+security+service on existing prototype how support future
subject descriptor grammar%23NPR information storage retrieval information+search retrieval retrieval model search process keyword news+story grammar%23NPH introduction even priori conventional story story daily consume have some underlying fact during certain situation some particular user become important need unveil fact describe very permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816951
way publisher aim emphasize certain aspect story target specific audience respond particular viewer needs role consumer also evolve over time from passive less engaging behavior mode require deep knowledge about fact report this highly challenging ecosystem where story spread all+over different piece information interpret by many different user present by various data+source existence model represent entire context news+item become highly relevant
construction such advanced story representation already address our previous work make hypothesis that single video news+item be often not enough capture complete story report be biased even partially wrong develop various information+retrieval technique combine original news content additional data collect from other external source this process called be able produce ranked list named entity complement initial set detected entity video subtitle other item related entity capture from document top n item this list build conceptual structure call news+story this national+security+service evolve from plain ordered list entity multi-layered concentric model more appropriate represent duality between most representative entity other be relevant context news+item due diverse reason such as interestingness informativeness popularity
this paper analyze how this national+security+service support different requirement derive from news consumption process this structure need be easy exploit flexible enough give answer different application deal duality present news annotation by differentiate between entity that better summarize story one one acquire relevancy as story further consume emphasize relationship establish between different entity inside context story focus more reason have such connection less their absolute importance inside story last part paper analyze some prototype prototype project rich spectrum relationship within entity into simple human easy way consume story in+order+to understand how benefit from such news+item representation grammar%23NPR capture scientific software metadata grammar%23NPH grammar%23NPH information+sciences+institute admiralty way grammar%23NPH gil%40isi.edu varunr%40isi.edu grammar%23NPR dgarijo%40fi.upm.es abstract this paper present grammar%23NPR ontology describe metadata scientific software ontology design consider how scientist approach reuse sharing software include support scientist identify software understand assess software execute software get support software research software update software ontology be available owl contain more+than fifty term use grammar%23NPR structure software registry geoscience develop user+interface capture metadata author keyword ontology software reuse knowledge capture association+for+computing+machinery classification keyword grammar%23NPR information interface presentation user+interface interaction style introduction scientific software capture important knowledge share reuse although be many popular code repository use by scientist be still significant amount software never share reason not share scientific software include desire not expose code lack incentive credit software interest software commercialization among grammar%23NPR grammar%23NPR grammar%23ENT while loss dark data science well recognize grammar%23ENT be analogous problem pervasive loss dark software our interest be supporting software share geoscience with+the+exception+of big model package typically share modeling framework geoscience software rarely share in+addition scatter different site not easy find reuse our goal be improve software share by develop software registry framework framework include metadata useful discovery reuse among scientist scientist be not software developer this paper introduce grammar%23NPR software registry ontology describe scientific software metadata ontology contain basic metadata properties describe how identify software understand do utility research execute get support if question arise research contribute development be all topic interest scientist ontology revolve around category as way frame request metadata practical light incentivize scientist provide related ontology cso grammar%23NPR grammar%23NPH grammar%23ENT extend dolce ontology et al grammar%23ENT describe software+component web+service ontology design describe large software+system so their requirement include accessibility software+component middleware service execution failure composition software cso formalize concepts relate software data include both software+component service grammar%23NPR extend cso define software+component further include notion such as interaction protocol taxonomy however focus on complex software+system rather than end+user end+user be scientist need define software terms reuse by other scientist software repository e.g. grammar%23NPR cran use widely by scientist although allow user describe their software not use ontology model model exploit support reuse requirements be few important requirements take+into+account design grammar%23NPR first design grammar%23NPR center on broad range user software registry science although many scientist sophisticate software development skill vast majority scientist be able share reuse software do not consider very important design grammar%23NPR be accessible permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2816955second grammar%23NPR intend describe software write by scientist although scientist use wide range software infrastructure do their work not intend grammar%23NPR use describe such infrastructure third metadata capture by grammar%23NPR focus on software sharing reuse rather than software installation execution requirement software be quite complex often provide detailed documentation grammar%23NPR allow specification rather simple runtime requirements that scientist limited software skill want specify about their software fourth scientist be never excited provide metadata not dataset not software create automatic extraction tool tool get as+much information as possible from existing software repository for+example if user provide pointer grammar%23NPR system extract license implementation language other information automatically in+addition ontology ontology require formal description every item not have much uptake end+user therefore grammar%23NPR provide high-level structure but allow user provide+for textual description most field plan develop automatic text extraction tool tool help user by structure further textual information provide envision more formal ontology develop in+the+future once community use describe software more poise adoption more formal ontology overview grammar%23NPR metadata capture by grammar%23NPR fall into six major category base on information that scientist seek about software identify software understand assess software execute software get support software research software update software rationale this organization be allow user user contribute software understand why metadata request allow user user look reuse software where find metadata metadata be relevant figure show snapshot user+interface develop earlier version grammar%23NPR contrast different metadata properties several hydrological model grammar%23NPR ontology documentation be available grammar%23NPR implement owl figure show overview major metadata properties describe software grammar%23NPR show bold major category subcategory property each metadata property show name www.ontosoft.org nto o figure screenshot grammar%23NPR software registry where user compare similar software use metadata properties grammar%23NPR ontology grammar%23NPR registry currently host more+than parenthesis by range i.e. value take indicate plus+sign when take more+than one value remainder this section give overview class properties grammar%23NPR ontology along six major category above note that many properties not have structured value instead their value be description text believe that significantly reduce burden place on user describe their software as mention above identify software this category capture metadata metadata allow user identify software entry be for+example through name software mention by colleague through identifier find paper metadata properties include name short description short description software provide important keyword support search another important property be unique identifier unique identifier be permanent url uri some software have unique identifier provide by software repository such as identifier provide by grammar%23NPR reason property take text value understanding metadata this category allow user understand software do assess utility research several metadata properties do relate software knowledge about domain one way find+out detail about software do context science be look+up web+site software related project often associate software+product other properties provide domain specific description software including use assumption use limitation similar software software widely know community another set metadata properties do trust software one aspect find+out creator contributor be as+well+as contributor publish software registry be person project institution so value property be agent scientist have name name carry certain reputation so name allow user check on specific expertise creator contributor publisher another property allow user specify funding source software also ask reassurance on commitment support software be whether have development community whether creator support several year etc. another aspect trust be understanding use software how rate metadata properties capture include adopter again be people project entity use statistics extract from registry cumulative download statistics automatically extract from repository where grammar%23NPR relate domain knowledge have domain keyword desc use grammar%23NPR desc have use grammar%23NPR desc similar grammar%23NPR desc trust quality grammar%23NPR have creator agent have publisher agent have major contributor agent commitment support desc have adopter grammar%23NPR have use grammar%23NPR desc have use grammar%23NPR desc use grammar%23NPR grammar%23NPR have benchmark grammar%23NPR desc have salient grammar%23NPR desc fund source desc have grammar%23NPR grammar%23NPR execute access download have code grammar%23NPR grammar%23NPR have executable grammar%23NPR grammar%23NPR license license install grammar%23NPR requirements have grammar%23NPR grammar%23NPR have grammar%23NPR grammar%23NPR desc have grammar%23NPR language language have dependency grammar%23NPR version require average memory measurement support grammar%23NPR system o average run grammar%23NPR desc have other grammar%23NPR detail desc run grammar%23NPR grammar%23NPR have test data desc have test grammar%23NPR desc do research experiment run other data have input i%40o have input parameter i%40o have output i%40o have relevant data source desc compose run other grammar%23NPR have interoperable grammar%23NPR desc have grammar%23NPR grammar%23NPR grammar%23NPR cite grammar%23NPR grammar%23NPR prefer grammar%23NPR grammar%23NPR update track grammar%23NPR have grammar%23NPR version version have version release date date supersede version supersede by version contribute grammar%23NPR have grammar%23NPR development desc have grammar%23NPR community desc grammar%23NPR locate unique grammar%23NPR have name desc have short grammar%23NPR desc have grammar%23NPR category desc have unique id grammar%23NPR have project site grammar%23NPR get support discuss support community have email contact email have grammar%23NPR support desc figure high-level overview grammar%23NPR software reside publication that use software execute software this category focus on describe how execute software go beyond instruction install run include how access software how run test+data several properties address access code include license information location where code reside repository local url location where self-contained executable found be because many scientist not want go through trouble install original source+code prefer pointer executable directly run other property provide execution requirement requirement allow installation software include documentation installation instruction implementation language operating+system average memory requirements another important property be runtime dependency for+example library library install where software run average runtime be also important typically not be just number but explanation expect runtime depend on size characteristic data include property specify additional implementation detail describe free text form be also properties properties help user run software include pointer test dataset instruction check that software run properly check that right result obtain test+data get support software this category address how user get support if have any question about appropriate use software have any problem installation wish ask about specific case not describe documentation include email+address contact description whether how software support research software this category include metadata enable scientist do research software want use note that metadata properties second category understand software be relevant here but expect user be already aware since consider before wish install run software some metadata properties specify particular input output requirements software include type constraint on input+data parameter as+well+as expected output another important metadata property point user data+source where other data run software find another set properties be concerned run software combination other software not need be specified as formal workflow provide as textual diagrammatic description metadata property include specify how cite software scientific publication update software this category include metadata find new version software point user community support future extension software some metadata properties provide track software version release date as+well+as properties indicate whether be new version whether be old version that supersede by software finally some metadata properties allow user find whether software actively develop pointer on-line community mailing lists issue track site etc. user collaborate further develop software conclusion this paper describe grammar%23NPR software registry design use grammar%23NPR ontology describe metadata scientific software ontology contain basic metadata properties describe how identify software understand do utility research execute get support if question arise research contribute development key contribution grammar%23NPR be that design center on user user focus on do scientific+research rather than software developer acknowledgment gratefully acknowledge support from us national+science+foundation grant grammar%23NPR like thank other member grammar%23NPR project including grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH also like thank many early+adopter grammar%23NPR in+particular grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH reference grammar%23NPR grammar%23NPH grammar%23NPR grammar%23NPH scientific software production incentive collaboration proceedings on grammar%23NPR shedding light on dark data long tail science library trend grammar%23NPH d. grammar%23NPH grammar%23NPH s. grammar%23NPH d. towards ontology communication large software+system vol. no. a. grammar%23NPR grammar%23NPH grammar%23NPR c. grammar%23NPR a. grammar%23NPR l sweetening ontology grammar%23NPR proceedings 13th on knowledge engineering knowledge+management grammar%23NPR grammar%23NPR grammar%23ENT machine assist human translation system technical vishwajeet%40cse.iitb.ac .in kulashish%40gmail.com indian+institute+of+technology pr.pankajsingh%40gmail.com ganesh%40cse.iitb.ac ganesh.arnaal%40smcapitaladvisors.in abstract translation system know benefit from availability bilingual lexicon domain interest system aim build such lexicon from source+language corpus often require human assistance confront by conflict requirements minimize human translation effort while improve translation quality present approach approach exploit redundancy source corpus extract recurring pattern pattern be frequent syntactically well form provide maximum corpus coverage
pattern generalize over phrase word type our approach find succinct set good pattern high coverage our interactive system leverage pattern multiple iteration translation post editing thereby progressively generate high+quality bilingual lexicon
subject descriptor grammar%23NPR information+systems application miscellaneous grammar%23NPR machine+translation natural+language+processing general+term machine+translation keyword mining machine+translation natural+language+processing introduction grammar%23NPR work problem language translation be in+focus many decade see contribution from both linguistic computer+science community linguistic grammar%23NPR make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright grammar%23ENT grammar%23ENT
come form several language resource comprising dictionary grammar studies on unit translation statistical approach mt fall general category memory based mt corpus based approach suffer from two major drawback parallel corpus be often expensive generate be often scarce unavailable certain language pair domain their quality translation be not as good as human translation therefore not suitable certain application like involve translation government document academic book
based machine+translation system grammar%23NPR like grammar%23NPR alleviate need sentence align parallel corpus but require explicit linguistic data form morphological bilingual+dictionary grammar structural transfer rule in- domain especially technical legal corpus often adhere certain lexical syntactic structure be good candidate translation use rule based system provide necessary formalism theorize rule based translation also provide system author abstract concrete language syntax
be body+of+work study complementary contribution human mt model present machine centric translation system system leverage human input system refer as computer-aided+translation system typically employ statistical mt model translate text provide post editing tooling enable human correct resulting translation
alternative pure post editing system combine mt engine human interactive setup where mt engine continuously exploit human feedback attempt improve future translation constitute right unit translation how human feedback incorporate underlying translation model pose interesting research challenge
further motivate by grammar%23NPR principle compositionality state that meaning compound expression be function meaning part grammar%23NPR this paper tackle important task represent researcher expertise life+science domain develop algorithm all main underpinning research question automated mesh classification base on this algorithm also develop method matching researcher base on their expertise our main contribution be two-fold grammar%23NPR novel hybrid mesh classification framework framework combine three widely used method our evaluation use large benchmark dataset show on task classify use entire mesh taxonomy grammar%23NPR be effective outperform widely used state-of-the-art method compare grammar%23NPR another state-of-the-art system capable performing classification use entire mesh taxonomy grammar%23NPR also show high improvement ratio over
researcher match algorithm respective their expertise base on aggregation mesh classification researcher published article exploit structure mesh
our proposed framework matching researcher expertise have broad application beyond just life+science any discipline shared comprehensive taxonomy describe disciplinary topic utilise this framework discipline respective taxonomy include computer+science association+for+computing+machinery computing classification+system combined+chiefs+of+staff www.acm.org%2Fabout%2Fclass engineering www.ieee.org%2Fpublications_ standard publication service grammar%23NPR mathematics mathematics subject classification msc http msc2010.org physics physics pacs www.aip.org%2Fpublishing pacs name
background endeavour increasingly conduct interdisciplinary manner large body research scientometrics develop method analyse map scientific+research especially on global scale however from institutional perspective be also urgent need good understanding research expertise individual researcher organisational unit as+well+as institution as facilitate research collaboration enable informed decision+making research strategy policy
large+number tool system propose measure analyse map visualise research specialty expertise usually base on bibliographical information collaboration and%2For citation network
mesh classification classification article using controlled vocabulary be common practice a+number+of discipline where article assign a+number+of term from vocabulary denote major research topic life+science mesh be large taxonomy develop maintain by nlm as shared vocabulary classification research article widespread adoption comprehensive coverage domain enable mesh classification employ represent researcher expertise
human expert still need review automatically generated mesh term article require extensive domain knowledge from subject expert be time-consuming error-prone assist accelerate semi-automatic mesh classification process nlm first introduce automatic indexing tool en-nahdha have lead development various method improve automatic mesh classification
recent approach mesh classification divide into two broad category text+mining scheme machine+learning scheme
text+mining scheme aim identify particularly useful feature from training article use mesh classification premise be that such feature well characterize content article help identify strongly evident interesting relationship between feature article grammar%23NPH cohen employ term document matrix use represent such relationship where row represent article column represent term derive from training article grammar%23NPR et+al. propose refinement use entry binary valued weight instead real valued weight improve performance find neighbor jimeno-yepes et+al. show that combination citation use only title abstract different type summary generate automatically from full text article improve recommendation suggest by grammar%23NPH propose extract concepts from from target article convert concepts mesh term candidate finally recommend candidate co-occurrence frequency be above threshold mesh classification such frequency find from large corpus article 22%2C000%2C000 article previously index by domain expert
machine+learning scheme focus on learning from training article index ground truth mesh term learned model use assign mesh term high relevance score target article citation information exploit enrich feature set binary classifier e.g.
then train learn mesh term associate particular training article base on proposed feature representation this system evaluate on task classifying article use only grammar%23NPR most frequent mesh term benchmark dataset show good performance
et+al. propose use approach hereafter refer as grammar%23NPR where learn how assign relevant score mesh term from training article give target article find neighbouring article extract list mesh term from article finally assign relevance score term target article evaluate task classification over entire mesh taxonomy on large benchmark dataset grammar%23NPR show superior performance over existing system represent state-of-the-art
over entire mesh taxonomy be more useful task than over grammar%23NPR most frequent mesh term as allow more nuanced characterisation publication topic be especially true as half grammar%23NPR most frequent term from as show figure be very generic terms such as mutation cell cultured calcium reside on 2nd 3rd level mesh taxonomy on+the+other+hand also more challenge as classification method need handle much large number class label 26%2C000 versus this paper tackle problem classification over entire mesh taxonomy compare against grammar%23NPR our evaluation
researcher matching most previous approach researcher expert matching focus on measure matching score between researcher query application such as paper review assignment 35%25 20%25 17%25 17%25 15%25 11%25 10%25 10%25 10%25 9%25 9%25 8%25 8%25 7%25 7%25 6%25 6%25 6%25 6%25 0%25 10%25 20%25 30%25 40%25 50%25 60%25 im als grammar%23NPR m s m ole lar se qu en ce d at m ice ra t be ino qu en ce ba se se qu en ce m ut at ion ce ll ltu re d ce ll l ine ne tic s ba cte ria l p ro te royal+navy m e se ng er si gn al tra ns du cti on ra t sp ra gu e da wl ey grammar%23NPR bin din g pr ot ein s m em br e pr ot ein s re co m bin t p ro te ins ca lci um cl on ing m ole lar mesh term f re qu en cy figure grammar%23NPR most frequent mesh term
review assignment expert often represent use set publication query often represent form topic extract from document use topic model language model matching score mostly measure by probabilistic model such as keyword matching latent semantic indexing probabilistic topic modeling
work differ from previous approach some key aspect
our algorithm focus on researcher researcher matching but not expertise expertise matching our algorithm base on semantic similarity between terms taxonomy incorporate semantic knowledge taxonomy on+the+contrary previous approach mainly rely on free text hence not make use insight into target domain provide by taxonomy our matching algorithm also consider formalise utilise weight researcher expertise compare
our algorithm easily adapt other domain where domain knowledge exist form taxonomy
grammar%23NPR grammar%23NPR classification framework this section propose grammar%23NPR hybrid mesh classification method give target article x grammar%23NPR be able predict relevance score mesh term with+respect+to x rank mesh term by their relevance score relevance ranking form foundation mesh classification
hybrid method make use recommender system recommend item i.e. mesh term this context user
system generally divide into two category collaborative method compact+flash base on user similarity content based method cb base on item similarity specifically combine baseline grammar%23NPR classifier develop by nlm collaborative recommender denote grammar%23NPR content based recommender denote grammar%23NPR grammar%23NPR recommend mesh term x base on mesh term use classify similar article x grammar%23NPR recommend mesh term x base on model learn from training article use multi label classifier
grammar%23NPR be set mesh term classification
r x m be function measure relevance score mesh term target article x r x m normalise into from completely irrelevant completely relevant goal our method be return ranked list mesh term grammar%23NPR m2 mk m article x where r x mi r x mj j
data representation objective both grammar%23NPR grammar%23NPR be predict relevance score mesh term target article use training data consist biomedical article human mesh term represent training data as matrix where row represent article column consist two space feature space hold set most informative feature where each entry indicate how important feature be with+respect+to article take real+value class space hold set mesh term use classify each article where each entry take binary value indicate whether mesh term use classify article denote by not denote by
formally define representation training data following notation use rest this paper grammar%23NPR be set term from mesh taxonomy
let be set training article
let f be set all possible feature extract from a.
let grammar%23NPR be finite+set most informative feature carry primary information about article a.
above definition training data d formally represent as %C3%97 f m matrix each row ri represent combination feature space class space article ri wi wi f feature space ci ci m class space where wi indicate informativeness feature fj article ai ci indicate whether mesh term mk use denote by not denote by classify ai
grammar%23NPR grammar%23NPR use any type term concepts derive from training article that effectively characterize language+system grammar%23NPR be large collection comprehensive controlled vocabulary including mesh life+science also provide mapping vocabulary this work use grammar%23NPR concepts as feature recommend by grammar%23NPR from title abstract article premise be that feature be suitable capture essential semantic representation thus effective represent topic precisely
determine informativeness feature fj f each article apply widely used tf israel+defense+forces measure
this measure k most informative feature select use represent training data d
collaborative method grammar%23NPR grammar%23NPR take three main step first step give target article x feature extract feature then represent as vector denote as x use most informative feature f grammar%23NPR x informativeness feature calculate use tf israel+defense+forces from training data d where ti j denote number time that fj appear x
second step set near neighbouring article x identify d. find adopt widely used similarity measure grammar%23NPR correlation+coefficient more specifically give x use vector x find similar article a. each article compare also represent as vector using only feature space then similarity sim x between two article x i calculate as following similarity sim x between two vector x sim x j ej-e e j-e j ej-e j e j-e where ej resp. e j be j-th value x resp. e x j grammar%23NPR resp. e j grammar%23NPR j be average x resp. this similarity indicate ratio angle relationship between x standard+deviation two vector normalize into closer value be high similarity be use this equation say that two feature vector be similar if their similarity be above threshold often deem as minimum value strong correlation
final step once find most similar article new+hampshire x target article x grammar%23NPR recommend ranked list mesh term grammar%23NPR x their relevance score classify x use new+hampshire x formally relevance score rcf x m mesh term m with+respect+to x grammar%23NPR define as rcf x m x sim x e r e m x sim x e where sim x e be similarity between x new+hampshire x r e m be relevance score m with+respect+to e r e m be if m use classify e otherwise note that more similar x e be more r e m contribute calculation rcf x m
content grammar%23NPR method grammar%23NPR grammar%23NPR relevance score mesh term target article estimate by model learn from training data d use grammar%23NPR grammar%23NPR classifier this classifier learn how know particular mesh term i.e. multi label associate most informative feature f f article d then use predict highly relevant mesh term target article
classification method divide into two approach problem transformation method method transform classification task into one more grammar%23NPR grammar%23NPR classification task algorithm adaptation method method extend specific grammar%23NPR classification algorithm directly accommodate grammar%23NPR data this work grammar%23NPH representative first approach choose as grammar%23NPR classifier grammar%23NPR due highly predictive performance computational efficiency large+number label
randomly divide set label e.g. mesh term into set small k-labelsets train sl classifier on each k-labelset use lp thus grammar%23NPH learn t sl classifier grammar%23NPR ht use t disjoint labelsets grammar%23NPR t t j grammar%23NPR use lp lp be transformation method consider each k-labelset as distinctive class new grammar%23NPR classification task classify target instance hj t thus find most probable class i.e. set label from grammar%23NPR eventually grammar%23NPH perform t sl classifier grammar%23NPR ht make prediction different k-labelsets then combine their prediction as sl classifier choose grammar%23NPR rf be ensemble randomized decision+tree due ability run efficiently on large dataset
our context give training data d grammar%23NPH learn t rf classifier grammar%23NPR ht use lp from different k-labelsets randomly select from m m set mesh term use classify training article classifier hj t learn hj 2%2C000%2C000 where each instance i represent feature vector use tf israel+defense+forces 2%2C000%2C000 be powerset m. our study k k-labelset choose as t choose as m as optimal value according+to suggestion
new target article x binary prediction hi x lj all sl classifier hi all label lj li aggregate make final prediction about degree relevance all label lt denote as t grammar%23ENT mean each prediction each label calculate use as relevance score lj x formally relevance score rcb x lj label i.e.
mesh term with+respect+to x grammar%23NPR compute as rcb x lj t hi x lj where t be number sl classifier hi x lj calculate if lj see corresponding k-labelset li use relevance score all label lt grammar%23NPR finally recommend ranked list mesh term grammar%23NPR x their relevance score classify x
our grammar%23NPR hybrid recommender often combine cf cb some way gain complementary benefit eventually lead better recommendation performance motivate by independently linearly combine mesh term their relevance score score recommend from both grammar%23NPR grammar%23NPR be capability both recommender independently exploit make final mesh term prediction straightforward way i.e. linearly this approach show effective build composite hybrid recommender multiple recommender another feature our hybrid recommender grammar%23NPR be incorporation our objective be independently combine individual recommendation score in+addition+to grammar%23NPR grammar%23NPR improve quality overall mesh term prediction
general definition grammar%23NPR describe as+follows let red+cross grammar%23NPR grammar%23NPR rcn be set all recommender combine grammar%23NPR thus this work red+cross contain grammar%23NPR grammar%23NPR respectively red+cross also let rci n x be ranked list mesh term recommend by rci given target article x then given x grammar%23NPR generate ranked list mesh term grammar%23NPR x grammar%23NPR m2 mz m according+to their final relevance score x i.e. r x mi r x mj j relevance score mesh term mi z grammar%23NPR x with+respect+to x compute as r x mi red+cross j wj rcj x mi red+cross j wj red+cross j rcj x where rcj x mi be relevance score mi with+respect+to x determine by recommender rcj notation wj represent weight corresponding recommender rcj indicate how+much rcj x mi contribute calculation relevance score r x mi note that relevance score mesh term recommend by grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR normalize into real+value on+the+other+hand raw score directly recommend from grammar%23NPR not normalize normalize score first transform into their log scale scoresand then divide by respective maximum from target article here log transformation apply mitigate difference between each score maximum
definition optimal value each wj n require complete knowledge real recommendation capability each recommender rcj be not available thus this work assume that all recommender red+cross be equally important each other i.e. grammar%23NPR grammar%23NPR w red+cross where red+cross j wj this assumption be also consistent general assumption linearly weighted hybrid recommendation approach
mesh classification evaluation evaluate grammar%23NPR on task classification entire mesh taxonomy use dataset previously use evaluate state-of-the-art system grammar%23NPR evaluation conduct on nlm dataset hereafter refer as grammar%23NPR often use as benchmark evaluate grammar%23NPR classifier additionally compare as strong baseline method our evaluation grammar%23NPR show superior performance over both grammar%23NPR
evaluation methodology follow that grammar%23NPR i.e. evaluate performance base on grammar%23NPR rank mesh term each article grammar%23NPR use same performance metrics as grammar%23NPR be f measure more detail on metrics find
intend compare grammar%23NPR grammar%23NPR on same training testing that end use same test article from grammar%23NPR use evaluate grammar%23NPR train grammar%23NPR employ same training set grammar%23NPR collection 10%2C000 article consist most similar article each article grammar%23NPR neighbouring article grammar%23NPR
note that grammar%23NPR use two additional training set 13k article extract a+number+of meta- level feature require train ranking algorithm i.e. grammar%23NPR 10%2C000 article train grammar%23NPR ranking purpose
grammar%23NPR base on only use grammar%23ENT version mesh that current support where grammar%23NPR recommend terms from grammar%23ENT version mesh contain close 5%25 fewer main heading terms moreover grammar%23NPR evaluate against grammar%23ENT version denote grammar%23NPR only compare current version denote grammar%23NPR moreover surprisingly as see table performance more recent grammar%23NPR be sufficiently bad than older grammar%23NPR
above difference completely fair comparison between grammar%23NPR grammar%23NPR be not possible moreover give that be key component grammar%23NPR performance grammar%23NPR directly impact by performance therefore compare two system indirectly by their improvement ratio f measure over their respective baseline
training set total number grammar%23NPR feature be 28%2C000 only consider feature set consist 1%2C000 most informative feature increment from such large feature space total 9%2C696 class i.e. mesh term actually use training set however observe that distribution grammar%23NPR grammar%23NPR rate terms be highly imbalanced grammar%23NPR rate each mesh term represent rate number article contain term over grammar%23NPR grammar%23NPR grammar%23NPR
grammar%23NPR training testing set be available at www.ncbi.nlm.nih.gov%2Fcbbresearch%2Flu%2Findexing
number article training set for+example df rate 7%2C555 class be less+than 0.02%25 i.e. df while grammar%23NPR rate only class be above 0.5%25 i.e. df
such imbalanced distribution class cause unsatisfactory classification outcome classifier since target article be much likely classify into majority class avoid this problem one way be ignore minor class class rarely appear training set thus grammar%23NPR restrict number mesh terms recommend grammar%23NPR rate mesh term be equal 0.8%25 i.e. grammar%23NPR as be 10%2C000 article training set now compare performance grammar%23NPR result grammar%23NPR directly obtain from as+well+as grammar%23NPR
summarise result according+to grammar%23NPR sign rank test at 99.9%25 confidence find that grammar%23NPR significantly outperform grammar%23NPR terms f measure as indicate by bullet
previously described compare base on improvement ratio over their respective baseline grammar%23NPR terms f measure in+other+words compare grammar%23NPR as directly obtained from
compare grammar%23NPR grammar%23NPR due difference mesh version result be slightly different as see from table finally as observe grammar%23NPR be competitive grammar%23NPR terms f measure in+particular observe that grammar%23NPR significantly outperform grammar%23NPR terms improvement ratio both f measure noticeably much good improvement ratio be 48%25 compare 39%25 grammar%23NPR
comparison between grammar%23NPR grammar%23NPR
raw result improvement ratio f measure f measure n%2Fa n%2Fa grammar%23NPR 23%25 39%25 grammar%23NPR n%2Fa n%2Fa grammar%23NPR 26%25 48%25 through evaluation on classification over entire mesh taxonomy observe that grammar%23NPR significantly outperform current version widely used baseline system
also outperform state-of-the-art grammar%23NPR terms improvement ratio
researcher match algorithm base on semantic similarity match researcher by their expertise be important way identify research strength facilitate potential collaboration opportunity this capability be especially useful researcher different faculty institution disciplinary area
important problem need address researcher matching specifically how represent expertise researcher target domain 2. how measure weight importance each piece expertise with+respect+to research target domain grammar%23NPR note that be optimal value mesh term between increment how define matching score between two researcher target domain researcher expertise naturally derive from publication use as foundation represent researcher expertise moreover argue that domain comprehensive taxonomy be natural feasible represent researcher expertise through aggregation weighted term collect from publication
this section address researcher expertise match task by address above three problem first represent expertise researcher use mesh term term classify publication section second propose method method measure weight each mesh term i.e. piece expertise with+respect+to researcher use number occurrence publication section third propose algorithm algorithm define calculate matching score between two researcher by aggregate matching score between mesh term researcher section measure matching score between two mesh term key idea be combine semantic similarity between their weight with+respect+to researcher
be worth noting that our method be not specific life+science mesh be applicable any domain comprehensive taxonomy such as computer+science association+for+computing+machinery ccs engineering physics pacs as discuss section
expertise researcher as grammar%23NPR use as input our researcher match algorithm generate mesh term from list publication essentially expertise researcher represent by multiset mesh term aggregate from publication
be due fact give publication be possible mesh term appear once as mesh term extract by from each sentence content publication
e be set researcher expert let p be set all publication publish by expert e pe be all publication publish by expert let m be set all possible mesh term annotate classify publication p. then represent expertise expert e as set mesh term annotate publication pe
publication p p denote by tp m n multiset mesh term use annotate p express as function from set mesh term m natural+number n
give publication p assume that have three sentence abstract mesh term that annotate p represent by following set set mesh term each sentence grammar%23NPR m2 m3 grammar%23NPR m2 m2 m3
represent by tp grammar%23NPR m2 m3 grammar%23NPR m2 m2 m3 alternatively tp grammar%23NPR grammar%23NPR grammar%23NPR measuring weight each publication usually annotate by a+number+of mesh term each researcher usually have expertise a+number+of topic represent by mesh terms be important measure relative importance terms given researcher
key idea measure weight importance each piece expertise i.e. each mesh term expert base on number occurrence publication e i.e. pe our objective be generate ranked list mesh terms e. this purpose define function fm e e measure weight mesh term m use annotate one more publication pe e i.e.
e grammar%23NPR where weight be real+number normalise between high weight mesh term m be more relevant be researcher e
weight m e fm e obtain through aggregation from pe set all publication e. define function fm p e e measure weight mesh term within publication expert as+follows fm p e tp m maxztp z where tp m be number time that mesh term m appear publication p as define above maximum compute over tp z over all mesh term appear p. recall as describe section each mesh term appear once given publication
define function fm p e function fm e define more formally as+follows take+into+account all publication e i.e. pe fm e p e pe fm e calculate weight mesh term m researcher e from each publication average all publication pe
measuring matching score between generate ranked list mesh terms each researcher domain make use define matching relevance score between two researcher need define function fe e given two researcher e e fe e grammar%23NPR high weight be two researcher match relevant meaning that more share common expertise
this section present matching algorithm algorithm calculate fe e give two researcher basic idea underpin algorithm be aggregate matching score between all pair mesh terms from two researcher key idea measure matching score between two mesh terms be combine following two factor semantic similarity between mesh terms weight calculate previous section mesh terms
semantic similarity our context definition semantic similarity be function sim grammar%23NPR m2 where be mesh terms compare range this function normalize real+number between completely dissimilar identical thus similarity tend be as grammar%23NPR m2 have more more common characteristic notion be subjective define expertise similarity measure base on domain knowledge capture through mesh terms
traditional syntactic similarity measure have common limitation limitation rely on notion syntactical difference between element compare in+other+words approach have limited capability obtain reasonable similarity score due disregard domain knowledge .to improve traditional similarity approach importance exploit available background+knowledge about application domain be recent research focus
taxonomy such as mesh low concepts i.e. mesh terms inherit all characteristic from their superordinate concept namely high position concept taxonomy more abstract concept be in+addition highly related concepts group together path between two different concepts hierarchy reflect how be semantically related application domain as consequence use semantic knowledge about taxonomy provide useful insight underlying domain knowledge thereby facilitate comparative analysis concept similarity measurement
our work leverage mesh taxonomy where grammar%23NPR mesh terms arrange hierarchically from most general most specific up hierarchical level
mesh on first level mesh terms organize into broad category including category anatomy category b organism category c disease each category further divide into successive subcategory each mesh term appear at+least one place tree appear additional place where appropriate
general be two approach compute semantic similarity between two mesh terms distance based approach approach estimate distance between two element node base information content approach approach estimate amount shared information content between two element
main problem distance based approach be that assume that edge connect element tree represent uniform distance i.e. all semantic edge have same weight for+example assume that fig. every edge have uniform distance distance between disease organism be this distance be identical distance between neoplasm however intuitively judge that similarity between latter pair terms be high than former pair terms since latter pair be siblings position low level tree thus share more specific information than former terms
simple snippet mesh tree overcome this problem choose node based approach
tree node represent unique concept contain certain amount information edge represent relation between two concepts similarity between two concepts be then extent share information common
more information share more similar are measure information content each mesh term m use mesh grammar%23NPR represent as tree denote as ic m use following method ic m log sc m log m where sc m be set transitively descendent terms m m be size total number term m. denominator be equivalent value most informative concept serve as normalizing factor assure that ic m be merit this method be exploit only structural information tree not rely on corpus analysis as hence more efficient
apply grammar%23NPR similarity measure define similarity between two mesh terms reason be that several study commonly find that grammar%23NPR measure be most effective outperform other approach this measure define use information content mesh terms grammar%23NPR m2 measure by as+follows sim grammar%23NPR m2 ic grammar%23NPR ic m2 grammar%23NPR lc grammar%23NPR m2 where lc grammar%23NPR m2 be little common subsumer superclass subsume both grammar%23NPR m2 this formulation normalise similarity score interval
researcher match algorithm now present our researcher match algorithm let grammar%23NPR grammar%23NPR grammar%23NPR h grammar%23NPR grammar%23NPR m2 g be two researcher represent by their respective mesh terms compare
matching score between grammar%23NPR grammar%23NPR fe e grammar%23NPR grammar%23NPR compute from both similarity score measure by weight mesh terms compare compute by as+follows fe e grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m2 j sim grammar%23NPR m2 j grammar%23NPR m2 j where grammar%23NPR m2 grammar%23NPR m2 j denote strength weight two terms grammar%23NPR m2 j define as+follows grammar%23NPR m2 j grammar%23NPR grammar%23NPR j grammar%23NPR where grammar%23NPR grammar%23NPR grammar%23NPR j grammar%23NPR compute use note note grammar%23NPR grammar%23NPR j grammar%23NPR represent absolute+value grammar%23NPR grammar%23NPR grammar%23NPR j grammar%23NPR but not set cardinality as be case elsewhere paper our intuition be that two mesh terms have high strength value if have similar weight low absolute+value difference between weight value their respective researcher in+other+words similarity between two piece expertise represent by mesh terms adjust by difference expertise level between two researcher on two piece expertise
conclusion accurate representation researcher expertise lead more effective facilitation research collaboration more informed research planning decision+making however be very challenging task large institution diverse discipline
this paper vast life+science domain present framework matching researcher expertise base on taxonomy our main contribution be two-fold
novel hybrid mesh classification framework framework combine three widely used method our evaluation demonstrate superior performance over state-of-the-art system mesh classification use entire mesh taxonomy on recent benchmark novel researcher expertise match algorithm base on semantic similarity from aggregated mesh terms exploit structural information mesh taxonomy
proposed framework be domain independent although focus on mesh life+science this paper this framework apply any broad scientific+discipline develop shared comprehensive taxonomy ontology describe disciplinary topic discipline include computer+science engineering institute+for+cybernetics among
plan a+number+of future work direction continue enhance our mesh classification algorithm by exploit association between mesh terms article aim uncover latent mesh label further investigate researcher match algorithm exploit not only structure mesh but also statistical properties such as co-occurrence mesh terms dataset conduct user study evaluate accuracy also develop prototype system system incorporate functionality
reference on optimization expertise match various constraint neurocomputing
toward next+generation recommender system survey possible extension knowledge institute+of+electrical+and+electronics+engineers transactions on 734+749
improve mesh classification biomedical article use citation context
881+896
grammar%23NPH medical text indexer grammar%23NPR 268+272
performance prediction recommender system application dynamic optimisation aggregative method
thesis grammar%23ENT

forest mach grammar%23NPR grammar%23ENT
hybrid recommender system survey experiment user modeling 331+370 grammar%23ENT
provincia+de+herrera science map software tool review analysis cooperative study among tool journal grammar%23ENT grammar%23ENT
statistical power analysis behavioral+science 2nd edition

model expert finding proceedings 29th on ir research grammar%23NPR page 418+430 grammar%23NPR springer-verlag

hierarchical domain structure compute similarity association+for+computing+machinery grammar%23NPR inf. syst. grammar%23ENT
grammar%23NPH j. m grammar%23NPR variable selection use random forest
grammar%23NPR grammar%23NPR grammar%23ENT
recommend mesh term annotate biomedical article journal 660+667
grammar%23NPH mesh indexing base on automatically generated summary page
y. b grammar%23NPH knowledge rich similarity measure improve incident resolution process proceedings grammar%23ENT association+for+computing+machinery symposium on applied computing page grammar%23ENT grammar%23ENT new+york new+york united+states+of+america association+for+computing+machinery
unsupervised medical subject+heading assignment using output label co-occurrence statistics semantic predication natural+language+processing information+systems
unified medical language+system
information medicine 281+291
der veer marten
research specialty 213+295
grammar%23NPH s. v. grammar%23NPH measure semantic similarity relatedness biomedical domain
288+299 grammar%23ENT
science become more interdisciplinary measure map six research field over time grammar%23NPR 719+745 grammar%23ENT
machine+learning from imbalanced data+set proceeding on imbalanced data+set
semantic similarity taxonomy information based measure application problem ambiguity natural+language

interaction domain knowledge linguistic structure natural+language+processing interpret hypernymic proposition biomedical text j.
editor automatic text+processing united+states+of+america
intrinsic information content metric semantic similarity wordnet proceedings on artificial+intelligence page
random k-labelsets multilabel classification institute+of+electrical+and+electronics+engineers grammar%23NPR
grammar%23NPR grammar%23ENT
v. grammar%23NPH reflective random indexing semi-automatic indexing biomedical literature 694+700 grammar%23ENT
grammar%23NPH grammar%23NPH e. v. grammar%23NPR deterministic binary vector efficient automated indexing grammar%23NPR abstract
symposium
grammar%23NPR based document model ad-hoc retrieval proceedings 29th on research+and+development information+retrieval grammar%23NPR page 178+185 new+york new+york united+states+of+america testing grammar%23NPR axiom against data enrich schema grammar%23NPH univ. nice grammar%23NPH grammar%23NPR grammar%23NPR grammar%23NPH andrea.tettamanzi%40unice.fr grammar%23NPH univ. nice grammar%23NPH grammar%23NPR grammar%23NPR grammar%23NPH faron%40unice.fr grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPH fabien.gandon%40inria.fr abstract axiom scoring be critical task both automatic enrichment learning automatic validation knowledge+base ontology design develop axiom score heuristic base on possibility theory theory aim at overcome some limitation scoring heuristics base on statistical+inference take+into+account open world assumption linked data on since compute possibilistic score be computationally quite heavy some candidate axiom propose method base on+time capping alleviate computation heuristic without give+up precision score evaluate our proposal by apply problem test grammar%23NPR axiom against dataset
ontology learning open world assumption possibility theory introduction be common practice semantic+web put strong emphasis on construction reuse ontology base on principled conceptual analysis domain interest as prerequisite organization much like database schema design before database populate while this approach be quite successful when apply specific domain not scale well more general setting be dogmatic not lend collaborative effort etc. be why alternative bottom-up approach ontology knowledge+base creation good suit many scenario instead postulate priori conceptualization reality i.e. ontology require that our knowledge about fact comply start from rdf fact learn owl axiom
contribution towards automatic creation owl ontology from large repository rdf fact include foil-like algorithm learning concept definition permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america copyright grammar%23ENT grammar%23ENT
dx.doi.org%2F10.1145%2F2815833.2815835 statistical schema induction via association rule mining light weight schema enrichment method base on dl learner framework method apply extend technique develop within recent survey wide field ontology learning see
exist also need evaluate validate ontology be result analysis effort semiautomatic learning method this need witness by general methodological investigation survey tool like oops detect pitfall ontology ontology engineering methodology such as grammar%23NPR distinguish two validation activity namely verification through formal+methods syntax logic etc. validation through usage whilst this latter be usually thought as user study automatic process validation base on data provide cheap alternative whereby existing linked data regard as usage trace use test improve ontology much like log mining use provide test+case development replay approach
regard ontology as set integrity constraint check if data satisfy use tool like pellet translate owl ontology into grammar%23NPR query automatically validate data similar approach also underlie idea test driven evaluation linked data quality this end owl ontology interpret closed world assumption weak unique name assumption
this validation process see from reverse point+of+view instead start from priori assumption that given ontology be correct verify whether fact contain base satisfy treat ontology like hypothesis develop methodology verify whether rdf fact corroborate falsify ontology learning validation be thus strictly related even see as agile test driven approach ontology development where linked data use as giant test+case library not only validate schema but even suggest new development
learning validation rely critically on candidate axiom scoring this paper tackle problem test single isolated axiom axiom be first step solve problem validate entire ontology
focus our evaluation example restrict our attention subsumption axiom form grammar%23NPR c d most popular scoring heuristics propose literature base on statistical+inference argue that such probability based framework be not satisfactory propose axiom score heuristic base on formalization possibility theory notion logical content theory falsification inspire by grammar%23NPH approach epistemology work open world assumption
first result indicate that apply possibilistic approach task test candidate axiom ontology learning yield very promise result hint that same approach be beneficial ontology knowledge+base validation as+well however proposed heuristic be much heavy from computational point+of+view than probabilistic score aim complement fortunately be evidence see section below that time take test axiom tend be inversely proportional score suggest that time cap test be acceptable additional heuristic decide whether accept reject candidate axiom axiom take too long test likely end+up have very negative score this paper follow this suggestion investigate effectiveness time capped possibilistic testing owl axiom against fact contain repository our research question be therefore can time capping alleviate computation proposed possibilistic axiom score heuristic without give+up precision score this paper organize as+follows section present principle axiom testing section propose axiom score heuristic base on possibility theory framework axiom scoring based on such heuristic then present section evaluate on subsumption axiom section section draw some conclusion direction future work
principle axiom against dataset do by check whether formula entail by confirm by fact contain owl semantics refer direct model-theoretic semantics owl as define interpretation datatype map d vocabulary v over d define by interpretation domain d be object domain d data domain valuation function seven restriction c map class expression subset op map object property subset grammar%23NPR dp map data property subset grammar%23NPR map individual element grammar%23NPR map datatypes subset d lt map literal element set data+value grammar%23NPR grammar%23NPR d algerian+land+forces map facet subset grammar%23NPR grammar%23NPR
content support confirmation counterexample axiom let be candidate axiom denote by support i.e. cardinality set formula entail by grammar%23NPR call link data search+engine like grammar%23NPR virtually extend dataset whole cloud
www.w3.org%2Ftr%2F2012 grammar%23NPR section interpretation interpretation test against fact contain dataset define this notion support with+respect+to dataset more precisely
define content axiom content as finite+set formula test against dataset k construct from set theoretic formula express semantics by ground i.e. by omit quantifier substitute universally quantify variable symbol x denote individual by every resource r literal l occur k substitute all symbol ci denote subset by their corresponding class name datatype name c substitute all symbol rhode+island denote subset %C3%97 grammar%23NPR by their corresponding object data property name r
let consider test candidate axiom grammar%23NPR dbo launchpad dbo infrastructure dbo launchpad v dbo infrastructure syntax against dataset semantics be dbo launchpad dbo grammar%23NPR also write as x x dbo grammar%23NPR x dbo grammar%23NPR
thus express content as dbo launchpad r dbo infrastructure r r be resource occur
by construction content indeed let be model by+definition be also model formula express semantics fortiori also grounding since be grounding formula express semantics be model
give formula content rdf dataset k be three case k this case call confirmation k this case call counterexample this case be neither confirmation counterexample
definition content refine by adopt grammar%23NPR grammar%23NPR principle selective confirmation characterize confirmation as fact not simply confirm candidate axiom but further favor axiom rather than contrary for+instance occurence black raven selectively confirm axiom raven v black because both confirm fail confirm negation namely negation there exist raven raven be not black on+the+contrary observation green apple grammar%23NPR construe as grammar%23NPR x li xdoes not contradict raven v black but not disconfirm either i.e. not selectively confirm raven v black
definition content be further refined in+order+to restrict just be counterexample thus leave+out be trivial confirmation be like say that test hypothesis try as hard as can refute
case grammar%23NPR c d axiom involve existence resource r c r either be confirmation if k d r fall into case otherwise therefore such not interest leave+out content grammar%23NPR c d
this principle greatly reduce content therefore number check
now define support as cardinality content content since rdf dataset be finite be also finite
denote by u number formula content entail by rdf dataset confirmation by u number such formula negation entail by rdf dataset counterexample notice be possible that some content rdf dataset entail neither case above therefore u u for+example when test dbo launchpad v dbo infrastructure against grammar%23NPR dataset find that u i.e. be confirmation dataset u i.e. be couterexample dataset namely dbo launchpad united+states+of+america dbo infrastructure united+states+of+america since grammar%23NPR dbo launchpad united+states+of+america grammar%23NPR dbo infrastructure united+states+of+america
one formula content be neither confirmation counterexample namely dbo launchpad grammar%23NPR dbo infrastructure grammar%23NPR because grammar%23NPR dbo launchpad grammar%23NPR grammar%23NPR dbo infrastructure grammar%23NPR grammar%23NPR dbo infrastructure grammar%23NPR
interesting properties u u be following u u confirmation be counterexample u u counterexample be confirmation u have same support
present axiom score heuristic heuristic capture basic intuition behind process axiom discovery base on possibility theory assign candidate axiom degree possibility equal just means that this axiom be possible plausible i.e. not contradict by fact knowledge+base be much weak than assign probability equal meaning that candidate axiom certainly be axiom
possibility theory possibility theory be mathematical+theory epistemic uncertainty give finite universe+of+discourse ohm element ohm regard as event value variable possible+world state+of+affairs possibility distribution be mapping ohm assign each degree possibility range from impossible exclude completely possible normal possibility distribution there exist completely possible state+of+affairs ohm say normalize
be similarity between possibility distribution probability density however stress that just means that be plausible normal situation therefore not exclude degree possibility then view as upper+bound degree probability see discussion about relationship between fuzzy set possibility probability degree possibility theory be suitable represent incomplete knowledge while probability adapt represent random observed phenomena
possibility distribution induce possibility measure dual necessity measure denote by n respectively both measure apply set ohm formula by way set model define as+follows max n grammar%23NPR min grammar%23NPR here be few property possibility necessity measure induce by possibility distribution on finite universe+of+discourse ohm n ohm n ohm ohm grammar%23NPR grammar%23NPR duality ohm n imply imply n
complete ignorance on grammar%23NPR
possibility necessity axiom basic+principle establish possibility formula be that absence counterexample rdf repository mean i.e. that be completely possible
hypothesis regard as all+the+more necessary as explicitly support by fact not contradict by any fact all+the+more possible as not contradict by fact in+other+words give hypothesis if no counterexample found as number counterexample increase strictly monotonically n grammar%23NPR no confirmation found as number confirmation increase no counterexample find n strictly monotonically notice that confirmation be counterexample that counterexample be confirmation
definition n n capture above intuition but by+no+means only possible one be u n if u if u
axiom score combine possibility necessity axiom define single handy acceptance rejection index grammar%23NPH as+follows grammar%23NPH n n n negative grammar%23NPH suggest rejection whilst positive grammar%23NPH suggest acceptance n strength proportional absolute+value value close zero reflect ignorance about status
this grammar%23NPH be useful purpose analyze result our experiment visualize distribution tested axiom with+respect+to single axis always bear+in+mind that axiom score by proposed heuristic term two bipolar figure+of+merit meaning though related be very different express degree consider normal sense not exceptional not surprising not contradict by actual observation n on+the+other+hand express degree be certain granted by positive evidence corroborate by actual observation
framework candidate axiom test general algorithm test all possible owl axiom given rdf store be beyond scope this paper
restrict our attention expression grammar%23NPR axiom
axiom their grammar%23NPH require compute interpretation expression
computational definition define mapping q e x%29 from owl class expression grammar%23NPR graph pattern where e be owl class expression x be variable such that query select grammar%23NPR x where q e x%29 return all individual individual be instance e. denote set by q e x%29 q e x%29 v x v grammar%23NPR select grammar%23NPR x where q e x%29 class class expression i.e. atomic concept dl q x%29 x where i valid grammar%23NPR
grammar%23NPR class expression thing be slightly more complicated since rdf not support negation model-theoretic semantics owl class expression form grammar%23NPR c c dl syntax where c denote class be ci however learn axiom from rdf dataset open world hypothesis make absence supporting evidence not necessarily contradict axiom moreover axiom hold even face few counterexample therefore as propose define q c x%29 as+follows approximate open world semantics q c x%29 x dc
not exist z dc
c z%29 where z be variable not occur anywhere else query
class expression become q x%29 x dc
not exist z dc z
computational definition support grammar%23NPH grammar%23NPR axiom semantics grammar%23NPR axiom form c v d be ci defend+international defend+international also write x ci x defend+international therefore according+to follow principle selective confirmation grammar%23NPR d k c because if c hold then c d c d d d
result computational definition grammar%23NPR be following grammar%23NPR query select count grammar%23NPR x%29 as u%29 where q c x%29 in+order+to compute score grammar%23NPR axiom grammar%23NPH c v d provide computational definition u grammar%23NPR u grammar%23NPR start following statement confirmation be individual such q c x%29 q d x%29 counterexample be individual such q c x%29 q d x%29
translate into following two grammar%23NPR query compute u grammar%23NPR u grammar%23NPR respectively select count grammar%23NPR x%29 as grammar%23NPR where q c x%29 q d x%29 select count grammar%23NPR x%29 as grammar%23NPR where q c x%29 q d x%29 notice that such q c x%29 q d x%29 not contradict c v d because well be thecase that assertion d just miss likewise i q d x%29 such q c x%29 not treat as confirmation base on our choice regard as evidence favor hypothesis only selective confirmation
be plain implementation resulting scoring scheme
test grammar%23NPR axiom plain version without time cap
axiom form grammar%23NPR c d output n list confirmation list counterexample
compute use query compute u use query if u then query list confirmation if u then compute u use query if u then query list counterexample else u compute n use equation
scalable axiom scoring based on already discuss proposed acceptancerejection index be more accurate than probabilistic score
figure show comparison grammar%23NPH probabilistic score propose grammar%23NPR axiom involve atomic class when test against grammar%23NPR be extension result present
same work suggest use acceptance threshold grammar%23NPH decide whether accept axiom base on possibilistic score while propose accept axiom probabilistic score be greater than notice that both criteria roughly partition score range into three consider as valid axiom score lie upper third range
improvement accuracy possibilistic score however come at very high cost test single axiom against rdf dataset size grammar%23NPR take as little as few millisecond some case but many case even take day on powerful workstation circle represent axiom figure are color use terrain palette base on test time green represent short time rosish brown long+time color scale be logarithmic way speed+up testing badly need lest method propose be practically useless fortunately closer analysis time take test different axiom pinpoint some regularity base this analysis on above-mentioned result altogether staggering 2%2C147%2C483%2C647 ms little less+than day cpu+time spend gather
test grammar%23NPR axiom require execute at+least three grammar%23NPR query query count number counterexample be where most cpu+time spend order magnitude more than other query take as+a+whole reason reside use filter not exist clause q d x%29 graph pattern see
acceptance rejection index b %C3%BC h m n n n d l e h m n n s c o re figure comparison acceptance rejection index probability based score use on axiom test without time capping vertical red+line show acceptance threshold grammar%23NPH horizontal blue line acceptance threshold probabilistic score
approximation open world semantics be critical ingredient our scoring heuristic not dispense
if denote t time take test axiom plot t as function score grammar%23NPH see figure rather striking pattern emerge all axiom have positive grammar%23NPH thus n have very small t n means no counterexample found u some confirmation found u axiom have grammar%23NPH around zero negative t vary widely from figure appear that range this variation increase more grammar%23NPH approach relation between t grammar%23NPH be probably more complex but our purpose describe by say that t o grammar%23NPH perhaps t o exp grammar%23NPH
that as may axiom axiom take too long test likely end+up reject naturally suggest that strategy speed+up test be cap time allow execute query on line reject axiom if test+run out time
question now be how+much time should allow in+order+to be reasonably sure not throw+out baby out bathwater while avoid waste time on hopeless test know relation between score candidate axiom t be little help since not have grammar%23NPH before test however study elapsed+time candidate axiom end+up accept such+that grammar%23NPH observe that time take test grammar%23NPR axiom form c v d tend be proportional product support grammar%23NPR number class have at+least known instance grammar%23NPR acceptance rejection index e la p s e d t im e m figure plot time take+for test systematically generated grammar%23NPR axiom without time capping as function grammar%23NPH vertical red+line show acceptance threshold grammar%23NPH
c. since such product use predict time test valid grammar%23NPR axiom take call time predictor tp c v d grammar%23NPR grammar%23NPR where grammar%23NPR denote number intersect class c
computational definition grammar%23NPR be following grammar%23NPR query select count grammar%23NPR a%29 as nic where q c x%29 x a. where represents atomic class expression
be plot ratio elapsed+time test axiom their time predictor this diagram clearly show that this ratio be very small accepted axiom on right acceptance threshold show as red vertical+line figure while soar large value aris around
time predictor only depend on subclass i.e. left-hand side grammar%23NPR axiom if compute time predictor all class grammar%23NPR sort by increase value their time predictor get diagram show figure y-axis diagram be logarithmic+scale observe that value time predictor increase by more than seven order+of+magnitude as go from most timeconsuming axiom suggest that in+order+to maximize number axiom test begin by less time-consuming axiom number be large leave more time-consuming axiom number be small end
as result define two heuristics scale axiom score dynamically time cap grammar%23NPR query compute grammar%23NPH candidate axiom by time+out grammar%23NPR e e e e acceptance rejection index e la p s e d t im e t o t im e p re d ic r r ti o figure ratio time take test axiom time predictor as function their acceptance rejection index without time capping
vertical red+line show acceptance threshold grammar%23NPH horizontal dashed line correspond slope b coefficient linear+regression time predictor accepted axiom on their test time
as tmax b tp where b be slope regression+line maximum times observe each value time predictor intercept i large enough include below time+out all accepted axiom set axiom test without time cap then interrupt test any take longer than tmax reject be highly likely get negative grammar%23NPH reject anyway this heuristic yield new version axiom test procedure procedure summarize by
construct candidate axiom form c v d by consider subclass c increase order time predictor enable maximize number tested accepted axiom given time+period
evaluation experimental protocol evaluate proposed dynamic time capping heuristics summarize by performing test subsumption axiom use english as reference rdf fact repository in+particular obtain comparability use local dump version along ontology version
local dump consist 812%2C546%2C748 rwanda+defense+force triple be bulk load into prototype perform axiom test use proposed grammar%23NPR 100+200+300+400 e e e e class rank t im e p re d ic r figure plot time predictor all class as function their rank when sort by increase value time predictor
code use access rdf repository
systematically generate test subsumption axiom involve atomic class only accord following protocol class c refer rdf repository construct all axiom form c v d such+that c d share at+least one test axiom increase time predictor order determine dynamic time cap accord use b on basis result previously collect use plain implementation scoring scheme without time cap summarize
experiment perform on celsius workstation equip twelve six core processor at grammar%23ENT clock+speed grammar%23ENT cache each grammar%23ENT ram grammar%23ENT disk+space grammar%23ENT ssd cache ubuntu-world+forum+of+civil+society+networks grammar%23NPR 64-bit operating+system be same machine use both case running+time report refer user system cpu+time give by sec field structure return by system+call getrusage grammar%23NPR as consequence times be commensurable
result thanks+to greatly reduced overhead dynamic time capping heuristics manage test axiom 1%2C232%2C801%2C812 ms little less+than grammar%23ENT half s axiom on+average at time write experiment still run have idea time save let consider that testing that many axiom take 2%2C147%2C483%2C647 ms day without time cap estimate this duration base on experiment describe score axiom without time cap
give 142-fold reduction computing time figure test grammar%23NPR axiom time capped version
axiom form grammar%23NPR c d b coefficient linear time cap equation
n list confirmation list counterexample
compute use query compute nic use query tp nic compute u use query if u then query list confirmation if u then tmax b tp wait+up tmax min do compute u use query if time-out then u u else if u then query list counterexample else u compute n use equation
summary result comparison probabilistic score
be axiom axiom test without time capping outcome test be different on just represent error+rate 3.96%25
take+into+account dramatic improvement terms speed look like very reasonable price pay term accuracy degradation in+addition observe that by construction error be same direction i.e. some axiom axiom accept in+fact reject at+least be conservative heuristic since not generate false+positive moreover axiom supposedly reject by error repeatedly involve set class semantics be not clear e.g. gml feature
further discuss section
validate result our scoring absolute+term take all grammar%23NPR axiom ontology add all grammar%23NPR axiom axiom infer from thus obtain gold+standard axiom be consider as valid tested axiom grammar%23ENT occur gold+standard get grammar%23NPH below yield error+rate about 17%25 in+fact most case grammar%23NPH axiom be around zero zero mean that our heuristic give suspended judgment only axiom have grammar%23NPH below
take latter as real error error+rate fall just 1.78%25 case superclass be dbo grammar%23NPR dbo settlement however be not obvious if draw any conclusion from result
conclusion present possibilistic axiom score heuristic heuristic be viable alternative statistics based heuristics
test by apply problem test grammar%23NPR axiom against database also propose additional heuristic greatly reduce computational overhead consist set grammar%23NPR acceptance rejection index b %C3%BC h m n n n d l e h m n n s c o re figure comparison acceptance rejection index probability based score use on axiom test time capping vertical red+line show acceptance threshold grammar%23NPH horizontal blue line acceptance threshold probabilistic score
on test each axiom
result strongly support validity our hypothesis that be possible alleviate computation grammar%23NPH without loose too+much term accuracy
human evaluation axiom score by system show that axiom accept by+mistake invert grammar%23NPR relation between concepts e.g. dbo case v dbo grammar%23NPR instead dbo grammar%23NPR v dbo case occur when counterexample miss all instance class be instance other class too two axiom positively score other mistake be on axiom involve vague concepts e.g. seem that appear on map type gml feature therefore many class be subclass but be not clear wether be correct not use more general sense thant expect e.g. dbo grammar%23NPR v dbo athlete be not really mistake sense that be several other such concepts involve dbo athlete another example mistake be use concept at+least two sens e.g. dbo library designate both building institution other frequent mistake be on axiom involve concept use as zoological class name taxon therefore mark as subclass dbp species as set animal therefore subclass dbo animal dbo eukaryote same confusion between instance level ontological level explain result on axiom involve skos concept
consideration confirm interest using axiom score heuristics like not only learn axiom from legion+of+doom but also drive validation debugging ontology grammar%23NPR dataset
reference universal owl axiom enrichment large knowledge+base grammar%23NPR grammar%23ENT page springer

ontology language direct semantics second edition world+wide+web+consortium recommendation world+wide+web+consortium grammar%23ENT
fuzzy set probability misunderstanding bridge gap fuzzy set system 143+202
dl foil concept learn description logic grammar%23NPR grammar%23ENT page 107+121 springer
rez
from ontological art towards ontological engineering grammar%23NPR grammar%23NPR

rdf data property axiom grammar%23NPR grammar%23ENT page 718+735 springer
theoretical framework ontology evaluation validation swap

model ontology evaluation validation grammar%23NPR grammar%23ENT page 140+154 springer
learning owl class description on very large knowledge+base
j. semantic+web inf. syst.
grammar%23NPH grammar%23NPH
driven evaluation linked data quality
grammar%23ENT
editor perspective on ontology learning volume studies on semantic+web
grammar%23NPH l. grammar%23NPH grammar%23NPR turn biography future challenge machine+learning
n rez validate ontology oops grammar%23NPR grammar%23ENT page 267+281 springer
selective confirmation raven reply foster grammar%23NPR grammar%23ENT
grammar%23NPH towards integrity constraint owl grammar%23NPR grammar%23ENT ceur-ws.org

evaluation validation editor theory application ontology computer application
springer
grammar%23NPH grammar%23NPH grammar%23NPH
test owl axiom against rdf fact possibilistic approach grammar%23NPR grammar%23ENT page 519+530

fuzzy set as basis theory possibility fuzzy set system grammar%23ENT comparison different strategy automated semantic document annotation ggb%40informatik.unikiel.de c.nishioka%40zbw.eu grammar%23NPH a.scherp%40zbw.eu abstract introduce framework automated semantic document annotation compose four process namely concept extraction concept activation annotation selection evaluation framework use implement compare different annotation strategy motivate by literature concept extraction apply entity detection semantic hierarchical knowledge+base tri- gram rake concept activation compare set statistical hierarchy base graph based method select annotation compare top- k as+well+as grammar%23NPR total define different strategy including novel combination like use graph based activation grammar%23NPR evaluate strategy use three different dataset varying size from three scientific+discipline economics politics computer+science contain 000 manually labeled document total obtain good result on all three dataset by our novel combination entity detection graph based activation e. g. hit degree grammar%23NPR economic political+science dataset good f measure be respectively
computer+science dataset maximum f measure reach experiment be by+far largest on scholarly content annotation annotation typically be up few hundred document dataset only
applied computing document analysis annotation keyword document annotation hierarchical knowledge+base introduction semantic annotation scientific document allow explicit description content subject lay permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT new+york united+states+of+america c grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815838 foundation advanced analysis visualization like topic network traditionally annotation scientific document conduct by domain expert at library public institution for+example in+the+past domain expert economics grammar%23NPR manually label about 1%2C600%2C000 document on average five annotation document take from large semantic thesaurus grammar%23NPR consist several thousand descriptor goal manual label semantic concept be provide good search capability scientific document on search portal like grammar%23NPR however amount digitally published document be tremendously increasing manual annotation become too expensive thus manual document annotation need support by automatically suggested descriptor
research on automated text annotation use variety different natural+language+processing method like grammar%23NPR grammar%23NPR information+retrieval method such as tf israel+defense+forces as+well+as machine+learning approach like grammar%23NPR however typically fairly small number different strategy compare paper at most ten strategy in+addition existing experiment be typically limited dataset consist few hundred document only this paper address challenge by introduce framework automated semantic document annotation compose four process concept extraction concept activation annotation selection evaluation framework be capable integrate various different method propose by existing literature thus different strategy automated semantic document annotation create compare
total different strategy define use our framework some strategy motivate by literature be new respectively extraction concepts from document four method integrate our framework entity detection use semantic knowledge+base n- gram subsequently assign score concepts represent importance specific document compare two statistical method concept activation namely frequency compact+flash israel+defense+forces beyond also evaluate three hierarchy based activation method namely base activation branch activation grammar%23NPR zbw.eu%2Fstw grammar%23NPR www.econbiz.eu%2Fand novel method call grammar%23NPR exploit hierarchical+structure given knowledge+base finally use set graph based method concept degree hit grammar%23NPR base on activated concepts next process select annotation annotation attach document from concepts compare top- k machine+learning approach k nearest+neighbor grammar%23NPR both method be capable continuously integrate new document annotation not require expensive training phase finally quality selected annotation evaluate by compare gold+standard here aim at answer research question strategy overall work best ii concept extraction iii concept activation iv annotation selection method perform best apply standard information+retrieval measure i. e. precision recall f measure evaluate result
apply strategy on three different dataset varying size origin economics dataset document political+science dataset document have gold+standard create by expert third dataset on computer+science document use author reader keyword as gold+standard obtain good result on all three dataset novel strategy combine entity detection graph based activation grammar%23NPR best our knowledge strategy not employ literature before economics political+science dataset f measure be when employ hit as graph based activation computer+science dataset strategy employ entity detection graph based activation method degree grammar%23NPR result f measure slightly outperform variant using hit f measure concept extraction method use entity extraction clearly outperform consider concept activation method graph based method work well when grammar%23NPR employ statistical method perform well when top- k apply
review related work section present our framework employed method in+detail section introduce experiment procedure three dataset evaluation measure result present section discuss section before conclude
related work different field research deal automatic document annotation in+the+past natural+language+processing method include grammar%23NPR grammar%23NPR retrieve keyword keyphrases by analyze text give score in+order+to generate document annotation rise et+al. demonstrate that outperform grammar%23NPR other state+of+the+art method
et+al. experiment tf israel+defense+forces annotate observational data+record field ecology conclude that perform good if sufficient textual content be available
scientific document digital+library annotate semantic entity store domain specific knowledge+base for+instance medical subject+heading mesh knowledge+base field medicine grammar%23NPR www.nlm.nih.gov%2Fmesh use as annotation biomedical article lot domain specific knowledge+base like mesh be hierarchically structured contain synonym provide rich information about relations among entity thus existing works try extract entity annotate document use knowledge+base be various method give score detected entity compact+flash israel+defense+forces extension traditional tf israel+defense+forces take+into+account importance entity entire document corpus beyond method method exploit hierarchical graph structure knowledge+base develop method reveal concept concept not directly mention document but highly related in+addition be graph based method method take+into+account co occurrence concept concept degree number edge use connect other concepts as+well+as well-known web+site assessment method hit grammar%23NPR
select annotation various kind approach employ such as top- k by grammar%23NPH et+al. also machine learning approach employ e. g. naive grammar%23NPR random forest decision+tree learning rank
grammar%23NPR use base on assumption that similar document share similar annotation grammar%23NPH et+al. show that grammar%23NPR significantly outperform other machine+learning technique their experiment author use dataset 000 biomedical article in+addition grammar%23NPR not require expensive training like other machine+learning approach
term evaluation dataset grammar%23NPR evaluate approach publication from agricultural+science publication from medicine publication from physics grammar%23NPR also conduct experiment on scientific dataset contain 000 abstract article field computer+science corresponding title keyword grammar%23NPR et+al. perform automated mesh classification over 000 medical article use their title abstract as input this paper conduct experiment three dataset contain total almost 000 document be by+far largest experiment automated scholarly content annotation
annotation framework our framework automated document grammar%23NPR have four process first framework extract concepts concepts represent feature target document e. concept extraction detected concept get score e. concept activation take+into+account compute score select annotation document e. annotation selection
annotation evaluate by compare gold+standard below first present method implement each process subsequently describe how create different strategy by configure framework
concept extraction this process concepts concepts represent feature target document detect thus concepts be candidate annotation below introduce four concept extraction method implement our framework grammar%23NPR at https%3A%2F%2Fgithub.com%2Fggb%2Fshortstoriesentity entity e. concepts extract from document use domain specific knowledge+base knowledge+base each entity have one more label unique descriptor descriptor enable integrate synonym into one entity base on label entity extract from document like
gram n- gram be contiguous sequence n word from document long use document annotation such as kea experiment set n thus employ tri- gram for+instance be document preprocess text be paper present empirical analysis from text extract phrase paper present empirical analysis paper present present empirical empirical analysis paper present empirical present empirical analysis as concepts thus form union all unigrams bi gram tri- gram
be unsupervised method extract keyword from individual document incorporate word co occurrence information base on degree i. e. connectivity other word frequency word score each word define as ratio degree frequency multi- word phrase score calculate as sum ratio degree frequency each word multi- word
be unsupervised machine+learning technique infer latent topic document corpus topic model infer by each document represent as probability+distribution over topic while each topic again represent as probability+distribution over word treat generated topic as concept use document annotation like score concept i. e. topic define by probability topic experiment set parameter number topic k along grammar%23NPR et+al. number iteration following grammar%23NPR et+al. remove word word appear less+than different document dataset reduce dimensionality computational cost
concept activation compute activation score extract concept by apply three different type activation method statistical method hierarchy based method graph based method hereafter refer score concept c document d by score c d
method statistical method take+into+account only concepts concepts directly mention document introduce following two method
as baseline introduce frequency where scorefreq c d be simply number time c appear d concept extraction method entity tri- gram scorefreq c d be equal original score concept c document d produce by rake see also above scorefreq c d define by probability topic c d
israel+defense+forces compare traditional tf israel+defense+forces use document classification by grammar%23NPR et+al. grammar%23NPR grammar%23NPR count frequency concepts instead term frequency
grammar%23NPR et+al. show compact+flash israel+defense+forces outperform tf israel+defense+forces on news classification task compute as show below scorecfidf c d cf c d log d d d c d where cf c d indicate ratio number concept c d number all concepts d. d refer entire dataset d indicate number document d. d d c d denote number document contain c
based method hierarchy based method assume that concepts extract from document connect each hierarchical+structure exploit hierarchical+structure score concept not directly mention but connect concept boost by following method
activation base spread activation boost score concept high level base on assumption that if document contain concept e. g. apple document be also relevant parent concept e. g. fruit
base activation work as define
c d freq c d c scorebase ci d where freq c d equal number time c appear d cl c denote decay parameter set child concept concept c. set as grammar%23NPR et+al. suggest
activation different from base activation branch spread activation take+into+account number concept each level hierarchy specifically normalize score by number concept high level as describe
c d freq c d bangladesh+navy c scorebranch ci d where bangladesh+navy denote reciprocal number concept that locate on one high level level where c store like base activation set
activation introduce novel grammar%23NPR activation method develop collaboration domain expert from grammar%23NPR initially scoreonehop c d give by freq c d activation process if c have two more child concept concept directly mention d c boost as define where cd denote set concept directly mention document d. thus grammar%23NPR activation be limited activate concepts at maximum one hop distance
c d freq c d c freq ci d if cl c cd freq c d otherwise graph based method while method present above base on hierarchical+structure introduce here method method apply any graph structure this end use co occurrence graph graph represent concept concept be close each other as example consider concept aleph argentine writer short+story poet l. borges aleph short+story aleph poet that extract this order from document from concept make pair aleph argentine writer argentine writer short+story short+story poet poet l. borges l. borges aleph aleph short+story short+story aleph aleph poet along order concept occurrence base on pair co occurrence graph generate as show figure co occurrence graph produce each document subsequently concepts activate use following method
example co occurrence graph
this method take degree e. number edge concept node as score thus scoredegree c d degree c d where degree c d return degree c graph d. for+instance score concept poet figure be three because node have three edge total
hit originally introduce as link analysis algorithm search+engine however also show effectiveness keyword extraction document hit compute two score call hub authority each node i. e. concept graph idea behind hub authority be that good hub represent node that point many other node good authority be node link by many hub initially two score be algorithm traverse graph activate score node by node score hub authority compute by
final score compute as sum two score
scorehits c d hub c d auth c d
c d c auth ci d auth c d c hub ci d grammar%23NPR grammar%23NPR be another algorithm search+engine originally measure importance web+page base on intuition that web+page web+page link from many different web+page be more important
demonstrate effectiveness document classification define below scorepage c d c scorepage ci d grammar%23NPR ci where be parameter called dumping factor set as page et+al. grammar%23NPR c grammar%23NPR c denote set incoming edge c set outgoing edge c respectively
annotation selection this process select annotation document evaluate top- k grammar%23NPR as discuss related work section
k annotation candidate e. concept rank by score compute previous process subsequently concepts that rank above k select experiment k k since document annotate semantic entity as gold+standard see description dataset section concepts e. keyword by tri- gram rake convert into entity by string match entity label lemmatization stopword removal thus only phrase keep appear knowledge+base as entity label
nearest+neighbor grammar%23NPR grammar%23NPR be parametric algorithm use classification use document annotation base on assumption assumption document similar concept vector share similar annotation conduct grammar%23NPR each document convert into numerical vector where each element be score concept nearest+neighbor detect by calculate cosine similarity between two document document obtain annotation annotation be union annotation attach k near k optimize base on f measure
configuration create different annotation strategy by combine method describe previous section however some method not combine certain other method
possible configuration strategy present table when entity employ as concept extraction method be possible apply all method each process
thus have strategy concept activation method %C3%97 annotation selection method entity tri- gram be not possible exploit hierarchy based method because not use any knowledge+base trigram result strategy concept activation method %C3%97 annotation selection method rake grammar%23NPR not use any concept activation method use only frequency furthermore since grammar%23NPR define concept as topic represent by probability+distribution over multiple word be not possible convert topic entity not apply top- k as annotation selection method in+other+words only apply grammar%23NPH when grammar%23NPR employ as concept extraction method thus rake have three strategy grammar%23NPR have one strategy therefore have strategy total
framework easily extend further method at four process thus be possible define new strategy by combine different concept extraction concept activation annotation selection method table configuration strategy all strategy all evaluation measure apply parenthesis show number possible option e g. be two statistical method
extraction entity tri- gram rake grammar%23NPR concept activation statistical hierarchy graph statistical graph frequency frequency annotation selection top- k grammar%23NPH top- k grammar%23NPH top- k grammar%23NPH grammar%23NPH experiment conduct experiment use implemented framework preprocess document by lemmatization stopword removal subsequently partition dataset into equal sized subset eight subset use as training data one test one optimize parameter k case grammar%23NPH below describe dataset use our experiment subsequently introduce our evaluation measure in+addition explain how apply evaluation measure grammar%23NPH setting
dataset conduct experiment three dataset open+access publication manually label in+addition use different knowledge+base each academic domain
dataset contain open+access publication field economics each publication be on+average entity sd by expert from grammar%23NPR economics
annotation come from domain specific hierarchical knowledge+base standard thesaurus economics grammar%23NPR
be domain specific knowledge+base economics develop by grammar%23NPR contain entity label
dataset provide by international+relations area study grammar%23NPR contain publication field political+science on+average publication have entity sd corresponding knowledge+base be european thesaurus on international+relations develop by on international+relations area study contain entity label
use grammar%23NPR grammar%23ENT task grammar%23NPR contain publication originally publication be test publication be training original dataset publication be not entity but on average string base keyword in+order+to enable employ hierarchy based method conduct fair comparison strategy over other dataset convert original keyword based annotation into entity annotation by string match entity label like grammar%23NPH et+al. as knowledge+base field computer+science use association+for+computing+machinery version from grammar%23ENT provided by association+for+computing+machinery association+for+computing+machinery contain grammar%23NPR zbw.eu%2Fstw%2Fversions%2F8.12%2Fabout.en.html grammar%23NPR www.fiv-iblk.de%2Feindex.htm grammar%23NPR www.ireon-portal.eu grammar%23NPR semeval2.fbk.eu%2Fsemeval2.php%3Flocation%3Dtasks%23t6 grammar%23NPR www.acm.org%2Fabout%2Fclass%2Fclass%2F2012 grammar%23NPR www.acm.org entity label result publication dataset be entity sd
evaluation measure evaluate selected annotation use standard precision recall f measure by compare gold+standard precision define by number correct annotation divide by total number annotation make by strategy recall define by number correct annotation divide by total number annotation give by gold+standard f measure be harmonic+mean recall precision selection method grammar%23NPH first create union annotation all k nearest+neighbor before apply measure see section
result report result with+respect+to each concept extraction method as be first process our framework mark result result show good performance bold
entity table show performance document annotation with+respect+to each dataset when entity employ as concept extraction method concept activation method statistical method e. frequency grammar%23NPR outperform when annotation select by top- k however cf israel+defense+forces work much worse than computer+science dataset when annotation select by grammar%23NPH graph based method e. grammar%23NPR degree hit grammar%23NPR perform best especially difference between result graph based method be large economics political+science dataset economics computer+science dataset hit demonstrate good performance term annotation selection method grammar%23NPH perform better than top- k three dataset parameter k grammar%23NPH optimization result k economics political+science dataset k computer+science dataset entity perform best economics dataset f measure
tri- gram table present performance document annotation with+respect+to each dataset same style table but when tri- gram employ as concept extraction method term concept activation method frequency perform best general however concept activation method almost work equally when grammar%23NPH employ annotation selection method top- k perform better than grammar%23NPH economics political+science dataset cf. strategy employ entity as concept extraction method table optimization grammar%23NPH result k economics political+science dataset k computer+science dataset tri- gram perform best computer+science dataset f measure table performance document annotation concept extraction method entity
economics grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency cf israel+defense+forces grammar%23NPR degree hit grammar%23NPR b political+science grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency cf israel+defense+forces grammar%23NPR degree hit grammar%23NPR c computer+science grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency cf israel+defense+forces grammar%23NPR degree hit grammar%23NPR table performance document annotation concept extraction method tri- gram
economics grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency cf israel+defense+forces degree hit grammar%23NPR b political+science grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency cf israel+defense+forces degree hit grammar%23NPR c computer+science grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency cf israel+defense+forces degree hit grammar%23NPR rake table describe performance when rake employ as concept extraction method as discuss section employ only frequency as concept activation method general observe that rake perform worse than entity term annotation selection method rake demonstrate good result when grammar%23NPH be grammar%23NPR performance document annotation concept extraction method rake
economics grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency b political+science grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency c computer+science grammar%23NPR grammar%23NPR grammar%23NPH recall precision f recall precision f recall precision f frequency like entity tri- gram optimization grammar%23NPH result k three dataset rake demonstrate good performance computer+science dataset
grammar%23NPR table describe performance document annotation base on grammar%23NPR as discuss section only apply frequency as concept activation method grammar%23NPH as annotation selection method observe that grammar%23NPR perform general worse than other three concept extraction method strategy demonstrate good performance computer+science dataset three dataset optimization grammar%23NPH result k
annotation performance concept extraction grammar%23NPR annotation selection grammar%23NPH
economics grammar%23NPH recall precision f frequency b political+science grammar%23NPH recall precision f frequency c computer+science grammar%23NPH recall precision f frequency discussion discuss result show previous section along research question outline section
performing strategy look into f measure strategy strategy employ entity as concept extraction method hit as concept activation method grammar%23NPH as annotation selection method perform best economic political+science dataset f measure respectively computer+science dataset strategy strategy employ entity degree as concept activation method grammar%23NPH perform best f measure
state that general our novel combination graph based activation method e. g. hit degree combination entity detection grammar%23NPH outperform other strategy
concept extraction method concept extraction method i. e. entity tri- gram rake grammar%23NPR observe that entity consistently outperform on all dataset thus concepts define domain specific knowledge+base assist document annotation explanation be that concepts avoid detect noisy entity entity be not relevant document such knowledge base be available for+free high+quality many highly adopt library+science relate community
concept activation apply different concept activation method when employ entity trigram as concept extraction method while previous work like et+al. show that hierarchy based method perform well our experiment document that hierarchybased method generally perform lower than graph based method see table possible reason be that different from previous work use full text scientific publication experiment full text already contain so+many different concept that other concept concept not directly mention not activate for+instance detect on average unique entity sd employ entity as concept extraction method economics dataset therefore statistical method work better than hierarchy based method while statistical method perform well when employ topk as annotation selection method graph based method even work good when apply grammar%23NPH as annotation selection method graph based method require computing cooccurence graph over entity thus finally be noteworthy that much simple grammar%23NPR activation introduce this paper already achieve very similar result terms f measure but at much low computational cost
annotation selection our experiment compare three annotation selection method grammar%23NPR grammar%23NPR grammar%23NPH state that general grammar%23NPH enhance performance except one strategy use frequency where result be overall very low believe that our result be generalized other domain be similar characteristic like social+science information+science psychology in+particular if gold+standard annotation be similar style thesaurus be available base this assumption on large-scale dataset grammar%23NPR list maintain at world+wide+web+consortium www.w3.org%2F2001 sw wiki grammar%23NPR grammar%23NPR content use experiment almost two order+of+magnitude larger than any result document so+far make result robust stable in+addition b result obtain field very different from our domain namely on 000 article biomedicine 000 article medicine also determine grammar%23NPH as good performing selection method however both study not employ graph based activation method like this paper show even high improvement grammar%23NPH
conclusion perform extensive empirical analysis different strategy automated document annotation use three dataset different size scientific offspring economics politics computer+science total almost 000 manually labeled document find out that on all three dataset novel combination graph based concept activation method grammar%23NPH as concept selection method make good document annotation terms f measure overall good result obtain strategy combine entity hit as graph based concept activation grammar%23NPH f measure economic dataset political+science dataset computer+science dataset strategy employ entity degree as graph based concept activation method grammar%23NPH perform slightly good f measure than use hit grammar%23NPR
thank grammar%23NPH grammar%23NPH from grammar%23NPR invaluable discussion feedback economics dataset also thank grammar%23NPH from from political+science dataset finally thank grammar%23NPH support analyze economics dataset
reference extraction professional interest from social profile augment user model real+world experience enhance personalization adaptation page springer
grammar%23NPH pattern+recognition machine+learning springer
dynamic topic model
grammar%23NPR page 113+120 association+for+computing+machinery
grammar%23NPH grammar%23NPH latent dirichlet allocation grammar%23NPR
combine labeled unlabeled data co training association+for+computing+machinery
linguistically motivate feature enhanced indexing page 932+940
grammar%23NPH news personalization use compact+flash israel+defense+forces semantic recommender grammar%23NPR association+for+computing+machinery
find scientific topic grammar%23NPR suppl
grammar%23NPH lting
process extract user+profile from social+media use hierarchical knowledge+base grammar%23NPR page 197+200 institute+of+electrical+and+electronics+engineers
recommend mesh term annotate biomedical article
660+667
combine machine+learning natural+language+processing automatic keyword extraction
thesis
user interest identification on twitter+inc. use hierarchical knowledge+base grammar%23NPR page 99+113 springer
authoritative source hyperlinked environment journal association+for+computing+machinery 604+632
human competitive automatic topic indexing phd thesis

competitive tagging use automatic keyphrase extraction grammar%23NPR page grammar%23ENT grammar%23ENT

grammar%23NPR bringing order into text grammar%23NPR

automatic indexing by co-occurrence graph base on building construction metaphor anti-defamation+league page institute+of+electrical+and+electronics+engineers
grammar%23NPH grammar%23NPR citation ranking bringing order technical report grammar%23NPH

keyword extraction from individual document text+mining theory application page grammar%23NPH son
term weighting approach automatic text retrieval information+processing management 513+523
grammar%23NPH grammar%23NPH v. lee mesh up effective mesh text classification improved document+retrieval bioinformatics

tag recommendation metadata annotation use probabilistic topic modeling grammar%23NPR page 239+248 association+for+computing+machinery

health related knowledge social+media use ensemble heterogeneous feature grammar%23NPR page grammar%23ENT grammar%23ENT association+for+computing+machinery
how preprocess affect unsupervised keyphrase extraction grammar%23NPR page 163+176 springer
grammar%23NPH grammar%23NPH grammar%23NPH kea practical automatic keyphrase extraction dl association+for+computing+machinery
vote theory concept detection grammar%23NPR page 315+329

vote theory concept detection grammar%23NPR springer grammar%23ENT taxonomy semantic+web data+retrieval anila.butt%40anu.edu.au armin.haller%40anu.edu.au lexing.xie%40anu.edu.au provide access increasing amount structured information wide variety domain information+overload due large amount structured data be as+much problem as on traditional solve this problem ample research propose on semantic+web data+retrieval technique more than decade research this domain be now reasonable consider question be field semantic+web data+retrieval make progress be direction direction take be promising significant direction pursue future research answer question review state-of-the-art semantic+web data+retrieval technique define taxonomy technique classify ongoing research find potential future research direction
taxonomy semantic+web search data+retrieval approach introduction increasing popularity semantic+web be continuous growth amount publicly available owl rwanda+defense+force s dataset on problem find information this huge amount data rapidly become as challenge problem as information+retrieval on traditional problem be at+least mitigate by fact that meaningful actionable information user query theoretically retrieve by exploit inherent nature semantic+web data however regard retrieval technique be wide range work originate different community available claim some sort relevance semantic+web data+retrieval for+example terms encounter literature claim be relevant semantic+web data+retrieval include ontology search link data+retrieval entity search sub graph match etc. give this diversity be difficult permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT palisade new+york+state united+states+of+america
association+for+computing+machinery isbn grammar%23ENT grammar%23ENT dx.doi.org%2F10.1145%2F2815833.2815846
identify problem area compare solution part issue be lack comprehensive survey standard terminology hide assumption undisclosed technical detail dearth evaluation metrics
paper aim address gap contribute survey prominent historical state-of-theart technique semantic+web data+retrieval grammar%23NPR develop taxonomy grammar%23NPR technique consist dimension group into five topic retrieval aspect storage search approach ranking evaluation practical aspect five topic consist three more dimension discuss dimension provide brief review on how existing grammar%23NPR technique fit into our taxonomy base on our review be able identify gap existing technique allow highlight future research direction
rest this paper be structured as+follows sec. present general semantic+web data+retrieval process provide reader necessary background require understand our taxonomy grammar%23NPR technique
describe dimension identify that allow characterize grammar%23NPR technique describe how some prominent grammar%23NPR technique fit into our taxonomy summarize characteristic survey technique table then discuss direction future research sec. conclude paper sec.
semantic+web data+retrieval process data+retrieval on be complex process consist several steps common semantic+web data+retrieval framework be similar typical search process as show fig. here box denote component data+retrieval process line indicate data+flow among process processes concern pre processing phase i.e. data+acquisition warehouse indexing reasoning while remaining processes concern query time processing phase i.e. query evaluation ranking be part pre processing query time processing phase depend on approach
first step data+acquisition i.e. data crawl parse be crucial retrieval approach because quality any retrieval system depend on quality underlying dataset data+acquisition necessitate web+crawler more specifically structured data crawler semantic+web data crawl purpose crawler be gather collection linked data as quickly outline general semantic+web retrieval process as possible while provide at+least required feature respect limitation impose by publisher i.e. politeness robustness large+number linked data crawler propose including crawler mostly gather data from by traverse linked graph moreover some crawler also clean data syntactically output data+acquisition process materialize further processing
graph based nature rwanda+defense+force s data necessitate special data+structure data+storage semantic+web community propose variety storage structure as discuss sec. some grammar%23NPR approach also infer implicit data triple from data before materialize infer logical+consequence from set asserted fact axiom special purpose reasoner design reuse by community research this area reasoning conduct separately but not context as part grammar%23NPR approach therefore reasoning approach not cover this work however be existing benchmark study study compare feature performance different available reasoner
large semantic+web data+collection find match structured query keyword query require comparison be neither feasible necessary because+of infeasible query response+time large+number matching triple data+collection respectively indexing technique require mitigate this problem single word uri combination uri commonly call key use decide where find insert data on disk as traditional information+retrieval technique indexing have trade-off between computational complexity quality matching result many small but more specific key index more filter result smaller candidate result set thus reduce computational cost but at+the+same+time be more likely that some possible partial match miss on+the+other+hand less specific key result lager candidate result set but likely more exact match
technique indexing link data develop analysis on indexing base on structure content key present sec.
providing information response user query in+real+time through indexing some retrieval approach also provide ranking result ranking try determine result be most appropriate query substantial amount ranking mode design adopt semantic+web data ranking as discuss some grammar%23NPR technique rank data data+collection i.e. corpus off-line independent user query materialize rank along indexing retrieve result query apply ranking model rank result only once indexing ranking finish semantic+web data become available retrieval like search+engine grammar%23NPR technique allow user explore linked data through keyword query but also through structural query model where user pose additional more complex navigational query this purpose user+interface provide mean user specify complex query user pose their query through interface query mostly evaluate bottom-up i.e. first match content resource found help index then other information use filter result selection through real-time query
taxonomy semantic+web data+retrieval technique this section describe taxonomy grammar%23NPR technique our aim develop this taxonomy be provide clearer picture current approach retrieve semantic+web data identify gap technique help identify future research direction describe dimension technique categorize into five main topic as illustrate fig. following section discuss each dimension in+detail while also provide overview methodology technique apply dimension
retrieval aspect all existing grammar%23NPR approach categorize into three major dimensions with+respect+to retrieval design decision type s data explore approach way s user initiate retrieval process type s output as result user query
retrieval scope grammar%23NPR technique classify into explore schema define by ontology describe conceptualization domain interest explore data generate accord schema former refer as ontology retrieval technique latter as technique linked-dataretrieval technique focus on retrieval entity relationship among entity sub- graph while ontology retrieval technique find class properties within ontology ontology type approach focus on different component retrieval process large size linked data available require retrieval technique mainly focus on efficient indexing query evaluation plan on+the+contrary dataset dataset only consist ontology be relatively small thus ranking result be more relevant retrieval process than indexing efficient query plan execution graph retrieval technique be category grammar%23NPR technique comprise approach propose general graph based data but be also applicable and%2For test on semantic+web data+retrieval task
query model grammar%23NPR technique generally consider one more out four query model keyword search structured query search facet browse hyperlink based navigation keyword based grammar%23NPR technique user pose grammar%23NPR technique retrieval aspect storage search ranking evaluation practical aspect implementation j p c c dataset efficiency query execution+time q index construct time ic index update time iu effectiveness scalability ranking scope ranking factor ranking domain semantic+web s graph database g document+retrieval d machine+learning m data+acquisition manual collection m html agnostic crawler ag html aware crawler foreign+intelligence+agency focussed crawler f indexing query match exact match e partial match p scope ontology o linked data l graph structure data g query model relation centric r entity centric e document centric d no index n full text+index f structural index s graph index g multi-level indexing m no ranking n popularity p authority informativeness relatedness r coverage co centrality c learned model l feedback f recall r precision p f-measure f map m grammar%23NPR n synthetic s real r keyword search k structured query search s grammar%23NPR browse f hyperlink navigation h global g focus f data+storage relational+database r native storage n grammar%23NPR database no user+interface gui g api figure dimension use characterize semantic+web retrieval technique string compose one more keyword while result retrieve base on match one more keyword query string structured query search introduce complexity while provide more flexibility meet user requirement by retrieve result user specify pattern grammar%23NPR technique provide simply endpoint query data through grammar%23NPR structured+query+language graph query few technique allow user find filter result base on faceted browsing approach each facet be characteristic i.e. property facet values be object values that characteristic facet be fixed irrespective+of search result as define by ui developer generate dynamically base on characteristic search result hyperlink based technique facilitate user navigate within data each hyperlink be query execute when user click on keyword search facet browse hyperlink based navigation facilitate user explore data whereas structured query interface be expert user need know syntax query+language underlying schema data
result examined grammar%23NPR approach mostly consider three different output type facilitate user exploration data document centric approach list uri label matched document i.e. ontology and%2For document part i.e. class property entity
document centric approach list uri same ontology resp. resource multiple times contain different piece information about ontology resp. resource
ii entity centric approach consolidate available data about entity from multiple document consolidated information present as profile entity therefore rather than list match document as incomplete piece information entity centric search output one more matched entity their available profile dataset iii relation centric approach find relationship between entity mostly structured query facet browsing help perform relation centric retrieval
storage search approach data+acquisition quality retrieval system depend on quality underlying dataset data+collection mostly do two way manual collection admin owner collect dataset manually consider requirement scope designed approach link data crawler application application gather collection linked data as quickly efficiently as possible existing linked data crawler divide into three category base on their crawling approach html agnostic crawler not crawl html+document therefore crawler be not able discover linked data embed html+document rdf document surround by html+document ii html aware crawler crawl both rdf html+document follow rwanda+defense+force html link within however when crawl crawler visit many html+document html+document no embed linked data not point any rdf document iii focused crawler use limited html crawl approach in+order+to control efficiency html crawling crawler crawl both rdf html+document but limit crawling space html+document for+example crawl only html+document html+document explicitly provide as endpoint by user extract embed linked data href link rdf extension within grammar%23NPR technique generally consider three storage structure native storage grammar%23NPR approach deploy persistent+storage their own designed storage architecture generally consider be more efficient than one rely on relational+database grammar%23NPR database grammar%23NPR technique use grammar%23NPR database increase processing power storage grammar%23NPR one most widely used grammar%23NPR database use for+example grammar%23NPR relational+database grammar%23NPR approach employ tradtional relational+database+management+system such as grammar%23NPR store triple quad semantic+web data store vertical representation big triple table quad table horizontal representation property table vertical partitioning this storage approach mainly adopt by approach early+days semantic+web but due slow response+time it no+longer choice
indexing various technique indexing link data develop since advent semantic+web several survey technique present this work divide rdf data indexing into four major category grammar%23NPR technique implement one more type four index
text+index implement as inverted index compose lexicon i.e. dictionary term allow fast term lookup set inverted lists one inverted+list term however compare traditional documentbased invert index difference be structure inverted lists base on structural difference inverted+list full text index semantic+web further divide into node based full text index graph based fulltext index node based index resp. graph based index inverted lists compose list resource node identifier resp. ontology identifier each terms lexicon improve space time complexity full text index some grammar%23NPR approach separate node based full text index entity attribute object values into entity node invert index attribute node invert index value node invert index
index specially design rwanda+defense+force data+store such index classify into that index triple subject predicate object that index quadruple context subject predicate object former know as triple index latter as quad index contrast separate index on subject predicate object and%2For context where join operation require derive answer query complete index on quad triple pattern allow direct lookup on multiple dimensions without join operation make search more efficient index all possible pattern quadruple triple implement i.e. index quadruple triple respectively
index recently graph index introduce support efficient structural query over graph rdf data compare traditional index where each node have key value pair index difference be content structure key value pair graph index traditionally key index node be either text identifier graph index key be subgraph pattern value be set database graph ontology contain subgraph data+structure adopt implement graph index enhance filtering include feature matrices graph lattice
index other+than create multiple type index grammar%23NPR technique also introduce multi-level index improve efficiency retrieval process one such approach present index at different level narrow down search space by reduce size relevant dataset query
query match efficiency effectiveness query evaluation heavily influence by how match found data+collection matched result query repository find either exact match partial approximate match exact match be effective since exact keyword structure query match always ensure right answer user however sometimes result empty result set if either exact match be unavailable user be unaware content structure dataset on+the+other+hand partial match enhance chance come+up approximate similar result user but disadvantage potentially large+number result need order mechanism suggest most appropriate result user
ranking in+addition+to provide information response user query some retrieval approach rank result ranking indicate result entity ontology deem be most appropriate query ranking model design adapt semantic+web data ranking be distinguished along several dimension discuss this section
ranking scope ranking scope denote if grammar%23NPR technique be query dependent not query dependent approach refer as focussed ranking i.e. ranking model apply only on result set relative order each result result set compute second+class ranking approach refer as global ranking implement on complete dataset ontology link data irrespective+of query since focussed ranking approach apply only on subset result dataset lead high efficiency compute rank however rank calculate be not global optimum global ranking be more time consume but compute globally optimum ranking score query result
ranking factor one other important dimension ranking be ranking factor base on rank calculate factor factor use different ranking approach explain here popularity similar document+retrieval domain ranking technique adopt semantic+web data order output user query in- term popularity result dataset different grammar%23NPR technique adopt different popularity measure model originally design information+retrieval grammar%23NPR tf israel+defense+forces be most widely used popularity measure semantic+web data rank
authority measure trustworthiness be another factor on basis individual resource document ontology rank hit design informational retrieval use compute authority resource variation hit also investigate
semantic+web data informativeness be measure degree information carry by each resource help identify several grammar%23NPR technique adopt grammar%23NPH entropy as informativeness measure according+to informativeness resource be negative log probability presence resource given dataset
relatedness be similarity between feature property value pair resource resource rank high if feature resource relate each other different relatedness model propose such as wordnet measure relatedness between two feature base on their text similarity distributional relatedness i.e. two feature be more related if more often co-occur certain graph ontology
coverage be query dependent ranking factor measure how+much query term structured query cover by resource grammar%23NPR be document+retrieval model compare similarity between query matched document model adapt on task resource ontology ranking
model other approach ranking semantic+web data root learn rank technique develop machine+learning approach different graph ontology feature select compute on basis feature ranking model learn then learned model use produce ranking search result
some ranking model design adopt by grammar%23NPR approach consider centrality concept resource compute their rank some approach find centrality as connectivity node resource graph ontology mostly be measure number relation edge concept node
user feedback some grammar%23NPR technique consider user feedback such as view count query log compute ranking result set
ranking domain grammar%23NPR technique either design purely semantic+web data borrow from other domain approach adopt from document+retrieval domain including grammar%23NPR hit grammar%23NPR tf israel+defense+forces grammar%23NPR
graph structure rdf model grammar%23NPR technique adopt ranking approach approach design graph general i.e. shortest path centrality measure recent trend ranking semantic+web data be adaptation approach from machine+learning domain however model be not applicable semantic+web data original form because+of nature data therefore variant model implement study
evaluation performance grammar%23NPR technique need evaluate terms three factor efficiency effectiveness scalability efficiency grammar%23NPR approach provide measure how fast retrieval process be while effectiveness approach measure by accuracy retrieval model quality result scalability measure grammar%23NPR technique capability handle large scale dataset complex query
efficiency efficiency evaluate use measure measure be dependent upon resource utilization on computing platform i.e.
consumption measure that base on time take retrieve relevant result existing approach evaluate time take on different process retrieval process including query evaluation time index construction time index updation time
effectiveness one more out five popular metrics use evaluate effectiveness grammar%23NPR approach
fraction relevant document retrieve i.e.
relevant result retrieve relevant result retrieve relevant precision fraction result be relevant precision relevant result retrieve retrieve result relevant retrieve be hard determine relevance irrelevance all result query result large number matched result therefore mostly precision determine cut off value i.e. top- k result precision at k p%40k k value calculate as p%40k relevant result top k result k f grammar%23NPR f measure be measure trade off precision versus recall be weighted harmonic+mean precision recall i.e. f measure precision recall precision recall mean average precision average precision query q grammar%23NPR technique define as associated+press q k rel ri p%40i k where rel ri be if ri be relevant resource query q otherwise p%40i be precision at k be cut off value map define as mean ap over all query run experiment calculate as table categorization retrieval technique search aspect storage search ranking evaluation practical aspect technique s h s co p e q u er y m o d el r e u lt t y p e d ta q u be io n d ta s ra g e d ex g q u er y m h r n k g s co p e r n k g f ct o r r n k g d o m e ffi ci en cy e ff ec ti v en e s s can la b il y grammar%23NPR p le m en ta ti o n d ta se t u se r te rf ce grammar%23NPR o k s d m n f e g p d j r g grammar%23NPR o k f d m e g f d j r g grammar%23NPR o k co grammar%23NPR o k f d m ag d r g grammar%23NPR o grammar%23NPR o l k s d ag n s f e n d j r g grammar%23NPR o k d ag r f g p d f r g grammar%23NPR o k d grammar%23NPR c g d p grammar%23NPR l k d f no f p f co d q c u d j r g grammar%23NPR l k e no f p f co d q j r g grammar%23NPR l k e ag n s p f p d q c grammar%23NPR g k d m n s f p f c m q p m n d j r s grammar%23NPR g s d m n g p q c p m n d r s grammar%23NPR g s d m n m g p q c u ds r s grammar%23NPR g s d m n g p q c u p r f d r blink g k d m n m p f c g q c ds r grammar%23NPR g s d m n g p q q d r s represent undefined unknown dimension approach map grammar%23NPR associated+press q q
be standard evaluation measure ranking task binary relevance judgement define base on gain vector g be vector contain relevance judgement at each rank then discounted cumulative gain measure overall gain obtain by reach rank k put more weight at top ranking
q grammar%23NPR grammar%23NPR
compute by divide grammar%23NPR by optimal value grammar%23NPR put most relevant result first grammar%23NPR calculate by compute optimal gain vector ideal ordering
scalability grammar%23NPR technique evaluate use measure measure be dependent on size no. triple structural complexity dataset query and%2For on flexibility approach former refer as space scalability latter refer as structural scalability evaluation conduct compute resource utilization including memory time scalable approach resource utilization not grow intolerable level as size complexity data+set query increase metrics scalability characterise as data size ii data complexity iii query size iv query complexity
practical aspect final category cover practical aspect grammar%23NPR technique including type dataset use implementation experimental evaluation how solution implement if proposed solution develop specific application area mind
implementation this dimension specify implementation technique technique use implement prototype grammar%23NPR technique in+order+to conduct experimental evaluation
solution propose literature provide only theoretical proof but not evaluate experimentally no detail about their implementation publish
dataset experimental evaluation on one ideally several dataset be important critical evaluation grammar%23NPR technique
difficulty obtain real-world data data contain large+number triple synthetically generate dataset commonly use
user+interface grammar%23NPR approach develop primarily interactive browse while additionally provide programmatic access content browse graphical+user+interface mostly form use make more interactive experience user while programmatic access make available through web+service enable application+developer use content grammar%23NPR technique their application
discussion research direction this section analyze grammar%23NPR technique as characterize table with+regard+to proposed taxonomy this analysis highlight several area potential future research direction grammar%23NPR since beginning development technique aim provide solution grammar%23NPR be clear path progress start from early technique solve problem semantic+web data+retrieval exact keyword document search use approach move+on entity search technique allow advanced faceted browsing still be some research gap gap focus on in+the+future
facet browsing ontology search+engine library either not facilitate facet browse at+all filter result base on fixed facet all search e.g. grammar%23NPR grammar%23NPR more satisfying approach seem lie find facet dynamically base on matched result query however major hurdle identify dynamic facet be syntactic diversity describe same property for+example title resource describe as name title label etc. different vocabulary potential solution be cluster similar type property into single group use machine+learning data+mining technique declare group property as facet rather than have individual properties as facet
retrieval ontology search system retrieve ontological terms concept relation provide ontology search base on some keyword ontology search system system retrieve matched ontology multi keyword query often return ontology ontology match query term however lack criteria find relevant ontology ontology cover query term related concepts terms grammar%23NPR provide opportunity find ontology base on text description however be domain dependent ontology library not deal all type ontology general solution ontology retrieval base on text description several keyword still need devise
ranking model ontology collection limit size therefore ranking become core task ontology search+engine library rather than efficient search however ontology ranking be pragmatic because search result be match search term more expressive class property ontology description there exist many ontology ontology contain concept relation their label match keyword query however describe differently mainly terms their perspective concept define different perspective e.g. person class define many ontology for+example foaf ontology capture social aspect person whereas appearance ontology model natural attribute person i.e. weight height nature ii level detail concept define same perspective different ontology but different level detail i.e. abstract detailed iii extension concepts define one ontology then extend another ontology problem be how find order many matched result keyword search satisfy user information need ontology retrieval system not focus on ranking at+all adopt ranking approach that root graph document+retrieval ranking model without consider underlying nature ontology provide ample opportunity research significantly improve ranking ontology ontological terms base on more expressive user query
data+retrieval effectiveness vs. efficiency link data+retrieval approach classify into two major category effectiveness orient technique apply ranking model retrieve most appropriate answer ii efficiency orient technique mainly focus on efficient indexing achieve efficiency retrieve result less focus on ranking be scope linked data+retrieval technique make+off reasonable trade off between effectiveness efficiency retrieval approach
triple entity retrieval recent year linked data+retrieval paradigm shift from document+retrieval entity retrieval entity retrieval process find entity consolidate attribute entity from multiple data+source require ranking triple entity prioritize relevant attribute that entity existing approach rank properties general context base on their occurrence dataset however ranking property depend upon entity belong property attach more than one entity relative importance property vary each entity secondly object value multivalued property mostly have different ranking criteria depend upon entity property belong but also rank accord popularity current approach constitute significant gap between state-of-the-art entity ranking technique ideal ranking present opportunity future research
evaluation framework semantic+web data+retrieval technique be currently no comprehensive evaluation strategy strategy facilitate comparative evaluation different grammar%23NPR technique regard their effectiveness efficiency scalability researcher use variety evaluation measure dataset both real synthetic make compare existing technique difficult be currently not possible determine technique s perform better than on data different characteristic different size so+far seem that no single grammar%23NPR technique outperform all other technique all aspect on large benchmark on ontology ranking publish recently contribute ontology collection ten benchmark query gold+standard evaluation eight ranking model on task ontology search however benchmark deal ontology concept ranking only be no comprehensive study study compare many existing technique within same framework on different dataset conduct such large experimental study be one avenue research be highly beneficial better understand characteristic technique
conclusion this paper present brief overview historical current state-of-the-art technique semantic+web data+retrieval identify dimensions dimensions allow characterize technique generate taxonomy such technique propose taxonomy use as comparison analysis tool semantic+web data+retrieval technique through this taxonomy identify various shortcoming current approach suggest several future research direction this field
reference ranking ontology aktiverank grammar%23NPR page springer
grammar%23NPR ranking complex relationship search result on semantic+web grammar%23NPR 14th international conference on page 117+127 association+for+computing+machinery
grammar%23NPH p. grammar%23NPH
grammar%23NPR intelligent crawler semantic+web application 524+530
grammar%23NPR owl reasoner grammar%23NPR
grammar%23NPR dynamic ontology library support ontology selection grammar%23NPR grammar%23NPR
ontology search empirical evaluation grammar%23NPR page 130+147 springer
relationship based top- k concept retrieval ontology search knowledge engineering knowledge+management page 485+502 springer
grammar%23NPR learning concept ranking ontology search semantic+web journal
scalability performance evaluation semantic+web database grammar%23ENT grammar%23ENT
grammar%23NPH relatedness informativeness based centrality entity summarization grammar%23NPR page 114+129 springer
grammar%23NPH association between entity via top- k ontological pattern facet grammar%23NPR page 422+437 springer
grammar%23NPH
independent learning rank rdf entity search research application page 484+498 springer
grammar%23NPR more than semantic+web search+engine semantic+web
where publish find ontology survey ontology library semantics science service agent on
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
search metadata engine semantic+web grammar%23NPR thirteenth acm-cikm page 652+659 association+for+computing+machinery
set measure centrality base on betweenness sociometry page
compute shortest path search meet graph+theory grammar%23NPR
sixteenth annual acm-siam symposium on discrete algorithm page 156+165
v. grammar%23NPR racer owl reasoning agent semantic+web grammar%23NPR international on application product service web-based conjunction page

grammar%23NPR federated repository query graph structure data from grammar%23NPR 6th international semantic+web 2nd asian conference on asian semantic web+conference page 211+224 springer grammar%23NPR
blink rank keyword search on graph grammar%23NPR grammar%23ENT association+for+computing+machinery grammar%23NPR international conference on management data page 305+316 association+for+computing+machinery
grammar%23NPR scalable ranking method semantic+web data context
search browse link data swse semantic+web search+engine semantics science service agent on 365+401
search browse linked data grammar%23NPR search+engine semantics science service agent on 365+401

open source crawl framework linked data grammar%23NPR

grammar%23NPH grammar%23NPR fast graph search label similarity grammar%23NPR vldb endowment 181+192
authoritative source hyperlinked environment journal association+for+computing+machinery grammar%23NPR 604+632
grammar%23NPH grammar%23NPH grammar%23NPH store index massive rdf dataset semantic search over page springer
grammar%23NPH grammar%23NPR sure semantic portal-the seal approach spin semantic+web page 317+359
focus crawl structured data grammar%23NPR 23rd acm-cikm page association+for+computing+machinery
linked data informativeness application page 629+637 springer
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH a
et+al. grammar%23NPR ontology integrated data resource at click mouse
research w170-w173
grammar%23NPR com document oriented lookup index open linked data
semantics ontology
extend facet navigation rdf data grammar%23NPR grammar%23ENT page 559+572 springer
pagerank citation ranking bring order

grammar%23NPH grammar%23NPR semantic+web portal ontology searching ranking classification grammar%23NPR 5th association+for+computing+machinery international workshop on information+management data+management page association+for+computing+machinery
grammar%23NPH grammar%23NPH grammar%23NPH et+al. okapi at grammar%23NPR national+institute+of+standards+and+technology special publication sp page 109+109
term weighting approach automatic text retrieval information+processing management 513+523
c. s yang vector+space model automatic indexing communications association+for+computing+machinery 613+620
mathematical+theory communication association+for+computing+machinery grammar%23NPR mobile+computing
grammar%23NPH grammar%23NPH pellet practical owl dl reasoner semantics science service agent on
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al. foundry coordinated evolution ontology support biomedical data+integration nature biotechnology grammar%23ENT grammar%23ENT
grammar%23NPR searching ontology semantically grammar%23NPR grammar%23NPR grammar%23ENT workshop on owl experience direction volume
learn rank information+retrieval 359+381
sig. ma live view on data semantics science service agent on 355+364
p. y grammar%23NPR link open vocabulary european+research+consortium+for+informatics+and+mathematics news
ontology based subgraph query grammar%23NPR 29th international conference on data engineering page 697+708 institute+of+electrical+and+electronics+engineers
substructure similarity search graph database grammar%23NPR grammar%23ENT association+for+computing+machinery grammar%23NPR international conference on management data page 766+777 association+for+computing+machinery
grammar%23NPR structureless graph query grammar%23NPR vldb endowment
grammar%23NPR lattice based index graph database vldb journal 229+252 grammar%23ENT grammar%23NPR grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
grammar%23NPH grammar%23NPR real-time classification entity text proceeding grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH learning spotting named entity on intersection nerd machine+learning proceeding grammar%23NPR grammar%23NPR concept extraction challenge
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH media+exchange vocabulary lightweight interchange format machine+learning experiment proceeding 11th international conference on association+for+computing+machinery 169+176
grammar%23NPH grammar%23NPH fast accurate annotation short text institute+of+electrical+and+electronics+engineers software grammar%23ENT
grammar%23NPH sander grammar%23NPR generate flexible proper+name reference text data model evaluation grammar%23NPR
vol.
grammar%23NPH grammar%23NPH grammar%23NPH capture semantic similarity entity link convolutional neural grammar%23NPR preprint grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH
entity disambiguation resource poor language proceeding eighth workshop on exploit semantic annotation information+retrieval

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH probabilistic entity linking proceeding 25th international conference grammar%23NPR wide international republic canton 927+938 https%3A%2F%2Fdoi.org%2F10.1145%2F2872427
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH real-time rdf extraction from unstructured data+stream grammar%23NPR
grammar%23NPH collective entity link text graph based method proceeding 34th international association+for+computing+machinery grammar%23NPR conference on research+and+development information+retrieval association+for+computing+machinery 765+774
grammar%23NPH grammar%23NPH grammar%23NPR emerging entity ambiguous name proceedings 23rd on new+york new+york united+states+of+america 385+396
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
robust disambiguation named entity text conference on empirical method natural+language+processing
grammar%23NPH authoritative source hyperlinked environment j.
grammar%23ENT 604+632
computational generation refer expression survey computational+linguistics grammar%23ENT 173+218
grammar%23NPH
collective annotation entity text 15th grammar%23NPR 457+466
grammar%23NPH grammar%23NPH design challenge entity linking transactions vol.

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
proceedings 41st annual meeting on 152+159
gang luo chin- yew grammar%23NPH joint named entity recognition disambiguation grammar%23NPR grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH
spotlight shedding light on document 7th on semantics
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
representation word phrase their compositionality advance neural information+processing+system
grammar%23NPH grammar%23NPH learn link 17th grammar%23NPR 509+518
grammar%23NPH grammar%23NPH grammar%23NPH entity linking meet word+sense disambiguation unified approach grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPR automatic construction evaluation application artificial+intelligence grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH open knowledge extraction challenge semantic+web evaluation challenge communications computer information+science vol.
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR citation ranking bringing order
grammar%23NPH grammar%23NPH grammar%23NPH
lightweight multilingual entity extraction linking proceedings tenth on search data+mining
grammar%23NPH grammar%23NPH from new entity annotator proceeding first international workshop on entity recognition disambiguation
grammar%23NPH et+al. grammar%23ENT use tf-idf determine word relevance document query proceedings first instructional conference on machine+learning
grammar%23NPH local global algorithm disambiguation cogcomp.cs.illinois
paper grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
baseline approach type extraction first open knowledge extraction challenge at 12th grammar%23ENT
towards language independent named entity linking proceedings 54th annual meeting grammar%23ENT grammar%23ENT grammar%23ENT volume long paper
grammar%23NPH text approach application challenge computing survey grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH semantic multimedia information+retrieval base on contextual description semantic+web semantics big data grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR lecture note computer+science vol. springer grammar%23NPH 382+396
grammar%23NPH sang introduction language independent named entity recognition proceeding grammar%23ENT
grammar%23NPH grammar%23NPH enrich knowledge source use maximum entropy part+of+speech tagger proceedings grammar%23ENT joint grammar%23NPR conference on empirical method natural+language+processing very large corpus hold conjunction 38th annual meeting
grammar%23NPH cross lingual wikification use multilingual embeddings proceeding naacl-hlt 589+598
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR disambiguation named entity using linked data semantic+web grammar%23NPR 13th international semantic+web conference grammar%23ENT proceeding part 457+471
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH gerbil general entity annotator grammar%23NPR framework proceedings 24th on grammar%23ENT grammar%23ENT

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH entity link analysis current benchmark dataset roadmap do good job 10th on language resource evaluation grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH compare apple orange gerbil fine grained grammar%23NPR evaluation proceedings 12th on semantics grammar%23ENT new+york new+york united+states+of+america
han grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPH ma grammar%23NPH grammar%23ENT language domain independent entity linking collective validation proceedings grammar%23ENT conference on empirical method natural+language+processing 695+704
grammar%23NPH grammar%23NPR grammar%23NPR cross lingual semantic annotation
grammar%23ENT grammar%23ENT grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH context aware entity disambiguation text using markov+chain institute+of+electrical+and+electronics+engineers grammar%23NPR on grammar%23NPR united+states+of+america
grammar%23NPH grammar%23NPH grammar%23NPH doser knowledge-base-agnostic framework entity disambiguation use semantic embeddings semantic+web latest advance new domain 13th grammar%23ENT grammar%23ENT grammar%23ENT proceeding cham 182+198 orchestrate network grammar%23NPR topo logical south+africa mkeet%40cs.uct.ac .za grammar%23NPH oliver.kutz%40unibz.it abstract grammar%23NPR use widely ontology subject domain model guidance glean from ontology yet offer multiple mereological theory even more when combine topology i.e. mereotopology complicate landscape decidable language put restriction on language feature so only fragment mereo topo logical theory represent yet during model full feature need check correctness address issue by specify structured network theory formulate multiple logic that glue together by various linking construct grammar%23NPR grammar%23NPR theory five sub- theory together+with dl based owl species first second order logic this network grammar%23NPR orchestrate ontology further propose automated step toward resolution language feature conflict when combine module avail new owl classifier tool pinpoint profile violation
concept computing methodology knowledge+representation reasoning ontology engineering description logic keyword mereology grammar%23NPR owl ontology engineering reference format grammar%23NPH orchestrate topo logical theory proceeding knowledge capture grammar%23NPR grammar%23ENT united+states+of+america grammar%23ENT grammar%23NPR grammar%23ENT page
introduction mereology theory parthood be well establish ontology philosophy use widely applied ontology ontology on semantic+web other ontology driven information+system for+instance medical terminology grammar%23NPR part whole relation grammar%23NPR ct gene ontology ontology be widespread use for+instance level database integration biological+science foundational model anatomy all use variant mereology grammar%23NPR be extension mereology topological notion so as be able distinguish between interior part tangential part use geographic+information+system annotation picture infer e.g. whether country be landlocked e.g.
grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT isbn 978-x-xxxx-xxxx-x grammar%23NPR mm grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2Fnnnnnnn.nnnnnnn due trade-off between expressiveness logic language computational complexity be difficult represent mereology mereotopology in+full such that obtain desired inference attempt include sep triple workaround extension owl reflexivity reflexivity grammar%23NPR trade-off assessment owl species on consequence automated reasoning mereotopology most expressive description logic based owl language owl grammar%23ENT create further complication modeller due expressiveness limitation on object properties as consequence computational complexity trade-off concrete example such trade-off be choice between parthood transitivity use qualified number restriction give modeller three option use human their limb foot as example human have as part any number limb infer that if foot be part limb limb+part human then that foot be part that human canonical human have as part exactly four limb but not infer that foot be part human human have exactly four limb make transitive inference about foot at cost scant tool support poor performance compare option due translate call corresponding reasoner if some owl ontology grammar%23NPR use option owl ontology option then import merge ontology lead option owl file outside owl grammar%23ENT un decidability general this lack closure modular combination be rather unusual aspect logic based modelling language notice that this problem not exist e.g. typical situation ontology development prot%C3%A9g%C3%A9 leave modeller strand
within owl context confuse model as mereo topo logical theory include not meet representation reasoning requirement domain expert computationally incompatible modelling choice not be obvious yet various scenario different choice be applicable
aim solve issue by tie together two component
first structure mereological theory use twopronged approach be several recognised sub- theory grammar%23NPR mereotopology establish language their language feature feature represent various subset theory investigate this intersection elucidate maximum possible sub- theory each language their interaction present formally networked set theory represent as specifically design link theory represent different logic up high order logic support by tool ecosystem ontohub.org hets see below select appropriate reasoner rather than deterministically stick one upfront result interconnected micro ontology grammar%23NPR grammar%23ENT grammar%23ENT united+states+of+america as basic version ontology design pattern mereology topology mereotopology modeller then choose precisely theory include usage scenario ontology when use linking more expressive version when reasoning time be not crucial because+of interaction language feature push ontology into one fragment another propose step towards conflict resolution pinpoint automatically axiom violate logic so that modeller make informed decision
remainder paper first summarise mereotopology section logic use formalisation as+well+as as structuring language section subsequently outline technique network formalisation section contribution conflict resolution describe section discuss section conclude section
preliminary grammar%23NPR this summary mereotopology base on focus on go from basic axiom make+up simple theory parthood m location t up grammar%23NPR theory as depict fig.
theory focus on this paper from weak strong go uphill description grammar%23NPR grammar%23ENT
mereology side basic theory be ground mereology m m have part as primitive be reflexive antisymmetric transitive grammar%23NPR grammar%23NPR grammar%23NPR table from define proper parthood grammar%23NPR table from reflexivity grammar%23NPR deduce following from antisymmetry p reflexivity pp asymmetry grammar%23NPR proper part be also transitive grammar%23NPR
now also define grammar%23NPR add notion supplementation among other things m result general extensional mereology m grammar%23NPR grammar%23NPR
other branch begin ground topology t connection relation be reflexive symmetric grammar%23NPR grammar%23NPR
extend by add spatial enclosure grammar%23NPR as define grammar%23NPR then combine into grammar%23NPR consist grammar%23NPR converse monotonicity grammar%23NPR self connect grammar%23NPR bridge connection part grammar%23NPR fusion grammar%23NPR axiom definition then define interior proper part grammar%23NPR from tangential proper part grammar%23NPR final aspect be then about closure interior exterior result grammar%23NPR i.e. grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR three extra axiom require their definition grammar%23NPR so then also belong grammar%23NPR
be possible construct mereotopological theory different way such as take proper part as primitive merge part connection into ternary relation add atomicity boundary see detail be also multiple option incorporate more comprehensive topological theory e.g. start from connection define other relations including parthood focus on containment convex hull see also table possible ontological commitment our scope be not mereotopology s%C3%A9 however but find usable way reusable approach represent at+least one such set interconnected theory computationally reason over automatically
axiomatization grammar%23NPR core axiom definition base on summarise from p part pp proper part o overlap c connection e enclosure eq indiscernible ipp interior proper part grammar%23NPR tangential proper part south+carolina self connect c closure interior e exterior sum complement
x x grammar%23NPR p x y p y z p x z grammar%23NPR p x y p y x x y grammar%23NPR p y x z p z y o z x grammar%23NPR w o w z v v o w v grammar%23NPR c x x grammar%23NPR c x y c y x grammar%23NPR p x y e x y grammar%23NPR e x y df z c z x c z y grammar%23NPR e x y p x y grammar%23NPR south+carolina x y z x y z c y z grammar%23NPR z south+carolina z o z x o z y w p w z o w x o w y c x y grammar%23NPR z y c y z x x c y x grammar%23NPR p x cx grammar%23NPR c cx cx grammar%23NPR c x y cx cy grammar%23NPR cx df ex grammar%23NPR ex df x grammar%23NPR ix df c z y o x y grammar%23NPR table basic additional axiom definition theorem base on summarise from
x y df p x y p y x grammar%23NPR o x y df z p z x p z y grammar%23NPR eq x y df p x y p y x grammar%23NPR grammar%23NPR x y df pp x y ipp x y grammar%23NPR ipp x y df pp x y z c z x o z y grammar%23NPR pp x x grammar%23NPR pp x y pp y z pp x z grammar%23NPR pp x y pp y x grammar%23NPR orchestrate network grammar%23NPR topo logical grammar%23ENT grammar%23ENT united+states+of+america finally note that such grammar%23NPR theory do not have exist in+isolation grammar%23NPR relation extend from basic taxonomy part whole relation use dolce domain range restriction grammar%23NPR incorporate so grammar%23NPR be compatible grammar%23NPR effectively extend other foundational ontology rely heavily on theory part notably grammar%23NPR grammar%23NPR struggle reconcile expressive theory from ontology practicality owl e.g. first order logic version bfo include universal parthood theory theory include partonomic inclusion grammar%23NPR yet be absent from owl version finally serve many attempt domain ontology development subject domain such as medicine environmental data
preliminary logic while many logic consider zoom into considerable uptake some tool support owl family language grammar%23NPR newly standardised metalanguage
grammar%23NPR owl latest standardised language owl family be owl most species family base on dl be decidable fragment grammar%23NPR aim find sweet+spot trade-off between expressiveness decidability e.g. with+respect+to full grammar%23NPR prohibit use negation use number restriction greater than one so as obtain language language be computationally good
general most dl not have extensive support relational properties in+fact be already problematic represent even just ground mereology fully as show table dl based owl species owl grammar%23ENT column be deceptive transitive property be not simple anymore simple properties be be not transitive defined by property chain so not use jointly properties properties involve negation i.e. reflexivity asymmetry appear qualified cardinality restriction cause increase subtheories as see further below e.g. theory vs. table
properties parthood p proper parthood pp ground mereology connection c ground topology their inclusion owl family grammar%23NPR
dl based owl species grammar%23NPR feature dl lite grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR c grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR ifomis.uni-saarland.de%2Fbfo grammar%23NPR www.acsu.buffalo.edu grammar%23NPR theory bfo relevant feature situation where two independently developed ontology suppose reuse as module larger ontology difference between ontology typically prevent from work together properly solve interoperability issue involve identification synonym homonym development bridge axiom link ontology appropriately
challenge be diversity notion provide+for design pattern interrelation among ontology distributed ontology model specification language aim at provide unified metalanguage handle this diversity general theoretical background present detailed description language find
approve as standard object+management+group object+management+group enjoy following distinctive+feature structure construct building ontology from existing ontology like imports union forget interpolation filter open world versus closed world semantics module extraction mapping between ontology like interpretation theory conservative definitorial extension alignment etc. network ontology their combination
structuring language design as multi logic meta language already support mainstream ontology language in+use today as depict fig. graph organise along two dimensions quality logic translation expressivity logic expressivity range from rdf owl variant first second order logic exhaustively cover modelling requirements diverse community framework base on theory institution abstract from peculiarity syntax semantics particular logic see technical detail quality translation be related how directly proof support guarantee along translation e.g. subinstitution correspond intuitively directly sub- logic target require one encode semantics source into extra axiom express target logic for+instance when encode many sorted logic back into single sorted logic
tying it outline mereo topo logical theory various logic then combine two construct theory theory list table come+about by one+by+one take theory list fig. axiom axiom make+up see sect 2%29 assess represent selected language note that be not theory because not every combination axiom table make sense ontologically e.g. not want theory consist say grammar%23NPR reflexivity parthood grammar%23NPR exterior because that combination be not meaningful converse just six named theory fig. not apply either because even ground mereology not represent fully owl grammar%23ENT recall table hence be also not simply mereo topo logical theory %C3%97 dl based owl species grammar%23NPR grammar%23NPR due overlap exclusion feature among language as discuss section finally not assert property definition like grammar%23NPR owl whereas can grammar%23NPR www.omg.org%2Fspec%2Fdol%2Fk-cap grammar%23ENT grammar%23ENT united+states+of+america owl grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR el owl el dl liter owl ql grammar%23NPR owl grammar%23NPR logic simultaneously exact model expansive comorphisms model expansive comorphisms grey no fixed expressivity green decidable ontology language yellow semi- decidable orange some second order construct red full second order logic grammar%23NPR uml cd clschema.org grammar%23NPR figure sub- graph logic currently support by grammar%23NPR link variety logic translation
grammar%23NPR grammar%23NPR therefore add as primitive owl formalised theory theory be already available on grammar%23NPR at https%3A%2F%2Fontohub.org%2Frepositories%2Fmereotopology repository currently extend first secondorder model full presentation purpose anticipated usage present structuring network theory use by focus on owl species extension owl grammar%23NPR from structure point+of+view second order axiom deal+with just same way including sol second order sublogic grammar%23NPR grammar%23NPR higher+order extension grammar%23NPR common logic grammar%23NPH grammar%23NPR hol- light as+well+as grammar%23NPR see e.g.
micro ontology basic structuring operation logical theory be already available on logic specific level for+instance when work purely owl employ purely homogeneous dl based owl fragment call grammar%23NPR most basic mechanism relevant present paper gain on top owl be following control over signature via rename symbol along imports extend existing theory new axiom theory extension union theory theory interpretation syntactic extraction module use specified symbol lemma book-keeping counterexample specification
ontology repository grammar%23NPR theory be grammar%23NPR see stl.mie.utoronto.ca co lore include grammar%23NPR grammar%23NPR grammar%23NPR system mereology relate via theory extension interpretation however not use explicit structure not cover owl based system
illustrate this section two more advanced feature item discuss section begin by illustrate idea extend theory by new axiom feature use simple possible example on+purpose illustrate underlying idea as clearly as possible
extend grammar%23NPR owl el ql into grammar%23NPR owl ql by add symmetry grammar%23NPR write as+follows logic grammar%23NPR ontology grammar%23NPR grammar%23NPR then grammar%23NPR c characteristic symmetric grammar%23NPR now simultaneously illustrate union theory feature control over signature feature union union grammar%23NPR owl el grammar%23NPR owl grammar%23NPR result grammar%23NPR express owl grammar%23ENT simultaneously illustrate howwe directly manage signature given theory use diverge vocabulary talk about connection connection be very common phenomenon in+practice
grammar%23NPR ontology grammar%23NPR grammar%23NPR con c grammar%23NPR co c result be theory combine axiom theory unified signature connection use symbol c
note union owl theory more generally framework owl ontology ontology break syntactic restriction impose by owl grammar%23ENT still equip sound complete reasoning by translate grammar%23NPR call corresponding reasoner see also data policy user grammar%23NPH submit grammar%23ENT united+states+of+america agreement occur by+chance 95%25 confidence+interval grammar%23NPR grammar%23NPR between system either human team grammar%23NPR be not significantly different from 95%25 grammar%23NPR grammar%23NPR between grammar%23NPR in+other+words system behave as user do also from statistical point+of+view
discussion result show that task be feasible all scenario that our system exhibit good accuracy follow analyse case user disagree system also highlight table grammar%23NPR
expect three type disagreement system miss policy b system block policy c system not decide about as not have enough information note that option c never emerge from study team always make clear decision whether propagate policy grammar%23NPR
data journey both team agree that permission lds extract propagate system explanation show that policy actually block by dn grammar%23NPR both team indicate as error identify anomaly anomaly intentionally introduce reassure about committment perform task dis agreed about two specific permission cc distribution ldr re utilization although general activity be web+site crawl index considered type indexing implement affect interpretation content web+site such way potentially damage interest original owner process have step some values change then change value assign be label item permission distribute new output not give granted protect owner content this choice change depend on content data not on general action perform
data journey this process produce two output performance dataset including performance vector machine+learning algorithm compare consequent lift chart i.e. graphical+representation data system not propagate permission this decision align two team however changed their decision see explanation give by system moreover decided no policy propagate output process while agreed system both prohibition duty preserve performance dataset output only duty case lift chart this difference motivate by interpretation nature performance dataset by analyse conversation grammar%23ENT although one participant observe questionnaire that task decide on policy apply derived dataset be impossible
be relevant here be not point be arguable but fact that participant believe process policy be not enough decide whether propagate policy
user study emerge that considered dataset contain only measure performance algorithm while interpreted as labelled dataset therefore contain enhanced version input+data correct interpretation be once reflect data+flow system block all policy as be not applicable performance dataset
data journey dis agreed on behavior set rule about two permission odrl copy odrl display mark as wrong behavior dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR dn grammar%23NPR
grammar%23NPR propagate because input have permission copy in+case two have not should not need reason on combination decide propagation during session team propose propagate permission combined node as+soon+as no prohibition be present
data journey changed their mind about permission see explanation reasoner
dataset modify make+sense that permission not propagate however dis agreed both system other team identify problem by inspect explanation system relation dn grammar%23NPR propagate various permission as far as understand be no bias introduce data therefore permission keep intact outcome process depend entirely on input without additional information be just mathematical+process keep information intact in+fact dn grammar%23NPR define as grammar%23NPR refer translation data another symbolic structure model while keep same interpretation be case where team disagree about system however justification seem robust enough accept change behavior dn grammar%23NPR relation
data journey both team observe that dn grammar%23NPR propagate permission involve while system decide not main argument be that relation consider policy all dataset involve combination however without know system propagate leave decision consistency check apply at later stage another discussion observe how some case be dependency between policy be case duty be always context permission therefore by propagate former system also propagate latter for+example permission use propagate as dependency duty obtain consent
issue illustrate group following general theme incomplete knowledge knowledge+base use by system be not complete rule add modify in+order+to fix behaviour with+respect+to certain policy relation use methodology present data+flow be accurate include all relevant cap grammar%23ENT submit grammar%23ENT et+al.
data reverse+engineering recurrent theme assess whether permission propagate be contingency data reverse+engineering define software+engineering as use structured technique reconstitute data asset existing system observe that correct interpretation nature output be crucial
be fundamental importance that data+flow description be accurate including assess how+much information input data+source extract from output data some case implemented data+flow be not complete enough reflect this issue
content dependent decision argue that one case impact process on output policy not assess without inspect content data not argue against in+principle however assume that new relations develop within methodology in+order+to capture fine grained implication process action on policy propagation make this case incomplete knowledge+base
dependant policy approach system be consider policy in+isolation focus on their interaction process action however be clear that policy on their incorporate a+number+of dependency derive from semantics action involve for+example odrl copy be kind odrl use from way formalise policy document grammar%23NPR duty always declare context permission see also discussion on however by know that policy need take+into+account on output given process dependant policy extract from original policy document
legal knowledge general observation participant make be that be legal issue therefore legal+expert involve definition policy action process descriptor grammar%23NPR on one hand suggest importance provide support data hub manager on decide on policy propagation as not expect this type user have legal knowledge on+the+other+hand highlight more general+issue in+fact validation system by legal+expert assume legal framework cover status metadata oriented automatic reasoner rule+of+law currently miss
conclusion this work evaluate approach system support assessment policy propagate from data+source derived dataset data hub participant agree that be possible decide whether policy associate dataset need associate another derived dataset result our user study demonstrate that task solve automatically good degree accuracy by consider both result user study feedback collect system be overall accurate be good value user study also let emerge set critical aspect involve be important that knowledge+base be complete in+particular that process description not hide element influence propagation policy for+example make clear how+much data input extract from output
this study conclude that be evidence fundamental correspondence between possible kind relation way affect policy propagation however more research require in+order+to include knowledge+base other aspect involve policy reasoning right other stakeholder involve process including one process executor action add value information right entity represent data from business private citizen
grammar%23NPH data reverse+engineering slay legacy dragon
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH protection composition crossmedia content collaborative environment multimedia tool application grammar%23ENT
conceptualisation right metarule law web data grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH address grammar%23NPR data grammar%23NPR institute+of+electrical+and+electronics+engineers
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH reasoning grammar%23NPH grammar%23NPH semantic+web journal grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
propagation policy rich grammar%23NPH proceedings 8th on knowledge capture

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
describe semantic+web application through relation between data node grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH smart city data challenge opportunity institute+of+electrical+and+electronics+engineers internet compute grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH license compose grammar%23ENT springer 151+166
grammar%23NPH grammar%23NPH city hub cloudbased smart city cloud+computing technology science grammar%23NPR institute+of+electrical+and+electronics+engineers 6th on institute+of+electrical+and+electronics+engineers 799+804
grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al.
swarm at edge cloud institute+of+electrical+and+electronics+engineers design test grammar%23ENT
grammar%23NPH dataset rdf license legal knowledge information+systems grammar%23NPR grammar%23ENT twenty-seventh annual grammar%23NPR ed. http doi.org%2F10.3233%2F978-1-61499-468-8-187 grammar%23NPH grammar%23NPH towards formal semantics grammar%23NPR policy international symposium on rule rule markup+language semantic+web springer 360+375 expanding snomed-ct through spanish drug summary pcalleja%40fi.upm.es rgarcia%40fi.upm.es lupe%40fi.upm.es asun%40fi.upm.es abstract terminology biomedical field be main resource use clinical practice keep up-to-date meet use case be critical operation even case well maintain terminology such as snomed-ct involve much effort from domain expert pharmacological products drug constantly approve make available in+the+market their clinical information also update terminology
new+drug provide document natural+language contain essential information
paper propose method populate spanish extension snomed-ct drug name use spc represent their clinical data section terminology more precisely method apply therapeutic indication adverse reaction section disease name recognize as name entity document map terminology
relation between drug name entity also represent terminology base on specific role have document
concept grammar%23NPR applied computing health+care information+system keyword snomed-ct mapping extension role association+for+computing+machinery reference format grammar%23NPH grammar%23NPH expand snomed-ct through spanish drug summary product grammar%23NPR proceedings grammar%23NPR grammar%23ENT knowledge grammar%23NPR grammar%23ENT association+for+computing+machinery new+york new+york united+states+of+america page
permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148028 introduction terminology lexicon from early moment create face problem not be up-to-date meet real-world use case maintain be quite expensive need constantly update well know for+example biomedical field deal new+drug product product constantly approve make available on+the+market medical surveillance medical+record need keep track prescribed drug this area main terminology use medical purpose be grammar%23NPR this multilingual terminology represent whole knowledge medical domain by+means+of concepts description associate code relate between nest different level detail category however drug catalog be not complete compose mainly generic products
commercialization new+drug grant by regulatory+agency country region this authorization depend on on such agency verify new product clinical research through be natural+language document essential information drug reflect e.g. drug name indication dosage contraindication adverse reaction etc. this valuable information capture by system
work biomedical field focus on corpus provide by conference challenge such as grammar%23NPR on exploitation paper abstract work closer health application exploit resource like while grammar%23NPR be usually short simple phrase write by doctor be complex detailed document
be most important task inside process involve natural+language+processing natural+language+processing consist find classify real-world entity denote referent term proper+name name entity however state+of+the+art orient towards retrieve all possible entity regardless+of whether be relevant not concrete use scenario apart+from paper abstract conference corpus natural+language document display mixed information be entity entity be not relevant use scenario that grammar%23NPR www.snomed.org%2Fk-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america p. grammar%23NPH grammar%23NPH grammar%23NPH produce noise on overall result task be case spc although divide section refer specific use scenario be lot mixed information
current classification cover grammar%23NPH system just deal taxonomic type not mean represent entity role such role determine general meaning function entity corpus however as conceptual model propose by grammar%23NPR show entity role also represent as classification model taxonomic hierarchy disease normally represent by affection type such as mental+disorder gastric disease however adverse reaction contraindication be role that entity have given context also represent taxonomic form
prove heterogeneity information present section some research found by carry+out this research focus on two specific section reflect that be entity same named entity type disease but different role apart+from proper role section
indication section this section provide information about disease disease treat drug however this section be sometimes verbose include information from other section commonly this extra information deal contraindication disease drug not prescribe e.g. not use as treatment disease disease refer medical+record patient patient not have recent history
reaction section this section provide information about unwanted effect cause by administration drug disease disorder their frequency however sometimes this section also contain therapeutic indication information specify adverse reaction study particular patient disease figure show example adverse reaction section all disease entity figure be italics
only entity surround continuous black+box be adverse reaction entity surround segmented black+box represent other role
tratamiento con ser amnesia confusi%C3%B3n hipersexualidad delirio y mareo
base lo ensayos controlados con placebo que incluyen un total pacientes con enfermedad grammar%23NPH tratados con grammar%23NPH y pacientes con placebo
gastrointestinales nervioso mareo somnolencia hipercinesia amnesia estre%C3%B1imiento grammar%23NPR grammar%23NPR frecuentes frecuentes figure excerpt adverse reaction section this paper propose method populate spanish extension snomed-ct drug name use spc represent their clinical data section terminology more precisely method apply therapeutic indication section adverse reaction section disease name recognize as entity document map terminology relation between drug name disease entity base on specific role have document
paper be structured as+follows section describe related work section address proposed method populate snomed-ct from spc section evaluate discuss obtained result finally section present conclusion work highlight future research line
grammar%23NPR be many work literature focus on exploitation snomed-ct clinical purpose however be reduced number work interest population terminology researcher such as aim expand swedish extension synonym extract from electronic health record
spanish be few project project use spanish extension snomed-ct focus on map processes without specific work area such as general mapping project use snomed-ct terminology english other language such as swedish
state+of+the+art show that good result provide by supervised machine+learning technique
technique require big corpus train such as thus use machine+learning technique be limited some domain such corpus not exist furthermore corpus not reflect entity role
define attach text segment entity task call template filling role define by act meaning given context normally associate lexical syntactic pattern nevertheless template fill task biomedical domain focus on event relation like cause effect drug drug interaction
mapping technique use biomedical area thorough analysis carry+out work be generalpurpose studies whereas not focus on recognize entity entity include numerous descriptor such as disease
observe that current technique not orient mapping terms extract from natural+language text current technique be very sensitive any linguistic variation not exploit syntactic+category word compose terms
method proposed method populate spanish extension snomed-ct drug name use spc represent their clinical data section terminology divide into three main processes name entity recognition disease drug name entity map snomed-ct creation snomed-ct extension
first process focus on recognition named entity different section this process divide into two task first aim find drug name name represent product second task focus on recognition disease clinical data section same role although have specific separated section each piece information present grammar%23NPR snomed-ct through spanish drug summary product grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america define introduction mixed information named entity recognition process base on gazetteer rule orient spanish language
second process aim map identified disease entity snomed-ct identify their equivalent concept mapping process try find good candidate by create all possible variant entity description concept appear terminology use word normalization synonym finally snomed-ct extension create populate product name create their adverse reaction indication concept relate appropriate role map disease
method develop collaboration project grammar%23NPR focus only on two concrete section clinical data therapeutic indication adverse reaction section implementation method rely on guerrilla+army+of+tamil+eelam+%28gate%29 text+processing framework grammar%23NPR provide set more than 1%2C000 spc spanish divide into different file section from this set randomly selected annotate by domain expert from agency annotation process make separately two required section spc each section disease represent target role their section therapeutic indication adverse reaction spc use create two gold+standard corpus evaluate grammar%23NPH process gold+standard corpus therapeutic indication section gold+standard corpus adverse reaction section moreover grammar%23NPR provide list disease entity their equivalent snomed-ct description concept code as gold+standard mapping process
external resource as mention before problem named entity recognition be identify named entity accord role however gazetteer named entity need support process drug name gazetteer create use snomed-ct description description contain label product preferred term as this work aim populate snomed-ct product concepts if concept already exist be not necessary create instance be possible refer directly
disease disorder name selected gazetteer extract from spanish version medical dictionary regulatory activity grammar%23NPR from grammar%23NPR grammar%23NPR be rich highly specific standardized medical terminology facilitate sharing regulatory information internationally medical products terminology be disease term list classify hierarchy by their degree depth distribute various language including spanish
two low hierarchy level call both level contain more+than 70%2C000 terms terms reflect simple observation report clinical practice due exhaustive list disease specific terms both level use create gazetteer entity named entity type disease
grammar%23NPR rather than subset disorder clinical finding snomed-ct be due reason preliminary study find that coverage grammar%23NPR common used terms spc be good grammar%23NPR base on terminology belong medicine healthcare products regulatory+agency grammar%23NPR united+kingdom develop pharmaceutical purpose
grammar%23NPR be easy use due classification fewer hierarchy snomed-ct have concepts high detailed description description be not relevant construct gazetteer e.g. anemia en la madre que com plica el iron+deficiency+anemia mother complicating birth
gazetteer use disease disorder name extract from grammar%23NPR grammar%23NPR contain lot international terms contain lot abbreviation use colloquially this gazetteer create by extract only abbreviation
collaboration grammar%23NPR synonym acronym catalog create enrich mapping process snomed-ct. use their expert knowledge resource such as grammar%23NPR set term group accord their semantic+relation this catalog not focus on detect all possible case but only most relevant e.g. c%C3%A1ncer tumor maligno malign tumour grammar%23NPR pr%C3%B3stata benign prostate hyperplasia set adjective incrementada increase elevada elevate aumentada augment
named entity recognition process each spc named entity recognition process perform two different task first identify drug name over name section this section product define different dosage pharmaceutical form commercialize e.g. grammar%23ENT c%C3%A1psulas dura grammar%23NPR grammar%23ENT c%C3%A1psulas dura grammar%23NPR so terms consider different named entity as snomed-ct represent product this task gazetteer create from snomed-ct use identify already drug if drug name not exist terminology different terms terms be section consider new entity
second task focus on identification disease clinical data section this case therapeutic indication adverse reaction section main challenge each section be identify not only disease but correct accord their role their corresponding section as disease recognition gazetteer extract from from use discover entity support disease recognition some coded pattern use find entity entity not reflect on gazetteer table show lexical pattern character combination affix syntactic pattern word combination use identify disease
show example document be some example pattern commonly use medicine be lexical pattern represent by suffix itis mean inflammation e.g. sinusitis lexical pattern use identify as disease noun contain at+least affix nominal phrase include adjectival phrase grammar%23NPR that join noun also be syntactic pattern represent by word combination disease eponim grammar%23NPH syntactic pattern represent ink-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH show between bracket part span text consider as entity syntactic pattern are disease that represent by fluctuation disorder biological substance organism present set have same meaning different word for+example pattern represent decrease biological substance be four words compose pattern disminuci%C3%B3n decrease reducci%C3%B3n reduction p%C3%A9rdida loss descenso decline optionality word present between parenthesis separate by symbol
disease name entity identify new set coded pattern execute in+order+to classify disease accord their role role pattern be syntactic pattern layout pattern format information present figure show three example syntactic pattern two layout pattern syntactic pattern la ser adverse reaction be reflect that all entity scope pattern until full+stop have adverse reaction role on+the+other+hand layout pattern also represent example all entity table be not header bold be also disease adverse reaction role other role represent different pattern first represent word combination pacientes con patient reflect disease role drug prescribe therapeutic indication role second represent by row bold font reflect classification role mentioned disease classification role
disease pattern identify lexical patten oma itis osis algia ema asis emia orrea penia plasia grammar%23NPR pattern np enfermedad de np enfermedad grammar%23NPR afecci%C3%B3n de np virus de np empeoramiento de np prolongaci%C3%B3n incremento elevaci%C3%B3n aumento de np alteraci%C3%B3n anormalidad cambios descompensaci%C3%B3n de np disminuci%C3%B3n reducci%C3%B3n p%C3%A9rdida descenso de np identified role pattern found prior study each corpus collaboration expert from table show pattern identify their role therapeutic indication section disease section role represent by syntactic pattern also two more role observe this section medical+record contraindication
medical+record advise patient that therapeutic indication drug be dependent disease patient clinic history
contraindication represent disease use drug not recommend
adverse reaction section observe that adverse reaction present spc represent table indicate affection classification type their frequency at+the+same+time document document not contain table also use affection classification type as header enounce adverse reaction
layout pattern use identify entity role pattern identify grammar%23NPR pattern role np therapeutic indication pacientes con np therapeutic indication alivio lo s%C3%ADntomas de np therapeutic indication asociado np therapeutic indication prevenci%C3%B3n de np therapeutic indication administraci%C3%B3n en combinaci%C3%B3n en np therapeutic indication en ensayos cl%C3%ADnicos de np therapeutic indication en estudios cl%C3%ADnicos de np therapeutic indication sin np medical+record que se hayan excluido np medical+record excluyendo np medical+record que no tiene np medical+record siempre que no exista np medical+record pero no np contraindication no protege np contraindication no debe ser utilizado np contraindication no se recomienda np contraindication no debe utilizarse np contraindication no se ha demostrado estudiado np contraindication no se han realizado estudios np contraindication potenciado por np medical interaction con el fin evitar np medical interaction no se asocia np adverse reaction no se observ%C3%B3 np adverse reaction sin indicios de adverse reaction layout pattern header bold tag b u classification header entity between label table table adverse reaction entity below classification header adverse reaction adverse reaction role pattern associate entity entity be inside html table tag as entity pattern identify entity entity be text header associate adverse reaction role role therapeutic indication be also present section specific case spc enounce disease drug prescribe other role role appear be medical interaction adverse reaction classification header medical interaction represent disease disease only appear specific case disease produce more adverse reaction adverse reaction role represent disease disease not discover as adverse reaction during drug clinical research last role refer classification header i.e. general disease disease classify enounce adverse reaction
named entity recognition process base on role pattern verify that named entity belong corresponding section discard one one belong other consider as assumption that presented information be representative section if entity not belong any role take same as section role represent taxonomic form as show figure however this work only focus on role therapeutic indication adverse reaction
disease map process disease map process provide equivalent concept snomed-ct terminology disease entity this process have two task first preprocess entity create grammar%23NPR snomed-ct through spanish drug summary product grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america pattern disease pattern therapeutic indication gazetteer medical+record pattern contra-indication pattern medical interaction pattern adverse reaction pattern classification group pattern adverse reaction figure disease role classification possible variant disease write description terminology use word normalization synonym second task identify candidate grammar%23NPH concept use mapping algorithm base on entity variant syntactical function word
preprocess entity preprocess aim prepare disease entity obtain last process identify their equivalent concepts snomed-ct next task task clean input entity create all disease entity variant that entity adopt all word be lowercase converted spanish accent remove stopwords delete once text be clean word classify into two main group keyword descriptor keyword be noun be main reference mapping process rest word such as adverb adjective number classify as descriptor all classified word use create disease entity variant be initial compose by original token entity case that be words plural new disease entity variant create token singular normalization finally disease entity variant check synonym acronym catalog create if one word match new disease entity variant create each synonym as show fig. figure show different disease entity variant disease term c%C3%A1ncer advanced cancer testicle keyword represent bold underline
identification concept identification task aim provide grammar%23NPH concept each disease entity use mapping algorithm base on classification word keyword descriptor algorithm take all disease entity variant search each through description terminology concepts as mapping focus on disease only concept concept belong category disease clinical finding take+into+account
algorithm use threshold threshold count number equal word by exact comparison between disease entity variant original entity variant testiculos testicle avanzado advanced cancer cancer maligno malignant testiculos testicle tumor tumour avanzado advanced synonymous entity variant entity variant testiculo testis avanzado advanced cancer cancer maligno malignant testiculo testis tumor tumour avanzado advanced normalization synonymous entity variant figure entity variant c%C3%A1ncer grammar%23NPH description pseudocode algorithm present algorithm
algorithm store grammar%23NPH description description have same threshold+value as candidate discard be below that value if this threshold exceed all previous candidate delete threshold update algorithm start save new candidate beginning one one produce change threshold distinguish between keyword descriptor name keyword prioritize so when algorithm process grammar%23NPH description firstly compare name name recognize disease entity variant grammar%23NPH description if number keyword be big than threshold update by erase previous candidate if number keyword be equal threshold algorithm do same comparison descriptor this approach term term have two keyword no descriptor be good choice than another term have only one keyword three descriptor common
disease entity variant testicle cancer algorithm go through grammar%23NPH store candidate terms terminology exceed threshold this case grammar%23NPH have no terms contain two keyword test%C3%ADculos testicle c%C3%A1ncer cancer but only one e.g. c%C3%A1ncer piel skin+cancer lo test%C3%ADculos open wound testicle etc. another disease entity variant same disease tumor testis malignant+tumour algorithm find new candidate two keyword discard previous candidate e.g. tumor simple test%C3%ADculo simple tumour testis
algorithm discard previous candidate when find term tumor maligno test%C3%ADculo malignant+tumour testis contain descriptor above previous candidate result test%C3%ADculos disease entity be grammar%23NPH term tumor maligno test%C3%ADculo malignant+tumour testis
identified candidate evaluate select good one grammar%23NPH be very large represent very detailed concepts so be necessary select best in+some+way each candidate have incorporated measure base on approximation distance grammar%23NPH description measure calculate by subtract number word grammar%23NPH description stopwords not count sum all word keyword descriptor that algorithm found disease entity variant this measure give quantification similarity between candidate grammar%23NPH description grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH by algorithm organize from low high distance value description description have low value select
example cancer testicle candidate retrieve from algorithm be contain word tumor tumour maligno malignant test%C3%ADculo testi optimal candidate be tumor maligno test%C3%ADculo malignant+tumor testis but algorithm also return tumor maligno test%C3%ADculo c%C3%A9lulas grammar%23NPR tumour testis first case distance be zero because grammar%23NPH description compose by three word not count stopwords match second case distance be two grammar%23NPH description have five word not count stopwords only three match good candidate be low distance tumor maligno test%C3%ADculo malignant+tumour testis if algorithm retrieve more+than one description consider as bad mapping
grammar%23NPR algorithm grammar%23NPR grammar%23NPR candidate grammar%23NPR grammar%23NPR all grammar%23NPR do all grammar%23NPH grammar%23NPR do keyword grammar%23NPR grammar%23NPH descriptor grammar%23NPR grammar%23NPH if keyword grammar%23NPR then candidate grammar%23NPH keyword threshold keyword end if if keyword grammar%23NPR then if descriptor grammar%23NPR then candidate grammar%23NPH word threshold word end if if descriptor grammar%23NPR then candidate candidate grammar%23NPH end if end if end end candidate grammar%23NPR candidate grammar%23NPH extension creation process third process consist create grammar%23NPH extension
identifier new concepts extension according+to grammar%23NPR manual create use grammar%23NPR each spc various concept create first identified drug name use find general drug name without dosage pharmaceutical form e.g. drug name grammar%23ENT grammar%23ENT create new concept subtype grammar%23NPH concept pharmaceutical biological doc.ihtsdo.org%2Fdownload%2Fdoc_technicalimplementationguide_current-enus_int_20150131.pdf product product then new subtype general drug concept create use drug name if general drug name already exist terminology subtype reference use grammar%23NPH identification+number grammar%23NPR identified product
therapeutic indication section new concept subtype drug indicate situation create show general drug name description all disease entity therapeutic indication role reference as subtype
same process make adverse reaction section new concept subtype adverse reaction section disorder create general drug name description also disease entity adverse reaction role reference as subtype
evaluation evaluation make separately two main process grammar%23NPH process mapping process former perform by measure result obtain over gold+standard corpus therapeutic indication adverse reaction section latter perform through gold+standard disease entity
grammar%23NPH evaluation each section evaluate separately four experiment related resource use grammar%23NPH process first experiment only take result obtain by gazetteer second experiment add result obtain by pattern coded disease name entity third experiment use role pattern section add associate entity section role finally fourth experiment use pattern other role discard entity entity not associate section role experiment both section work open world assumption i.e. entity no role consider be part target role
evaluationmetrics use grammar%23NPH process be precision p recall r f measure f evaluation measure detected entity twomatching criteria as propose other biomedical evaluation normally grammar%23NPH system use strict exact matching criteria entity detect by system entity gold+standard corpus have same span text named entity type however entity provided gold+standard corpus have problem consensus between annotator i.e. same entity same span text be different length different offset one side
difference between annotation be adjective expert take not into account annotation process such as adjective describe particular case patient disease e.g. recurrente recurrent intensity degree e.g. grave severe thus partial criteria allow that span text offset be different
present obtained result experiment therapeutic indication section firstly experiment denote that gazetteer cover entity entity present corpus but not representative enough cover experiment show how lexical syntactic pattern detect more disease name entity thus improve result gazetteer expand grammar%23NPH through spanish drug summary product grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america experiment show how pattern section role increase recall however precision decrease because syntactic pattern overlap other role pattern for+example sometimes pattern treatment overlap pattern no debe ser utilizado not use sentence no debe ser utilizado para el tratamiento la rinitis not use treatment rhinitis experiment demonstrate that apply pattern other role discard entity increase final precision minimal decrease recall all experiment reflect how corpus influence by annotation problem mention previously only partial matching criteria obtain good f measure experiment
therapeutic indication section evaluation strict partial exp. exp. exp. exp. evaluation adverse reaction section present table experiment show again that gazetteer cover entity however main problem gazetteer have be representation disorder represent disorder adjective e.g. increased glucose spc represent nominal form e.g. aumento glucosa increase glucose this problem solve by use syntactic pattern pattern represent nominal form disorder experiment pattern named entity type increase recall significantly partial matching criteria result exact matching criteria show again annotation problem corpus experiment show no modification over result layout pattern be associating discovered entity table header target role not discover new in+spite+of not improve result associate entity target role be critical if experiment be not open world assumption experiment finally show show pattern from other role use discard entity precision on overall result increase as previous evaluation partial matching criteria obtain good f measure
adverse reaction section evaluation strict partial exp. exp. exp. exp. disease map evaluation evaluation mapping algorithm perform four experiment base on different form be possible create disease entity variant measure use be accuracy obtained result over gold+standard first experiment address creation disease entity variant use token entity without synonym normalization second add possibility normalize disease entity in+case some words be plural third experiment focus only on creation entity variant use synonym last evaluate result obtain complete creation entity variant task normalization using synonym
disease entity map evaluation accuracy therapeutic indication section adverse reaction section exp. exp. exp. exp. table show mapping result experiment perform over disease entity recognize two section result therapeutic indication section prove that experiment normalization improve significantly result over simple mapping process use only original token as normally spc present disease plural experiment also improve result use synonym over experiment combination both technique create disease entity variant obtain good accuracy experiment contrast result adverse reaction section differ result also improve by normalization experiment but not affect by use synonym experiment experiment have same accuracy despite good result mapping evaluation be not complete representative because list gold disease map entity only cover disease from entity obtain therapeutic indication section from grammar%23ENT entity obtain adverse reaction section
evaluation perform only entity reflect all unknown case not take+into+account
unknown result subset therapeutic indication section subset adverse reaction section contain same grammar%23NPH description text as entity furthermore other unknown result be entity differ irrelevant adjective like frequency alopecia frecuente alopecia thus extension gold+standard develop by expert cover all possible case validate for+example real utility disease entity variant synonym adverse reaction section
conclusion grammar%23NPR evaluation grammar%23NPH process drive by role show that precision be increased natural+language document only entity require this process be key factor represent future relation disease drug
their pattern allow represent classification model entity identify named entity specific role
proposed role classification model represent target role all other role join as grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH complementary role target role however two specific use case also demonstrate that precisely classify define role benefit overall work because different section repeat role pattern thus be possible create complete role classification model all role reuse different use case homogeneous domain specific document without search reimplementing pattern
disadvantage themethod be that require very timeconsuming task involve domain expert although method propose real use case be good annotate only required entity instead annotate classify pattern role represent discover manually this method perform first approach introduce entity role inside grammar%23NPH task natural+language document detect specific entity accord role require
mention above most time-consuming task be pattern discovery creation role classification model
future line+of+work proposed method be explore automatic pattern discovery use algorithm base on distributional semantics such as same way next steps explore other section spc such as contraindication extract disease entity reflect into grammar%23NPH
proposed mapping process base on creation all possible disease entity variant show how result improve over use original token also algorithm use base on syntactic function word allow obtain close result in+case no clear map reduce retrieval false+positive case reflect gold+standard e.g. expert from agree that entity active rheumatoid+arthritis represent by grammar%23NPH concept artritis reumatoide map through algorithm not take+into+account descriptor word activa further evaluation do expert increase gold+standard unknown mapping case represent finally next step be request grammar%23NPR grammar%23NPR organization formalize created extension clinical data publish
this work fund by by by project grammar%23NPR
j. grammar%23NPH
recognition chemical entity combine dictionary base grammar based approach grammar%23ENT
use natural+language+processing identify drug drug interaction describe drug package+insert proceeding grammar%23ENT workshop on biomedical natural+language+processing grammar%23NPR
medical dictionary regulatory activity grammar%23ENT
biomedical named entity recognition survey machine+learning tool in+theory application advanced text+mining grammar%23NPR
grammar%23NPH spot drug unsupervised pattern match method extract drug name from very large clinical corpus healthcare informatics imaging system biology grammar%23NPR institute+of+electrical+and+electronics+engineers second+international conference on institute+of+electrical+and+electronics+engineers
measure lexical similarity method textual mapping nursing diagnosis spanish grammar%23NPH

grammar%23NPH advanced terminology coding+system ehealth grammar%23ENT
grammar%23NPH v. grammar%23NPR grammar%23NPH semantic+web machine reading fred semantic grammar%23ENT
resolve abbreviation their sens bioinformatics grammar%23ENT
grammar%23NPH grammar%23NPR grammar%23NPR terminology development populating from proceeding grammar%23ENT workshop on biomedical natural+language+processing
biomedical language processing be beyond grammar%23NPR molecular cell grammar%23ENT
grammar%23NPH grammar%23NPH v. grammar%23NPH assessment disease name entity recognition on corpus sentence grammar%23NPR bioinformatics vol. issue grammar%23NPR 3.
grammar%23NPH introduction bio entity recognition task at grammar%23NPR proceeding international joint workshop on natural+language+processing biomedicine application
grammar%23NPH grammar%23NPR corpus semantically corpus bio textmining bioinformatics vol. issue grammar%23NPR 1.
latent semantics named entity recognition application grammar%23ENT
grammar%23NPH grammar%23NPH s. grammar%23NPH
drug event detection tweet semi- supervised convolutional neural+network proceeding 26th international conference on international conference steering+committee
grammar%23NPH grammar%23NPH
semantic similarity based alignment between clinical archetype application observation international journal medical informatics grammar%23ENT
s. grammar%23NPH grammar%23NPH extract information from textual document electronic health record review recent research grammar%23NPR yearbook medical informatics grammar%23ENT grammar%23ENT
grammar%23NPR corpus named entity recognition spanish summary product characteristic grammar%23ENT
grammar%23NPH map protein disease terminology from grammar%23NPR mesh grammar%23NPR bioinformatics grammar%23ENT
survey named entity recognition classification grammar%23ENT
effective information extraction semantic affinity pattern relevant region proceedings grammar%23ENT on empirical method natural+language+processing computational natural language+learning vol.
script plan knowledge proceedings 4th on artificial+intelligence grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH disease ontology backbone disease semantic integration nucleic+acid research grammar%23ENT
biomedical named entity recognition use conditional random grammar%23NPH feature set grammar%23ENT
rule based entity recognition coverage swedish clinical text proceedings eight international conference on language resource evaluation
on representation role object-oriented conceptual modelling data grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
criteria evaluation biomedical named entity recognition grammar%23NPR bioinformatics grammar%23ENT
v. grammar%23NPR laguna grammar%23NPH grammar%23NPR grammar%23NPR sanidad y con sumo orchestrate network grammar%23NPR topo logical grammar%23ENT grammar%23ENT united+states+of+america table subset grammar%23NPR represent owl species owl species grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR be simplified add as primitive axiom number are append p readability not list where owl species list owl grammar%23ENT not list if list owl grammar%23ENT fragment
language subset grammar%23NPR axiom comment owl grammar%23NPR grammar%23NPR m p partially owl grammar%23NPR t c owl ql grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m pp owl ql grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR partial owl ql grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m p partially owl grammar%23NPR t c partial owl grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR mt partially owl grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR partial owl grammar%23NPR owl grammar%23NPR dl grammar%23NPR grammar%23NPR grammar%23NPR m p partial owl grammar%23NPR grammar%23NPR owl grammar%23NPR dl grammar%23NPR t c partial owl grammar%23NPR dl owl grammar%23NPR dl grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m p pp both partially owl grammar%23NPR owl grammar%23NPR dl grammar%23NPR grammar%23NPR grammar%23NPR mt partial owl grammar%23NPR owl grammar%23NPR dl grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR partial owl grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR partial owl grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR partial owl grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m p pp both partial owl grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m p pp both partial owl grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m p pp partial grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m p grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR m p pp grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR partial grammar%23NPR grammar%23NPR grammar%23NPR mt grammar%23NPR grammar%23NPR grammar%23NPR partial grammar%23NPR grammar%23NPR grammar%23NPR partial grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR gem grammar%23NPR grammar%23NPR grammar%23NPR theory interpretation continue previous example clearly grammar%23NPR grammar%23NPR interpret into grammar%23NPR sense that grammar%23NPR prove all consequence grammar%23NPR write as+follows interpretation grammar%23NPR grammar%23NPR con c semantics be that translation take con c grammar%23NPR prove all consequence grammar%23NPR
grammar%23NPR logic translation heterogeneous case i.e. when move from weak logic say owl dl more expressive one say full be specific strength grammar%23NPR illustrate by extend grammar%23NPR owl el antisymmetry weak supplementation be almost grammar%23NPR however still lack definition o eq as+follows logic grammar%23NPR ontology grammar%23NPR translation grammar%23NPR then forall x y thing p x y p y x x y grammar%23NPR forall x y thing not p y x exist z thing p z y not o z x grammar%23NPR definitional extension definitional extension be most basic tool ontology design cf. e.g. dolce be specified explicitly grammar%23NPR indeed be basic structuring mean organise mereotopological theory e.g. overlap o define terms part p terms part connection see table
definitional extension definition e.g. overlap o consist signature extension binary predicate grammar%23NPR x y together+with basic definition o term p. more formally theory grammar%23NPR signature be definitional extension theory signature if any model have unique expansion grammar%23NPR intuitively grammar%23NPR add neither additional constraint additional freedom interpretation grammar%23NPR but rather new symbol grammar%23NPR uniquely define terms symbol note be slightly different situation when previously introduce symbol even if unaxiomatised augment definition be general neither definitional conservative say grammar%23NPR be weakly definitional theory extension grammar%23NPR grammar%23NPR that mean that model re duct be bijection between model grammar%23NPR cap grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH if each realisation expand at most one realisation grammar%23NPR requirements express grammar%23NPR definitional extension annotate def weak definitional extension wdef complete previous incomplete definition grammar%23NPR as+follows logic grammar%23NPR ontology grammar%23NPR then forall x y thing o x y exist z thing p z x p z y grammar%23NPR forall x y thing eq x y p x y p y x grammar%23NPR result be complete specification grammar%23NPR where previously undefined symbol o eq augment their formal definition
network colimits entire network theory formally represent as grammar%23NPR network write as+follows network grammar%23NPR grammar%23NPR mp here theory grammar%23NPR list follow by defined theory grammar%23NPR connect various ontology node
network explore subtheories grammar%23NPR along dimensions language expressivity ontological soundness no conflict logical+statement among axiom in+fact grammar%23NPR combination technique base on computing colimits network compute colimit network grammar%23NPR grammar%23NPR result full grammar%23NPR
grammar%23NPR combine network construct allow exclude certain mapping ontology node therefore select language species compute maximal combination subtheories live that species note as discuss earlier that lead outside logic species some logic such as owl grammar%23ENT but not occur owl el sub-network
lemma consistency grammar%23NPR understand essential feature our network grammar%23NPR theory be important keep+track desired consequence theory b record counterexample property not follow from current theory both feature support by grammar%23NPR as discuss next
consequence keep+track desired consequence be easy grammar%23NPR continue example from above if proper part introduce rather than axiomatised directly then expect typical properties entail by definition this of+course depend on how precise axiomatised defining property part augment any given theory definition pp record consequence as+follows logic grammar%23NPR ontology grammar%23NPR then forall x y thing pp x y p x y not p y x then imply forall x y thing not pp x x grammar%23NPR forall x y z thing pp x y p y z pp x z grammar%23NPR forall x y z thing pp x y not pp y z grammar%23NPR semantics be that assume stated theory declared lemma sentence present logic namely three axiom list annotation imply be logical+consequence theory this ontology specification therefore introduce so-called proof obligation i.e. force connected automated reasoner this case grammar%23NPR vampire etc. prove axiom grammar%23NPR grammar%23NPR from theory specify before
consistency countermodels prove consistency theory be easy owl based simply use dl based owl automated reasoner theory check as+such be consistent according reasoner
as+soon+as move+away from owl establish consistency be extremely difficult in+fact hopeless current automatic reasoning capability as report even establish consistency classical extensional parthood break then current automatic reasoning support unless model finder strongly assist by human e.g. as oracle prescribe exact size model look+for
here outline how can within grammar%23NPR specify declaratively verify automatically consistency add several axiom existing theory related how prove that theory admit unintended model i.e. counterexample specific principle core tool do be theory interpretation b formal specification model model be declaratively specified as logical theory exactly one model up isomorphism be easy propositional+logic fix truth+value all proposition owl specify partial model help grammar%23NPR be standard method describe finite model grammar%23NPR describe also infinite model via free sort generation feature for+instance prove that grammar%23NPR be consistent specify finite model specification m assume use symbol co e p write following interpretation logic grammar%23NPR interpretation con grammar%23NPR m c co e e notice that only identical symbol need map explicitly by definition theory interpretation con be correct if only if every model grammar%23NPR reduce along morphism define con be model grammar%23NPR grammar%23NPR specify fixed model thus prove interpretaton correct axiom grammar%23NPR hold grammar%23NPR establish notice that same technique use show that theory t be both consistent admit counterexample specific principle by interpret t into grammar%23NPR violate principle note that heterogeneous feature grammar%23NPR here become powerful tool since counter model be specified more expressive logic e.g. than theory interpret e.g. owl el very simple example owl mereology leave+out antisymmetry grammar%23NPR say grammar%23NPR clearly have unintended model
specify non- well-founded model part grammar%23NPR solution problem establish consistency too large first-order theory propose be break+down global problem into series relative consistency proof use idea grammar%23NPR architectural specification prescribe how global model put+together from smaller model then small steps this global consistency proof consist construction as just describe orchestrate network grammar%23NPR topo logical grammar%23ENT grammar%23ENT united+states+of+america by interpret grammar%23NPR into this model use above technique give formal proof that grammar%23NPR admit such model but see philosophical argument why like drop antisymmetry
toward resolve conflict between language between representational expressivity therewith possible inference reasoning efficiency show clearly table range from owl profile profile have complexity satisfiability problem polynomial time highly un decidable when particular fragment choose ontology send appropriate automated reasoner such as cel elk owl el reasoner such as leo grammar%23NPR grammar%23NPH interface through hets and+so+on be engineering task now that option know therefore not pursue here move goalpost form conflict resolution decide about do conflict language feature language use represent ontology for+instance want methodically create ontology network by link several ontology where owl ontology use grammar%23NPR ontology o2+plc use grammar%23NPR however combine violate owl grammar%23ENT grammar%23NPR restriction thus lead un decidable language be conflict if requirement be remain within owl grammar%23ENT expressiveness modeller know about why cause then decide how resolve likewise algorithm determine optimise automated reasoner send ontology
main steps conflict resolution be automatically detect language violation report axiom use feature feature be beyond particular language assist describe consequence option have at disposal here focus on solve first two steps scope feasibility in+particular make proof-of-concept manageable owl species classifier be grammar%23NPR analyse owl file lists owl species be why violate owl classifier use version owl api owl in+order+to be able categorise owl species owl api grammar%23NPR all owl species extend grammar%23NPR class owl api keep+track axiom construct letters create justification reported expressivity therewith provide feature not available currently popular ontology editor for+instance merge ontology o grammar%23NPR into o have grammar%23NPR result ontology at+least owl grammar%23NPR grammar%23ENT full see fig. thus neither el grammar%23NPR profile that original be respectively section see fig. use resolve feature computational complexity conflict show axiom s delete in+order+to stay within particular profile as combine grammar%23NPR grammar%23NPR one s cause go beyond owl grammar%23ENT
step conflict resolution do be not possible perform this task automatically depend on modeller want at+best describe option grammar%23NPR github.com%2Fmuhummadpatel%2Fowl_classifier their consequence afore mention example theory no. be axiom reflexivity asymmetry on proper part make ontology interoperable other ontology represent owl grammar%23ENT
grammar%23NPR axiom transitivity on part proper part facilitate link ontology owl grammar%23NPR el grammar%23ENT
keep combine both set result owl full ontology ontology be un decidable be compatible ontology
option their consequence distil from set grammar%23NPR subtheories language consider their formal specification compute option from result generate some natural+language sentence be realm feasible however complicating factor be that not yet consider other knowledge represent ontology interfere this seemingly clean scenario but take+into+account leave future work
discussion many identifiable theory just few logic suffice demonstrate both perhaps untenable blow up ofmini theory why become confusing modeller yet also way cater grammar%23NPR be of+course possible further extend set mini- theory by consider also other dl other intermediate mereological theory e.g. extensional mereology em extend boundary and+so+on thus also provide mechanism verify computationally further extension grammar%23NPR such as grammar%23NPR while not be trivial at this stage represent grammar%23NPR previous section demonstrate that be feasible do chance modeller any two ontology make same choice as subset axiom include be small thus importing into other easily lead go beyond target+language as illustrate this overview present here clearly demonstrate option at disposal way deal within one framework
fine-grainedness feature negotiation be improvement over plain owl import import require one simply import whole ontology stick consequence also compare favourably with+respect+to related framework theory combination connection connection enjoy implementation owl as link properties because there link vocabulary element ontology only whilst here look+at combine theory also on object property characteristic
owl species classifier cover only all owl species not also negotiation expect be most useful nonetheless owl species classifier be proof-of-concept tool assist conflict resolution rather than end-user level but be first tool clearly organise necessary information be able do so
conclusion present network modular ontology grammar%23NPR relate various fragment grammar%23NPR mereotopological theory five sub- theory drive by grammar%23NPR grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH axiom axiom cause dl expressivity letter here reflexivity be language feature result r grammar%23NPR owl grammar%23ENT species ontology here merger grammar%23NPR grammar%23NPR offending axiom s ontology that cause ontology go beyond species here owl el brief explanation underlying description logic that ontology represent here grammar%23NPR figure screenshot owl species classifier output merger between grammar%23NPR grammar%23NPR
grammar%23ENT based owl species first second order logic
core step toward resolution feature conflict among owl species introduce owl classifier inform modeller about language violation in+order+to make informed decision
work include extend this network incompatible extension such as remove antisymmetry add atomicity as+well+as detailed specification countermodels formal consistency proof exploration logic specific sub-network
acknowledgment author like thank grammar%23NPH grammar%23NPH grammar%23NPH develop owl species classifier also thank grammar%23NPH fabian grammar%23NPH discuss grammar%23NPR feature anonymous reviewer valuable feedback corrections
n grammar%23NPR l grammar%23NPR toward geometry common+sense semantics complete axiomatization grammar%23NPR proceedings on artificial+intelligence grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPR ed.
architectural specification grammar%23NPR formal aspect computing grammar%23ENT 252+273
computational ontology parthood componenthood containment grammar%23NPR grammar%23NPR 382+387
grammar%23NPH grammar%23NPH categorical approach ontology alignment grammar%23NPR grammar%23NPR ceur-ws
antisymmetry non-extensional mereology philosophical quarterly grammar%23ENT 396+405

owl next step owl j grammar%23ENT 309+322
combine owl ontology use connection j grammar%23ENT https%3A%2F%2Fdoi.org%2Fdoi grammar%23NPR formal theory spatial representation reasoning biomedical ontology grammar%23ENT
grammar%23NPH grammar%23NPH ontology matching 2nd ed. springer
tool unification biology grammar%23NPH grammar%23ENT
institution abstract model+theory specification programming j association+for+computing+machinery grammar%23ENT 95+146
combine owl grammar%23NPR grammar%23NPR reasoning on environmental data grammar%23NPR grammar%23NPR grammar%23ENT
add convexity grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPR
grammar%23NPH grammar%23NPH region based theory space mereotopology beyond qualitative reasoning trend future direction grammar%23NPR grammar%23NPR ed. grammar%23NPR
semantic foundation medical information+system base on top-level ontology knowledge grammar%23NPR grammar%23NPR grammar%23ENT 107+115
grammar%23NPH
grammar%23NPR grammar%23ENT 452+457
from grammar%23NPR rwanda+defense+force owl making ontology language j grammar%23ENT
reasoning support expressive ontology language use theorem grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR springer
represent reason over taxonomy grammar%23ENT 91+110
represent mereotopological relations owl ontology grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR vol. springer 240+254 grammar%23ENT
grammar%23NPH e-connections grammar%23ENT
grammar%23NPH dolce grammar%23NPR grammar%23NPR san+francisco
grammar%23NPH carnap grammar%23NPR logical pluralism ontology design
grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH
pattern grammar%23NPR case blend grammar%23NPR grammar%23NPR vol.
cape+town south+africa grammar%23ENT
grammar%23NPH ontology library grammar%23NPR deliverable grammar%23NPR ver. grammar%23ENT grammar%23ENT

grammar%23NPH grammar%23NPH grammar%23NPH proof support common logic grammar%23NPR grammar%23NPR grammar%23ENT summer logic
grammar%23NPH grammar%23NPH grammar%23NPH road 50th birthday grammar%23NPH grammar%23NPR chapter distributed ontology modelling specification language grammar%23NPR
owl world+wide+web+consortium

spatial logic base on region connection grammar%23NPR grammar%23NPR grammar%23NPH 165+176
grammar%23NPH model part whole experience comparison grammar%23NPR grammar%23NPR 714+718
grammar%23NPH v. grammar%23NPR jr. grammar%23ENT reference ontology biomedical informatics foundational model anatomy j grammar%23ENT 478+500
modeling owl without restriction grammar%23NPR grammar%23NPR ceur-ws vol.
modeling anatomical spatial+relation description grammar%23NPR grammar%23NPR 779+783
neon methodology building neon deliverable grammar%23NPR
handbook spatial logic springer grammar%23NPH chapter spatial reasoning ontology part whole location 945+1038 grammar%23NPR uni- saleem%40informatik.uni leipzig.de grammar%23NPH uni- stadler%40informatik.uni leipzig.de grammar%23NPR qaiser.mehmood%40insight-centre.org grammar%23NPH jens.lehmann%40cs.uni bonn.de jens.lehmann%40iais.fraunhofer.de uni- ngonga%40informatik.uni leipzig.de axel.ngonga%40upb.de abstract ery containment be fundamental problem data+management main application be global query optimization a+number+of grammar%23NPR query containment solver grammar%23NPR develop recently best our knowledge ery containment benchmark bench be only benchmark evaluate containment solver however this benchmark contain x number synthetic query be by creator propose grammar%23NPR grammar%23NPR query containment benchmark generation framework framework be able generate grammar%23NPR containment benchmark from real grammar%23NPR query log e framework be exible enough generate benchmark varying size accord user-de ned criteria on most important grammar%23NPR feature consider query containment benchmarking e generation benchmark achieve use cluster algorithm compare grammar%23NPR query containment solver by use query containment benchmark generate from grammar%23NPR semantic+web dog+food query log in+addition analyze quality benchmark generate by grammar%23NPR
concept general reference metrics evaluation performance association+for+computing+machinery reference format grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPR ery containment benchmark generation framework proceeding knowledge capture united+states+of+america grammar%23ENT grammar%23NPR grammar%23ENT page
introduction ery containment be problem decide whether result set one query include result set another one be wellknown problem use a+number+of task such as devise query planner cache mechanism data+integration permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute commercial advantage that copy bear this notice full citation on rst page copyright third-party component this work honor
all use contact owner author s
grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s 123+4567+24+567+08+06 grammar%23ENT view maintenance determine independence query from update query rewriting etc. for+example suppose query grammar%23NPR contain query grammar%23NPR in+case grammar%23NPR execution be time expensive result grammar%23NPR obtain more from grammar%23NPR result set similarly be possible check if two query be equivalent by simply check their mutual containment e not growth data motivate considerable amount work on grammar%23NPR query containment best our knowledge grammar%23NPR ery containment benchmark grammar%23NPR be only benchmark design test grammar%23NPR query containment solver be benchmark contain x number query containment test by author while this benchmark contain variety test varying complexity number test be x all test be synthetic while grammar%23NPR serve purpose developer well provide developer limit insight on performance their system on real dataset in+addition this benchmark not allow user generate benchmark tailor towards use case
propose grammar%23NPR framework automatic generation grammar%23NPR query containment benchmark from real grammar%23NPR query log e framework be able generate benchmark customize by user terms number containment test structural feature grammar%23NPR query e.g. number triple number projection join variable join vertex degree etc. use grammar%23NPR construct e.g. union optional filter grammar%23NPR etc. for+example user opt create benchmark benchmark query super- query query query contain another query have minimal number triple equal less+than join vertices at+least union e framework generate desired benchmark from query log by use cluster method while consider selection criteria by user e contribution this work be as+follows present rst best our knowledge grammar%23NPR query containment benchmark generation framework from real query our framework allow automatic generation exible benchmark accord given use case criteria by user
identify key grammar%23NPR feature feature consider while design grammar%23NPR query containment benchmark then apply well-known algorithm grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR feasible feasible exemplar random selection select benchmark query grammar%23NPR grammar%23ENT submit grammar%23ENT et+al.
as assessment policy associate data result from computation diverse dataset implement within city data hub indeed assess how policy associate sensed data propagate result data+processing pipeline be important problem data consumer need check original source data need acknowledge because+of attribution requirement even whether form exposure re- distribution employ allow accord policy attach each individual piece data obtain from data hub research on policy model reasoning focus on problem licence compatibility composition however reasoning on policy propagation be necessary preliminary step any policy validation consistency check our work reuse model develop within research community for+example
discussion on grammar%23NPR action dependency how affect policy semantics include nonetheless best our knowledge first attempt analyse how policy propagate manipulation process be present our earlier paper introduce figure explanation propagation trace
notion in+order+to solve task automatically decide policy associate data+source need enforce output process that data+source involve establish fundamental connection between policy permission prohibition duty semantic+relation between two data object express thus community group https%3A%2F%2Fwww.w3.org%2Fcommunity odrl be possible derive that certain policy source need enforce on target allow model data+manipulation scenario as network data object make possible reason upon relation between data object apply
our earlier work focus on feasibility approach term knowledge+acquisition management scalability reasoner applicability end-to-end user scenario this paper go step further by perform user experiment in+order+to evaluate feasibility policy propagation as solvable problem hypothesis behind development system by rely on quantitative qualitative data+analysis method particularly grammar%23NPR comparison between automatic process manual perform by people typical skillet find data consumer processor publisher publisher carry+out this task realistic context
system at+a+glance role be support user assessment impact input+data policy on exploitation output data processes workflow consider case where food rate data release by trusted authority licence prohibit distribution use alongside public data about city road in+order+to assess good machine+learning approach among several option employ prediction good quality restaurant this task produce two type output set dataset about road label expected food quality rating b set dataset including detail about performance each algorithm test while prohibition distribution take+into+account when use former dataset same constraint not apply latter
system design work set reference knowledge+base data catalogue provides dataset general metadata including link associated policy set licence term condition and+so+forth
licence catalogue include set licence represent use
process catalogue define set process represent use
rule base develop manage as describe rule have form connection between atomic policy relation relation suppose propagate for+instance propagate dn grammar%23NPR odrl permission cc grammar%23NPR instruct reasoner propagate odrl permission cc grammar%23NPR whenever data+item be dn grammar%23NPR another so that cleaned item also have given policy
wiki theory grammar%23NPR version ontology www.w3.org%2Fns%2Fodrl%2F2%2Fodrl21 ontology purl.org%2Fdatanode%2Fns%2Fpropagating data policy user grammar%23NPH submit grammar%23ENT united+states+of+america system develop use owl reasoner conjunction grammar%23NPR rule engine by rely on grammar%23NPR policy graph system compute policy each node process grammar%23NPR resulting rdf graph query obtain policy output datanode moreover system generate explanation decision like figure trace lineage given policy highlight arc arc propagate block grammar%23NPR
methodology objective present study be evaluate extend be possible support user on take decision upon propagation policy whether be useful use system outline above assess two way by compare decision system one perform manually quantitative+analysis system accuracy by discuss issue raise disagreement qualitative+analysis user decision process further assess value user automatic support by ask participant set question concerning experience reasoning upon policy how relate processes through feedback questionnaire this section illustrate methodology employ study including design experiment criteria follow sampling user scenario data+collection analysis
experiment simulate set scenario data grammar%23NPR manager need take decision about policy need associate dataset derive from complex data+manipulation process perform by data hub infrastructure provide participant same knowledge as use by system ask perform a+number+of decision about policy propagation reference scenario that call data journey
each data journey input dataset associated licence present as+well+as formalised representation process user ask take decision about policy derivable from input licence also apply output data ask only decide whether policy propagate output ignore whether process violate policy whether policy be consistent each other user especially ask compare their choice system discuss potential disagreement
study conduct support tool develop guide participant process
apache grammar%23NPH jena.apache.org grammar%23NPR spinrdf.org grammar%23NPR detail about implementation reasoner find
system objective be compute set policy not check consistency policy do output set by rely on state+of+the+art deontic reasoner like use present work be only interested computing policy propagation in+relation+to action perform different process
decision participant compare one system
session start introductory phase where participant give short presentation about data hub task perform exemplify by tutorial data journey then participant leave face two data journey involve real data at end session user complete feedback questionnaire individually
single data journey be structured as+follows understand process participant ask become familiar data process describe rapid miner grammar%23NPR
understand input dataset this phase tool list dataset s select be input source s each dataset set permission prohibition duty grammar%23NPR
grammar%23NPR policy propagate output user ask indicate whether each apply output form likert question certainly+not probably not not know probably yes certainly yes
compare automatic reasoner this phase summarise figure choice user compare one system conflict be grammar%23NPR journey terminate after+all disagreement discuss
all case propagation trace propose user as explanation system decision see figure
can either agree representation but indicate that some relations behave differently for+example that dn grammar%23NPR propagate duty cc attribution b disagree on way data+flow represent indicate how be c change their opinion see explanation agree system case where user not decide be asked grammar%23NPR particular suggest answer following question be nature input+data be purpose process be intermediary step process be nature ouput data grammar%23ENT participant ask check their understanding nature action mention also document by tool accord their specification related semantic+web ontology often grammar%23NPR but also grammar%23NPR grammar%23NPR
be worth noting that while user request express opinion some degree uncertainty system always return answer policy propagate be cap grammar%23ENT submit grammar%23ENT et+al.
justify why believe not decide also give possibility abort task show why decision not make all case ask compare their decision system examine explanation propagation trace see figure in+order+to collect insight into fix system
participant scenario ten participant involve select among researcher phd student our university have background background include some data analytics skill absence specific legal expertise study participant intend evaluate system user user typically perform such task people be developer data scientist practitioner practitioner process reuse republish data realistically not assume have legal knowledge improve quality decision group participant team two person ask develop agreement before take action moreover introduce one intentional anomaly system check that user pay enough attention during study refer five team as+follows
effort allocate setting up realistic scenario comprise real data+source use conjunction real processes grammar%23NPR smart project collect large quantity data+source about city milton+keynes dataset use study be real data+source select from milton+keynes data hub data grammar%23NPR in+order+to select realistic workflow use our experiment search pre-existing processes instead design ad-hoc resource
grammar%23NPR be popular tool support user design articulated process by+means+of graphical+user+interface make good candidate selection our exemplary processes therefore explore open+source project available on grammar%23NPR search rapid miner process file select five workflow representative common data intensive task apply milton+keynes smart data design five scenario by associate real dataset from milton+keynes data hub from association data journey list table design each data journey have some exemplary characteristic
refer very common expedient crawl data out resource in+order+to setup textual corpus
journey be scenario already mention section where data+source use evaluate machine+learning approach refer extraction textual data from micro blog be many kind statistical operation perform on data be about calculation moving+average large part effort application rely on sensor put data preparation
aspect be+well reflect data journey
team give data journey each data journey two team later compare system twice on each scenario team between each other grammar%23NPR data hub datahub.mksmart.org grammar%23NPR miner https%3A%2F%2Frapidminer.com grammar%23NPR https%3A%2F%2Fgithub.com agreement analysis data journey allocate following latin square thus avoid assign two task same two group choose scenario scenario be complex enough b feasible within grammar%23ENT so people not be too fatigued c diverse enough term use case type operation perform although can not not formally claim scenario be representative set case because not have all case safely assume that be ecologically valid also license be different each scenario cover diverse range policy permission prohibition duty select from policy system overall experiment concern decision including policy as often policy be present more than one scenario
during experiment acquire three type data decision take by team system register by tool b record motivation behind decision in+particular about disagreement system borderline case c feedback about general difficulty task perceived user value our system obtain through questionnaire including nine closed end leading+question one single choice question see table figure author also attend study as supervisor provide support overall process for+example when user need clarification on semantics workflow action on usage tool but avoid influence their opinion on whether policy ought propagate not session audio be recorded as+well+as operation perform on screen preserve discussion small+talk occur motivate rationale behind user decision
perform two different type analysis agreement analysis quantitatively measure accuracy system b disagreement analysis focus on discuss quantitative result in+the+light+of user justification about controversial borderline decision qualitative way assess value user support policy propagation aggregate discuss response questionnaire that present section from point+of+view evaluate accuracy utility system quantitative data collect by tool expect produce following general result team agree system between each other therefore system be accurate b team agree each that system be not correct therefore task be feasible but system need improve c user not agree each other therefore task support automatically such decision be not grammar%23NPR accuracy analysis report section complement discussion on statistical significance qualitative+analysis conduct by focus on disagreement grammar%23NPR this last case in+fact not be able assess accuracy system be evidence that task not solve at+all at+least that knowledge+base use by system be not sufficient reason on policy propagation propagate data policy user grammar%23NPH submit grammar%23ENT united+states+of+america table data journey
website scraper
about milton+keynes scrape index locally
united+kingdom grammar%23NPR all right reserve page about milton+keynes policy permission re utilization reproduction distribution grammar%23NPR prohibition distribution grammar%23NPR reproduction grammar%23NPR grammar%23NPR re utilization extraction grammar%23NPR duty notice grammar%23NPR attribution
https%3A%2F%2Fgithub.com%2Fmtrebi%2Fsentimentanalyzer%2Ftree master process team food model food rate prediction
machine+learning technique compare process use data about food rating statistics about quality+of+life milton+keynes ward generate lift chart performance vector
grammar%23ENT grammar%23ENT average rating united+kingdom grammar%23NPR food establishment info rating term use policy permission grammar%23NPR distribution reproduction display extraction re utilization prohibition modify use duty attribution notice display
https%3A%2F%2Fgithub.com%2Fsamwar%2Ftree%2Fmaster%2Frapid miner training lift team cloud tag cloud from microblog post
post about milton+keynes collect process in+order+to obtain clean vector word associate occurrence score
miltonkeynes term use policy permission copy display prohibition give license sell transfer duty attribute
https%3A%2F%2Fgithub.com%2Fjccgit%2Frm-textmining-pubmed tree master grammar%23NPR team avg moving+average sensor record
moving+average plot from sensor record
term use policy permission aggregate anonymize archive derive index read use prohibition grammar%23NPR distribute give grammar%23NPR move sell transfer duty anonymize
https%3A%2F%2Fgithub.com%2Fbillcary%2Frapid miner tree master grammar%23NPR grammar%23NPR team clean sensor data cleaning workflow
process perform a+number+of clean operation on sensor stream link postcode in+order+to obtain dataset ready analysis
postcode location united+kingdom grammar%23NPR grammar%23NPR weather station grammar%23NPR weather station term use policy permission grammar%23NPR grammar%23NPR distribution reproduction display extraction re utilization use prohibition distribution give grammar%23NPR license transfer duty attribution notice inform grammar%23NPR
https%3A%2F%2Fgithub.com%2Fmartinschmitzdo grammar%23NPR process team borderline case select from quantitative result this aim transcribe note conversation occur during experiment from audio+recording tool from data derive set general theme about fundamental issue on policy propagation adopt method method be akin illustrate some exemplary case present theme discussion section
user feedback before analyse data journey how decision user relate behaviour our system be worth show feedback receive study conduct collect through questionnaire questionnaire pose some question about problem policy propagation assess value system user questionnaire complete by study participant individually table summarise nine closed end likert question while figure show result single choice question majority grammar%23NPR user feedback shading cell reflect distribution answer
answ unsure right answ
how difficult be take single decision on whether policy propagate output easy difficult grammar%23NPR do think have enough information decide yes no grammar%23NPR how difficult be reach agreement easy difficult strong technical skill absolutely require take this decision
agree yes no grammar%23NPR strong technical skill absolutely require take this decision even support automated reasoning agree yes no grammar%23NPR understand details process be fundamental take decision agree yes no grammar%23NPR how enjoyable be discuss decide on policy how propagate process very not grammar%23NPR how feasible sustainable do think be discuss decide on policy how propagate process feasible unfeasible grammar%23NPR how sustainable think be discuss decide on policy how propagate process support our system feasible unfeasible owner input+data process executor consumer processed data do together not do figure grammar%23NPR decide on policy propagate output process ipants our study believe that task can be difficult one however knowledge provide be adequate make informed decision decide whether policy propagate be possible even if not always trivial
agree on consider policy propagation problem problem not solve without understand details data+manipulation process therefore strong technical skill need involve objective be check whether user be positively invoved study assume that unengaged person not put enough effort on express opinion take thorough decision question focus on sustainability task user feedback on this matter spread our hypothesis be that two data journey be probably not enough understand how+much this task scale real setting however our system can effectively grammar%23NPR grammar%23ENT submit grammar%23ENT et+al.
data journey system decision h scrape food cloud avg clean tot user on take decision this feedback show that policy propagation be difficult problem although solve right knowledge model therefore tool support this task have good value user last question mean understand whether data hub manager actually decide on policy propagation turn+out that user think manager manager not solve issue alone but manager manager involve data owner process executor this task this conclusion reflect issue raise during study that discuss section
accuracy analysis this section show how decision make by user compare system decision take by system summarize table for+example data journey require check policy system decide propagate permission all prohibition duty table grammar%23NPR summarize result our study quantitative way value show two set including full number ratio consider all decision table grammar%23NPR grammar%23NPR then split permission table grammar%23NPR grammar%23NPR prohibition table grammar%23NPR grammar%23NPR duty table grammar%23NPR grammar%23NPR value first show each user study data journey each team aggregate each data journey average both team then as total consider decision from all data journey at+the+bottom data journey require from seven twenty-two policy analyse total seventy seven decision table grammar%23NPR show number decision each data journey column how+much team agree system be average value team on same data journey
agreement system be good distributed differently data journey team average ratio moreover this result support by high agreement rate between two team
observe that more than half case decision make same degree confidence 70%25 case user make sharp decision about whether policy propagate not total average be inspect table see that data journey show low agreement be 2. discuss next section low score on only show difference degree confidence decision be not especially relevant this global view table agreement analysis
total number decision agreement between system each team average agreement between team system agreement between team agreement between team only certainly yes grammar%23NPR no answer amount certainly yes grammar%23NPR no answer
on left indicate total while one on right show related ratio
all decision total scrape food cloud avg clean all b ratio c permission total scrape food cloud avg clean all d ratio narcotics+anonymous narcotics+anonymous e prohibition total scrape food cloud avg clean all f ratio g duty total scrape food cloud avg clean all h ratio narcotics+anonymous altough this aspect discuss when look+at specific class policy
grammar%23NPR grammar%23NPR only show result involve policy type permission average agreement between system user consider all decision be particularly data journey show low agreement also reflect number common sharp decision be low score compare agreement ratio prohibition duty observe table grammar%23NPR grammar%23NPR be sufficient consider at this stage how be much easy take decision on prohibition duty while permission where great source discussion disagreement system moreover decision about prohibition duty appear be sharp than one about permission as both agreement between team 12%29 choice strong confidence receive high score however on both type policy source disagreement be on data journey show that be case agreement complement this data statistical+analysis base on take+into+account possibility grammar%23NPR grammar%23ENT grammar%23ENT et+al.
given set containment query e detail analysis generated benchmark enable user select best t benchmark accord their needs compare state-of-the-art grammar%23NPR query containment solver use benchmark generate by our framework discuss result
be open-source available online along live grammar%23NPR
grammar%23NPR e query containment problem have rich literature relational+database this work focus on grammar%23NPR query containment
grammar%23NPR query containment problem reduce formula test use calculus while work provide theoretical proof model propose not provide any prototype implementation e alternation free twoway calculus grammar%23NPR be prototype implementation formula solver alternation free fragment calculus grammar%23NPR problem grammar%23NPR query containment also reduce formula test. e framework support projection blank node rdf reasoning on grammar%23NPR query another solver xml tree logic solver grammar%23NPR present perform static analysis on xpath query by transform into calculus be solver use decision problem such as containment equivalence overlap coverage
et+al. study fundamental problem grammar%23NPR query containment form subsumption relation grammar%23NPR be grammar%23NPR query containment solver base on theoretical study present be implementation support conjunctive optional query no projection e complexity well designed grammar%23NPR query containment restrict conjunction optional operator extension by union and%2For projection study eir complexity result range from np completeness un decidability e problem grammar%23NPR query containment expressive class navigational query call study finally reduce grammar%23NPR query containment problem problem decide whether sub graph isomorphism exist between normalize algebra expression tree view sub- tree user query
best our knowledge grammar%23NPR be only grammar%23NPR query containment benchmark compare grammar%23NPR grammar%23NPR grammar%23NPR containment solver e benchmark contain three test suite grammar%23NPR no projection grammar%23NPR be test suite contain total containment test+case design containment contain conjunctive query no projection
test+case check containment between two query union conjunctive eries projection grammar%23NPR be test suite comprise test+case test contain projection test contain union test contain both union grammar%23NPR rdf reasoning grammar%23NPR ere be total test+case this category be category query test query containment solver rdf reasoning comparison previous two test suite query this category have low github.com%2Faksw%2Fsqcframework complexity especially terms number triple number join variable be test suite make use small schema test correctness solver
provide good variety terms number triple number variable number join size ontology in+addition also take type graph connector union optional filter etc. type ontology no schema rdf owl etc. into account however benchmark be synthetic contain x number test+case as acknowledge by author used schema be not realistic user be not able generate containment benchmark pertain use case in+addition be likely that real query issue by user from synthetic query present grammar%23NPR grammar%23NPR address drawback allow user generate benchmark from grammar%23NPR query log i.e. real query post by real agent
preliminary this section key concept necessary understand subsequent section this work denote set solution mapping be as result grammar%23NPR query q over rdf graph g write q g according+to arity query as arity query answer note note when be outer projection i.e. explicitly mention query be by distinguished variable otherwise be by all free variable query given rdf graph say query contain another query if all answer include other query
grammar%23NPR query containment give two query grammar%23NPR grammar%23NPR same arity grammar%23NPR be grammar%23NPR grammar%23NPR denote by grammar%23NPR v grammar%23NPR if only if grammar%23NPR g grammar%23NPR g every rdf graph g. call grammar%23NPR as super- query grammar%23NPR as sub- query
mention that structural grammar%23NPR query feature e.g. number bgp number triple number join vertices number projection variable etc. be important consider while design query containment benchmark also consider feature while generate automatic grammar%23NPR containment benchmark following grammar%23NPR query feature formally
basic graph assume be pairwise disjoint set set iris b set blank node l set literal v set variable tuple from l v b %C3%97 v %C3%97 grammar%23NPR b be triple subset thereof i.e. set triple call
model remaining feature use model propose represent any given grammar%23NPR query as generalization directed graph hyperedge join any number vertices our case every hyperedge capture triple e subject triple become source vertex hyperedge predicate object triple become target vertices
consider query figure hypergraph illustrate figure unlike common grammar%23NPR representation subject object triple be grammar%23NPR grammar%23ENT grammar%23ENT united+states+of+america by edge our hypergraph based representation contain node all three component triple as result capture join join involve predicate triple formally our hypergraph representation be as+follows
directed hypergraph bgp e hypergraph representation bgp b be directed hypergraph hg v e vertices be all component all triple b i.e. v s p o b s p o contain hyperedge s t e every triple s p o b such+that s s grammar%23NPR p o
query select grammar%23NPR drug t t l e where drug db c t dbc mic
drug db grammar%23NPR d
grammar%23NPR r d f type kegg drug
grammar%23NPR b o r d f xref d
grammar%23NPR p u r l t t l e t t l e. title xref type grammar%23NPR drug join grammar%23NPR target grammar%23NPR title drug id drug m ic
c at
grammar%23NPR figure dh representation grammar%23NPR query give+in list grammar%23NPR ignore succinctness
e representation complete grammar%23NPR query as dh be union representation query bgp as example dh representation query figure show figure
on dh representation grammar%23NPR query can following feature grammar%23NPR query grammar%23NPR join vertex every vertex v v such hypergraph write grammar%23NPR v grammar%23NPR v denote set incoming outgoing edge respectively i.e. grammar%23NPR v s t e v t grammar%23NPR v s t e v s if grammar%23NPR v grammar%23NPR v call v join vertex
join vertex degree base on dh representation grammar%23NPR query join vertex degree vertex v be grammar%23NPR v grammar%23NPR v grammar%23NPR v where grammar%23NPR v resp grammar%23NPR v be set incoming resp. outgoing edge v
grammar%23NPR benchmark generation this section present benchmark generation process grammar%23NPR rst discuss selection real query use benchmarking then discuss key query feature consider while generate benchmark use cluster method nally discuss generation benchmark
grammar%23NPR our framework take set query as input select required sample query accord user-de ned criteria e input query commonly originate from query log grammar%23NPR endpoint manually provide by user this work aim generate benchmark from real user query this end use eries grammar%23NPR dataset dataset provide real query extract from log public grammar%23NPR endpoint e grammar%23NPR dataset provide various statistics e.g. number join number triple number join vertices etc. about query consider important discuss next section benchmark generation currently grammar%23NPR project contain dataset extract from grammar%23NPR grammar%23NPR feature query containment benchmark comprise query test varying complexity hence consider following query feature while generate containment benchmark number entailment sub- query unlike qc grammar%23NPR where superquery commonly have one sub- query be possible given superquery have multiple sub- query real grammar%23NPR query log be hence possible that given containment solver successfully identify one sub- query fail identify another sub- query same super- query
number projection variable as point+out by this dimension be not importance consider while design containment benchmark be be because query containment check if answer solution mapping projection variable one query contain answer another query
e high number projection variable hard check containment e number projection variable query show figure be i.e. drug title
number bgp bgp be sequence triple optional filter add other graph such as union optional terminate grammar%23NPR be feature consider by grammar%23NPR test suite previous work point+out that grammar%23NPR optional be most constructor grammar%23NPR query containment check e number bgp query give figure be
total number triple pattern e total number triple query have direct relation how expensive containment check be triple hard query containment check e number triple query give figure be
max. min. bgp triple pattern bgp be most essential important building+block grammar%23NPR query virtually every grammar%23NPR related system support be feature enable select query by size their bgp as example query figure have single bgp triple hence min. max. number bgp triple be
grammar%23NPR dataset be available from grammar%23NPR website grammar%23NPR www.w3.org%2Ftr%2Fsparql11-query%2F%23basicgraphpa_ernsk-cap grammar%23ENT grammar%23ENT et+al.
number join vertices be feature also consider by e answer size given query vary depend on join vertices query be possible that given triple have large+number solution mapping exclude performing join solution mapping another triple e number join vertices query give figure be i.e. variable grammar%23NPR id drug
mean join vertex degree e number solution mapping pertain given projection variable be directly by number join their degree erefore be important consider query feature containment benchmarking query give figure degree join vertices grammar%23NPR id drug be respectively e mean join vertex degree become
number grammar%23NPR feature e eries grammar%23NPR store additional grammar%23NPR feature such as use grammar%23NPR grammar%23NPR filter limit value grammar%23NPR by grammar%23NPR group by offset aggregate function e.g. count cma+2071 max etc. service optional union property path etc. make count grammar%23NPR operator function use as single query dimension e number grammar%23NPR feature query give figure be i.e. grammar%23NPR limit clause
e grammar%23NPR dataset contain statistics about query mention above e only feature that be missing be number entailment i.e. number sub- query every super- query dataset i.e. grammar%23NPR not store containment relationship among query make use existing grammar%23NPR query containment solver actually running query identify containment relationship among query grammar%23NPR containment result by actually execute both superand sub- query over underlying dataset programmatically check if result sub- query indeed include result super- query our benchmark only consider grammar%23NPR query query participate at+least one containment relation i.e. query be either super- query sub- query another query dataset
benchmark generation our benchmark generation problem be as follow grammar%23NPR benchmark generation problem let l represent set input grammar%23NPR query our goal be select n superqueries that best represent l and%2For more diverse n l
e user provide input grammar%23NPR dataset required number n super- query selection criteria consider generated benchmark en benchmark generation carry+out following four main step select all super- query along required feature from input grammar%23NPR dataset grammar%23NPR feature vector their normalization selected grammar%23NPR n number cluster from superqueries select single most representative super- query from each cluster include nal benchmark
since number sub- query given selected superquery be greater equal number query containment test benchmark will greater equal number superqueries now discuss step in+detail
super- query selection along required feature from grammar%23NPR dataset prefix l s q h t t p l s q. org vocab select grammar%23NPR sup count grammar%23NPR sub as e n t l m e n t s p r o j v r s bgps t p s j o n v e r t c e s grammar%23NPR grammar%23NPR les grammar%23NPR count f e t u r e as n o f e t u r e s sub l s q s grammar%23NPR n t l e d sup
sup l s q h s s t r u c t u r l f e t u r e s s f
s f l s q p r o j e c t v r s p r o j v r s
s f l s q bgps bgps s f l s q t p s t p s
s f l s q j o n v e r t c e s j o n v e r t c e s
s f l s q grammar%23NPR grammar%23NPR
s f l s q grammar%23NPR les grammar%23NPR les
s f l s q grammar%23NPR
s f l s q u s e s f e t u r e f e t u r e filter s t r sub s t r sup selection super- query since grammar%23NPR contain rdf dataset select super- query along statistics by simply use grammar%23NPR query our framework retrieve set super- query along required feature discuss previous section by use grammar%23NPR query give+in list e result this query execution store map use subsequent benchmark generation steps section show how this query be generate query containment benchmark
feature vector e cluster generation algorithm explain next section require distance between query compute this end query from input grammar%23NPR dataset map vector length store corresponding query feature discuss section min. max
from section are two feature assume number sub- query be feature vector query give figure be ensure that dimension high value e.g. number grammar%23NPR feature not bias query selection benchmarking normalize query feature vector value between be be ensure that all query locate unit hypercube this end individual value every feature vector divide by overall maximal value all vector that query feature suppose value represent maximal feature vector i.e. each individual feature value be maximum over all vector feature vector query give figure become
generation cluster as next step generate n cluster from given input grammar%23NPR query represent as normalize feature vector this step use existing well-known algorithm feasible feasible exemplar grammar%23NPR grammar%23NPR random selection allow generate required x number cluster grammar%23NPR mean that apply rst remove outlier query then applied grammar%23NPR generate required number cluster moreover note that our framework be exible enough grammar%23NPR grammar%23ENT grammar%23ENT united+states+of+america integrate any other clustering algorithm algorithm allow generate x number cluster our framework provide detailed analysis explain section benchmark generate use different method thus allow user pick good method required containment benchmark
selection most representative eries finally perform selection single prototypical query from each cluster
be step be exactly same+as perform grammar%23NPR each cluster s compute centroid c c be average feature vector all query s. grammar%23NPR compute distance each query s c select query query have minimum distance include resulting benchmark
that our framework also allow benchmark use random selection query randomly select
allow generate benchmark use grammar%23NPR cluster however agglomerative clustering not allow generate x size benchmark e aforementioned benchmark website show complete detail how generate benchmark use available method use command+line+interface from source+code
benchmark mention before our framework allow benchmark generation accord criteria by user
be do by simply specialize query give+in list
imagine user be interested generate benchmark following feature e number projection variable super- query be at most e number gbp be great than number triple be greater than e benchmark select from most recently executed query instead consider whole grammar%23NPR query benchmarking
e query selection such personalized benchmark give+in list note that grammar%23NPR dataset also contain agent information thus query containment benchmark from single agent query be also possible be be particularly helpful when developing caching mechanism grammar%23NPR query particular user system
evaluation result our evaluation comprise two main part first provide analysis benchmark generate by available method second part our evaluation use selected benchmark generate by our framework compare existing grammar%23NPR query containment solver their query containment precision execution+time require perform query containment task
benchmark not solely comprise query containment test be very similar their feature grammar%23NPR diversity benchmark query be important ensure overall quality generated benchmark this end analyze benchmark generate by our framework terms how well represent input grammar%23NPR query how diverse be benchmark query generate by our framework from statistics point list benchmark personalization prefix l s q h t t p l s q. org vocab prefix prov h t t p grammar%23NPR org ns prov select grammar%23NPR sup count sub as e n t l m e n t s p r o j v r s bgps t p s j o n v e r t c e s grammar%23NPR grammar%23NPR les grammar%23NPR count f e t u r e as n o f e t u r e s sub l s q s grammar%23NPR n t l e d sup
sup l s q h s s t r u c t u r l f e t u r e s s f
s f l s q p r o j e c t v r s p r o j v r s l s q bgps bgps l s q t p s t p s l s q j o n v e r t c e s j o n v e r t c e s l s q grammar%23NPR grammar%23NPR l s q grammar%23NPR les grammar%23NPR les l s q grammar%23NPR l s q u s e s f e t u r e f e t u r e
s t r sub s t r sup personalization benchmark sup l s q grammar%23NPR grammar%23NPR
grammar%23NPR grammar%23NPR
p r o j v r s bgps t p s order by grammar%23NPR grammar%23NPR limit view rst measure be equivalent select sample from population have characteristic here mean standard+deviation similar original population like grammar%23NPR measure terms how+much mean standard+deviation feature benchmark query deviates from input grammar%23NPR query call resp. mean resp. standard+deviation given distribution ith feature say distribution let b be benchmark extract from set query l. use two measure compute between b l i.e. error on means grammar%23NPR deviation grammar%23NPR e 1%2C000 j l b grammar%23NPR 1%2C000 j l b given that harmonic+mean be biased towards small value composite error estimation grammar%23NPR also call similarity error by use harmonic+mean e grammar%23NPR e e grammar%23NPR e grammar%23NPR e diversity score d be average standard+deviation query feature k include benchmark b d 1%2C000 j b note note given benchmark smaller composite error represent overall input query log high diversity score more diverse query include benchmark be important note that use grammar%23NPR grammar%23ENT grammar%23ENT et+al.
feature value while computing grammar%23NPR d thus value lie between
experimental setup grammar%23NPR dataset experimental evaluation this work use dataset
grammar%23NPR dataset total be available from project website use benchmark generation add query containment relationship choose dataset because use feasible evaluation
composite error diversity in+order+to compare quality benchmark generate by method generate benchmark size number super- query from benchmark size from e benchmark use grammar%23NPR as+well be interested generate small sized benchmark in+order+to allow user perform quick comparison query containment solver base on reasonably small number containment test see table in+addition want compare clustering algorithm various benchmark size
generation method generate above mentioned benchmark generate use feasible grammar%23NPR grammar%23NPR random selection grammar%23NPR provide detailed analysis their composite error diversity score
containment solver evaluate four grammar%23NPR query containment solver our evaluation grammar%23NPR algebra grammar%23NPR grammar%23NPR grammar%23NPR best our knowledge be state+of+the+art available grammar%23NPR containment solver
run query containment solver default
containment evaluation select superqueries benchmark generate by grammar%23NPR from dataset
be benchmark contain total grammar%23NPR query containment test ref. see figure
perform experiment on ubuntu-world+forum+of+civil+society+networks grammar%23ENT grammar%23NPR machine grammar%23ENT grammar%23ENT
as mention analyze benchmark generate by our framework similarity error diversity score while comparison grammar%23NPR query containment solver choose two standard metrics ery mix hour grammar%23NPR along standard+deviation precision containment solver terms number correctly containment test e metric be related how+long particular containment solver take check query containment between two query be metric generally use triple stores benchmark evaluation e second metric tell accuracy containment solver note each benchmark call test suite query mix set query timeout grammar%23ENT e query abort maximum time grammar%23ENT 99%25 query use as query containment runtime all query time out all data code benchmark result repeat our experiment along complete evaluation result be available at project website
experimental result composite error diversity figure show composite error diversity score benchmark generate from by use be currently supported method terms similarity composite error show figure grammar%23NPR figure grammar%23NPR grammar%23NPR selection outperform other method grammar%23NPR overall all generated benchmark similarity error be 2%25 small than grammar%23NPR be 28%25 small than random selection turn be 18%25 small than grammar%23NPR be 9%25 small than grammar%23NPR exemplar terms diversity score show figure grammar%23NPR figure grammar%23NPR grammar%23NPR exemplar outperform other method feasible exemplar overall all generated benchmark diversity score be 4%25 large grammar%23NPR be 21%25 large than grammar%23NPR turn be 6%25 large than grammar%23NPR be 47%25 large than random selection
summary have following key observation if aim be test query containment solver benchmark that good overall input query log then grammar%23NPR be good method however downside this approach be that benchmark query be not diverse be means mean be highly possible test be too easy even bias overall result best suited query containment solver grammar%23NPR smaller error be due fact that rst remove outlier query from population leading grammar%23NPR select more representative+sample distribution if aim be test query containment solver benchmark benchmark contain diverse test then feasible exemplar be good method e reason grammar%23NPR exemplar high diversity score be because+of method follow select exemplar rst select middle query as rst exemplar multi dimensional space e second exemplar be query have long distance from rst exemplar similarly third exemplar be one have long distance from rst two exemplar and+so+on be means feasible exemplar be method always give good possible diverse query benchmark
on down this benchmark may not well re ect overall query log our nal observation be interesting two measure be inverse each general if method be good similarity error perform bad diversity score vice+versa
e reason select outlier be always more diverse but not represent normal+distribution
show number query containment test corresponding benchmark generation time benchmark size dataset note refer number super- query be size benchmark see number containment test two query check containment benchmark be larger than number super- query be be simply because each super- query have multiple sub- query include all sub- query given super- query into resulting benchmark overall clustering method have comparable benchmark generation time from distribution over topic grammar%23NPH upm.es grammar%23NPH jluisred%40amazon.com united+kingdom grammar%23NPH upm.es abstract ere be many scenario where want nd pair textually similar document large corpus e.g. researcher do literature review r%26d project+manager analyze project proposal programmatically discover connection help expert achieve goal but brute force pairwise comparison be not computationally adequate when size document corpus be too large some algorithm literature divide search space into region contain potentially similar document document later process separately from rest in+order+to reduce number pair compare however this kind unsupervised method still incur high temporal cost this paper present approach approach rely on result topic model algorithm over document collection as means identify small subset document where similarity function then compute be approach prove obtain promising result when identify similar document domain publication compare our approach against state+of+the+art cluster technique topic modeling algorithm result suggest that our approach outperform other analyzed technique terms
mathematics computing probability statistics information system document topic model applied computing document+management text+processing keyword topic model semantic similarity large-scale text analysis scholarly data introduction give huge amount information about any domain produce capture daily become crucial provide mechanism automatically identify element element bring value involved agent general consumer expert be work support by project reference grammar%23NPR nanced by spanish ministry grammar%23NPR co- nanced by grammar%23NPR
make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute commercial advantage that copy bear this notice full citation on rst page copyright component this work own by than association+for+computing+machinery honor abstract credit be copy otherwise republish post on server redistribute lists require prior permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT united+states+of+america association+for+computing+machinery grammar%23ENT grammar%23ENT company investor discard noisy relevant information information present form textual document make necessary expert browse through text nd relevant data way explore knowledge inside collection document be by move from one information element base on certain criteria relate be approach require calculate similarity matrix all possible comparison between element so later select most pertinent since compute n %C3%97 n matrix take o grammar%23NPR time obtain all possible pair similarity large collection document be unfeasible because+of exponential cost compare every pair element
work derive from real need domain digital+library where target task nding relations among text base on similar content inside corpus contain 7%2C487 digital book 97%2C532 chapter 104%2C960 document total since time consume calculate similarity score between two document be t grammar%23ENT grammar%23NPR cpu%402.30ghz grammar%23ENT server total time compute all combination over whole corpus go+up around day consider that other task leverage entire collection such as train topic model only require grammar%23ENT execute calculate similarity score between pair document become not when make+sense big collection document
one possible way nding similarity based link between pair document be process item follow annotation technique entity keyword etc technique allow machine programmatically leverage on their content 2%29 create vectorial representation base on feature each document compare follow some distance divergence function
reduce execution+time some approach introduce mechanism mainly cluster algorithm preelection method alleviate problem make this calculation over whole set pair collection however method be still quite costly
novel cluster technique base on topic model distribution propose this paper in+order+to reduce required time nd relation between document large corpus textual document without compromise
leverage on as representational model in+particular as way make this process nding relation among document corpus more agile computationally feasible
topic modeling technique be statistical method analyze word original text discover theme theme run through base on insight further study how subject connect each other how change over time originally develop as text+mining tool grammar%23NPR grammar%23ENT et+al.
model now use detect instructive structures data such as computer+vision classify image connect image caption build image hierarchy population+genetics social+network reduce each document vector compose by x set real+number represent probability+distribution given topic
main advantage be that do not require any prior annotation label document e topic emerge as hide structures from analysis original text e topic produce by topic modeling technique be cluster similar word topic model capture this intuition mathematical framework allow examine set document discover base on statistics word contain each topic be be topic balance each document ose topic much+more intuitive yet sophisticated way perform knowledge discovery task big collection document
contrast existing unsupervised approach base on centroid density measure our algorithm rely on outcome assign each document cluster without consider other element corpus only take o n time compute all cluster
following section provide overview problem solve along existing solution grammar%23NPR detailed description our algorithm give section then section experimentally verify our clustering algorithm use real data demonstrate that our approach be competitive enough against both centroid base density based clustering baseline finally most relevant result conclusion present together+with some future line work section
background traditional retrieval task over large collection textual document highly rely on individual feature like term frequency tf israel+defense+forces however new way document base on automatic generation model surface main subject cover corpus develop during recent year
topic modeling algorithm be statistical method analyze word original text discover theme theme run through how theme connect each other how change over time
topic model not require any prior annotation label document e topic emerge as hide structures from analysis original text ese structure be topic distribution per- resource topic distribution per- resource perword topic assignment in+turn topic be distribution over terms be biased around word associate single theme be interpretable hidden structure annotate each resource collection annotation use perform deep analysis about relationship between resource this way topic modeling provide algorithmic solution organize annotate large collection textual document accord their topic
e simple generative topic model be be other topic model such as be part eld know as probabilistic modeling ey be well-known latent variable model high dimensional data such as representation textual data any other count based data representation
have roots propose as generalization be also by generative framework avoid over- ing issue issue observe
be statistical model try capture intuition that document exhibit multiple topic each document exhibit each topic proportion each word each document draw from topic where selected topic choose from per- document distribution over topic all document collection share same set topic but each document exhibit topic proportion document represent as vector count grammar%23NPR component grammar%23NPR be number word vocabulary each document corpus model as mixture grammar%23NPR topic each topick be distribution over vocabulary grammar%23NPR word formally topic be multinomial distribution over word x vocabulary represent some concept each topic draw from distribution parameter while each document mixture sample from distribution parameter ese two prior also+known+as hyper parameter estimate follow some heuristic
distribution be continuous multivariate probability+distribution parameterize by vector positive real element sum be continuous because relative likelihood random+variable take on given value describe by probability+density+function also be multivariate because have list variable unknown value in+fact distribution be conjugate prior categorical distribution multinomial distribution
restrictive cluster model where each document assign one cluster allow document exhibit multiple topic moreover since be unsupervised topic cover set document discover from own corpus mixed membership assumption lead sharper estimate word co-occurrence
similarity measure document topic model feature vector be topic distribution express as vector probability take+into+account this premise similarity between two topic based resource base on distance between their topic distribution also see as two probability mass function commonly used metric be grammar%23NPR grammar%23NPR divergence however present two major problem when topic distribution be zero grammar%23NPR divergence be not be not symmetric do not t well semantic similarity measure measure be usually symmetric
grammar%23NPR divergence solve problem consider average distribution as below j p q grammar%23NPR pi log pi pi qi grammar%23NPR qi log qi qi pi grammar%23NPR clustering from distribution over grammar%23ENT united+states+of+america where k be number topic p q be topic distribution transform into similarity measure as+follows sim j grammar%23NPR d j grammar%23NPR p q where grammar%23NPR d j be document p q topic distribution
distance be also symmetric use along j divergence various elds where comparison between two probability+distribution require p q grammar%23NPR pi qi transform into similarity measure by subtract from such that zero distance mean max. similarity score vice+versa grammar%23NPR grammar%23NPR d j p q approach our algorithm draw inspiration from other clustering technique divide initial space element into small sub- group where complexity calculate all possible distance reduce existing unsupervised approach base on centroid density measure require make comparison between element nd group similar element collection ey normally follow iterative methodology produce nal solution base on calculate distance between element inside each intermediate state approach need calculate all possible distance between element take o grammar%23NPR time n %C3%97 n matrix at make impossible apply such technique on large collection document since cost compare each element escalate quickly big volume data clustering task task only take linear time discover cluster alleviate this problem for+example method method not require any other data except element information assign item corresponding cluster take o n time compose group
e method need take+advantage both vectorial representation document similarity measure use relate corpus since representational model consider base on more on method leverage on particular behavior distribution describe each document by density vector where sum all probability value be equal analyze relation between topic compose topic distribution become more important than compare their probability value another topic distribution
hypothesis be that give collection topic distribution unsupervised high precision linear computing time perform by consider only topic distribution each document without need further compare other document distribution
algorithm compare terms cost cost base on number pairwise similarity value grammar%23NPR handle relevance measure such as precision recall try measure overall balance between cost more detail about measure include section
trend based clustering topic distribution formalize as probability+distribution follow distribution so their probability value sum
this way relevance topic be at+the+same+time relevance item distribution
rst approach name trend on distribution based clustering grammar%23NPR consider change relevance i.e. probability value topic instead directly rely on score associate given topic distribution express oscillation between topic weight consider x order between e order be any as+long+as remain constant all distribution
probability vector compose by n density value translate trend expression make+out n trend value such as upward downward sustain be trend expression identify cluster distribution fall into therefore corresponding item belong grammar%23NPR be as grammar%23NPR p t where ti when pi pi ti when pi pi ti when pi pi for+example give distribution assigned cluster be t e rst value be because be greater than other value
ranking based clustering propose clustering technique name ranking on distribution based clustering grammar%23NPR only consider top n topic from ranked list probability+distribution classify similar topic distribution base on focal document selection propose by validate based similarity algorithm against human perception similarity grammar%23NPR be as grammar%23NPR p r where r grammar%23NPR grammar%23NPR j p grammar%23NPR grammar%23NPR be base on assumption that high weighted topic have high rest topic terms calculate distance when compare continuous multivariate probability+distribution since similarity measure section base on probability+distribution orient determine uncertainty distribution when mixture probability+distribution consider as case topic model top n distribution i.e.
most relevant topic be allow grouping similar distribution take+into+account above consideration grammar%23NPR algorithm topic distribution accord only n high probability value for+instance give following topic distribution grammar%23NPR assigned cluster be from grammar%23NPR because be topic high cap grammar%23ENT et+al.
based clustering variant previous algorithm name on distribution based clustering grammar%23NPR also aim discover most representative topic topic help group similar topic distribution while grammar%23NPR base on x number topic grammar%23NPR base on cumulative sum weight high topic e number topic now dynamically determine by threshold once this threshold reach no more topic consider grammar%23NPR be as grammar%23NPR p c where c ci ci grammar%23NPR grammar%23ENT w grammar%23NPR size grammar%23NPR w cumulative weight threshold
consider grammar%23NPR algorithm consider cumulative weight threshold following topic distribution grammar%23NPR e assign cluster be come+up this cluster ranked list topic base on their weight be rst calculate grammar%23NPR en sum weight accord order describe by grammar%23NPR perform when accumulated sum be greater than threshold topic take+part sum select label cluster this case cumulative weight threshold be therefore use only rst two topic exceed threshold w experiment this section present experimental setup evaluate our trend base grammar%23NPR ranking base grammar%23NPR cumulative rankingbased grammar%23NPR cluster approach consider both j divergence case distance as similarity measure describe dataset baseline algorithm algorithm use comparison
dataset use two dataset evaluate performance algorithm e rst dataset grammar%23NPR grammar%23NPR be synthetic generate dataset sample k probabilistic distribution from randomly k dimensional selector base on distribution be imply that all probability must sum sample point e number point from this mixture distribution be n
e second dataset create from collection research paper publish journal ey retrieve from springer api by use grammar%23NPR framework topic model base on create from e sample also compose by n document
model train from dataset by use criteria describe by k k n where k be number topic n be number document since both dataset contain document n hyperparameters assign as follow number topic be x k further tuning be not crucial this evaluation process because not focus on quality model but on when calculate similarity from their representational distribution
similarity value group by frequency grammar%23NPR similarity reshold since be no criteria select threshold inside distance score spectrum allow determine when two document be similar decide study distribution similarity value calculate from all pairwise comparison figure result grouping all similarity by two most representative decimal i.e. rst two decimal similarity value show
en polynomial function red+line approximate describe trend value this function similarity score emerge as global minimum use ltering out similar document pair
baseline compare performance grammar%23NPR grammar%23NPR grammar%23NPR algorithm against following baseline k-means as centroid based clustering approach
grammar%23NPR as density based clustering approach
random randomly select r from dataset initially k-means randomly compose set centroid assign each point sample near cluster base on distance measure en new set centroid calculate from previous accord assigned point be process repeat until set centroid not change between consecutive iteration maximum number iteration reach e scalable k-means approach use our experiment be improved version k mean obtain initial set center ideally close optimum solution e algorithm implement at library use experiment base on empirical result good be k number o f topic grammar%23NPR widely known density based algorithm be grammar%23NPR compose cluster from neighborhood each point consider at+least minimum number point given radius require specify radius point neighborhood eps minimum number point neighborhood grammar%23NPR
on empirical result good result obtain following eps grammar%23NPR commons.apache.org%2Fproper%2Fcommons-math%2Fe_icient clustering from distribution over grammar%23ENT united+states+of+america e random algorithm take as input parameter m randomly divide dataset into m equal sized group similar document evaluation m set number topic dimension dataset
proposed algorithm take+into+account empirical result grammar%23NPR algorithm set use grammar%23NPR high topic cumulative weight threshold grammar%23NPR algorithm set
measure gold+standard create each dataset distance metric considered ey create by calculate all pairwise similarity from their document since n %C3%97 n similarity matrix require o grammar%23NPR time calculate selected size dataset not be too large n
consider three measure evaluate our algorithm with+respect+to baseline cost base on number similarity score calculation require by algorithm cost grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR e grammar%23NPR correspond number similar document obtain from use threshold score previously mention section e grammar%23NPR correspond cartesian+product existing document grammar%23NPR n n 000 000 grammar%23NPR correspond number similarity calculate by algorithm
base on precision recall express quality algorithm e f f ectiveness grammar%23NPR grammar%23NPR base on previous express compromise between quality performance e f f iciency e f f ectiveness cost result e code use evaluate algorithm along result obtain be available on grammar%23NPR
term figure result highlight that k-means grammar%23NPR outperform other algorithm k-means expect be top performer because algorithm perform comparison map cluster e fact grammar%23NPR have such good performance encourage think that in+fact most relevant topic when altogether exceed certain high weight threshold be that best represent document allow group together similar document however as show table consider x number more relevant topic grammar%23NPR consider trend their weight grammar%23NPR not seem perform so well on aggregate similar document since their precision recall value be very low both case surprise that grammar%23NPR have such low value take look at precision recall value also see number group that each algorithm create figure believe that have corpus contain very cohesive set document all paper corpus belong same journal performance this algorithm since divide corpus into low number figure grammar%23NPR j base grammar%23NPR figure grammar%23NPR figure base grammar%23NPR group be way obtain high value recall because pair wise distance compute but very low precision
e result also show that behavior algorithm do not when use similarity measure for+example grammar%23NPR divergence figure distance figure be highlights importance document topic distribution successfully classify into small group similar item while other particular aspect such as distance similarity metric used compare be less
random table precision j base grammar%23NPR multilingual entity linking grammar%23NPR grammar%23ENT united+states+of+america figure grammar%23NPR architecture overview
hence use grammar%23NPH tagger grammar%23NPH tagger our implementation on first line resource description collect any noun+phrase noun+phrase contain adjective for+example extract supplementary sf american singer our example be similar
acronym use large+number domain e.g. news see aida grammar%23NPR data+set thus reuse index from grammar%23NPR context grammar%23NPR work rely on semantic embeddings such as grammar%23NPR create index index model word surround resource mention textual corpus give that aim be kb agnostic deterministic our context index rely on resource literal find each resource first free stop word
preprocess string add as entry map say resource
all necessary information recreate our index build new index other kb find our grammar%23NPR candidate generation candidate generation disambiguation step occur online i.e. when grammar%23NPR give document set mention disambiguate goal candidate generation step be retrieve tractable number candidate mention
candidate later insert into disambiguation graph graph use determine mapping between entity mention see section
preprocess mention improve retrieval quality before use common normalization technique apply filter separate acronym acronym filter detect acronym by look+for string make+up uppercase letter less for+example psg be acronym case where mention consider acronym further preprocess steps skip otherwise use basic normalization method method remove punctuation symbol additional white+space use regular expression normalize structure grammar%23NPR rdf comment dbpedia.org%2Fresource%2Fmichael_jackson
www.abbreviations.com grammar%23NPR www.w3.org%2Fsubmission%2Fcbd grammar%23NPR github.com%2Faksw%2Fagdistis%2Fwiki%2F3-running-the-webservice string for+example our procedure map string lower+case except+for first letter therewith new+york normalize new+york furthermore our preprocess be capable recognize mention such as camel case grammar%23NPR add space between lower-case upper-case+letter i.e. true casing technique
second step candidate generation candidate search divide into three part by acronym if mention consider acronym by our preprocess expand mention list possible name from acronym index mention above for+example psg replace by
label this search rely on our sf index first grammar%23NPR retrieve candidate mention use exact match their respective principal reference for+example mention grammar%23NPH principal reference former president united+states+of+america be also grammar%23NPH match exactly case where find string similarity match main reference remaining steps skip if this search not return any candidate grammar%23NPR start new search use trigram similarity threshold over sf index case where set candidate be still empty grammar%23NPR stem mention repeat search for+example grammar%23NPR stem account linguistic variability stemming process not initialize step since be technique induce additional error both search type i.e. search by acronym label apply trigram similarity retrieve possible candidate
context here use two post- search filter find possible candidate from context index before apply both filter grammar%23NPR extract all entity contain input document
entity use as addition while search mention context index this search rely on tf israel+defense+forces reflect importance word string document corpus relative relevance index afterwards grammar%23NPR first filter unlikely candidate by apply trigram similarity second grammar%23NPR retrieve all direct link among remaining candidate kb our approach use number connection find highly related entity set specific mention be similar find dense subgraph figure illustrate example grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPR et+al.
search use context index white box on right side depict candidate discard by trigram filter
three ambiguous entity namely grammar%23NPH grammar%23NPH grammar%23NPH mention grammar%23NPH grammar%23NPR search context index use grammar%23NPH grammar%23NPH as query grammar%23NPR keep only grammar%23NPH grammar%23NPH trigram filtering only grammar%23NPH father grammar%23NPR have direct connection other candidate thus choose
improve quality candidate popularity given entity be good ranking factor if grammar%23NPR configuration use this factor number candidate retrieve from index be increased then result sort this sorting step grammar%23NPR return top candidate popularity calculate use page rank over underlying kb if be unable leverage page rank on certain kb fall+back heuristic link outlinks grammar%23NPR candidate generation process show algorithm entity disambiguation algorithm candidate generation step computation optimal candidate mention assignment start by construct disambiguation grammar%23NPR depth d similar approach
knowledge+base define kb k as directed graph grammar%23NPR v e where node v be resource k edge e be property k x y v x y e p x p y be triple k
set candidate c begin by build initial graph grammar%23NPR grammar%23NPR grammar%23NPR where grammar%23NPR be set all resource c grammar%23NPR start grammar%23NPR algorithm expand graph use grammar%23NPR grammar%23NPR technique in+order+to find hidden path among candidate extension graph be gi vi grammar%23NPR graph gi gi vi grammar%23NPR d. grammar%23NPR operator iterate d times on input graph grammar%23NPR compute initial disambiguation graph gd grammar%23NPR need identify correct candidate node given mention here grammar%23NPR in- document co- reference resolution base on early+work algorithm candidate generation
grammar%23NPR trigram similarity threshold result c candidate found c grammar%23NPR mention m mention co- reference m if grammar%23NPR m then grammar%23NPR grammar%23NPR m else grammar%23NPR grammar%23NPR m c grammar%23NPR do if c match then if grammar%23NPR c m then c grammar%23NPR c return else if grammar%23NPR c m then c grammar%23NPR c grammar%23NPR grammar%23NPR m c grammar%23NPR do if grammar%23NPR c m then c grammar%23NPR do if grammar%23NPR then c grammar%23NPR c rely on hit page rank as disambiguation graph algorithm this choice come from comparative study difference between both
use hub authority score define recursive relationship between node authority node be node that many hub link hub be node link many authorities
authority value be equal sum hub score each node point hub value be equal sum authority score each node point according+to previous work choose iteration hit suffice achieve convergence general
rank have wide range implementation implement general version in+accordance+with thus define possibility jump from any node any other node graph during random+walk probability w empirically choose page rank iteration iteration show be reasonable number el assign standard weightw each node finally sum calculate by spread current weight divide by outgoing edge
chosen graph algorithm high candidate score among set grammar%23NPR choose as correct disambiguation given mentionmi entire process present algorithm note grammar%23NPR also consider emergent entity assign new uri definition emergent entity if candidate generation step fail retrieve any candidate from target kb assume mention belong emergent entity
www.w3.org%2Ftr%2Fcooluris%2Fmag multilingual entity linking grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america algorithm disambiguation algorithm base on hit page rank
m grammar%23NPR m2 mn mention d depth k number iteration result c grammar%23NPR grammar%23NPR cn identify candidate named entity e v grammar%23NPR n g v e g grammar%23NPR g d if hit then hit g v e k grammar%23NPR v formi m do v v do if v be candidate formi then store mi v break if page rank then page rank g v e k grammar%23NPR v formi m do v v do if v be candidate formi then store mi v break evaluation goal aim our evaluation be three+fold first aim measure performance grammar%23NPR on data+set compare state+of+the+art el english second evaluate grammar%23NPR portability other language this end grammar%23NPR multilingual state+of+the+art use data+set from different language both evaluation use hit page rank third carry+out fine-grained evaluation provide deep analysis grammar%23NPR use method propose throughout our experiment use grammar%23NPR as reference kb
experimental setup our evaluation rely on gerbil platform focus on disambiguation kb grammar%23NPR experiment type task be map set given mention entity from given kb mean resource not found reference kb
data+set see table overview integrate into gerbil sake grammar%23NPR originate from be subset grammar%23NPR co- reference document annotation obtain through crowd sourcing where annotator link first mention each reference aida grammar%23NPR divide into chunk training testa grammar%23NPR exclusively contain annotation base on named entity this manually data+set use evaluate aida stem from grammar%23NPR tactical+air+command kbp data+set not include our evaluation because+of their redistribution license
grammar%23ENT shared task grammar%23NPR contain annotation first mention each entity newswire document
release along manually data+set contain short text named entity common entity such as cancer home grammar%23NPR create grammar%23ENT have high entity document density all corpus grammar%23NPR aim be stress+test el system through difficult disambiguation task use highly ambiguous mention use hand-crafted grammar%23ENT data+set create challenge contain only introduce grammar%23ENT by data+set contain news document rare san+francisco distinctive lexicalization grammar%23NPR grammar%23NPR comprise news+article news+article sample from grammar%23NPR news+article data+set randomly manually by domain expert grammar%23NPR grammar%23NPR consist data scrap from 1%2C457 rss+feed list include all major worldwide newspaper wide range topic
corpus annotate manually by domain expert cypriot+nationalist+organization+%28oke%29 grammar%23ENT use cypriot+nationalist+organization+%28oke%29 challenge data+set be curated manually divide into news.de be real-world data+set collected grammar%23ENT grammar%23ENT contain document from german news portal news.de be large multilingual corpus generate from enriched data abstract from six language table data+set statistics
news 57+253 aida grammar%23NPR complete english news grammar%23ENT aida grammar%23NPR test english news aida grammar%23NPR test b english news aida grammar%23NPR training english news grammar%23NPR english news 50+747 spotlight corpus english news 58+330 grammar%23NPR english mix kore english mixed 50+144 tweet tweet grammar%23NPR english news 20+747 grammar%23NPR grammar%23NPR english news 128+880 grammar%23NPR grammar%23NPR english mix cypriot+nationalist+organization+%28oke%29 grammar%23ENT task evaluation set english mixed 101+664 cypriot+nationalist+organization+%28oke%29 grammar%23ENT task example set english mix cypriot+nationalist+organization+%28oke%29 grammar%23ENT task training set english mixed 96+338 grammar%23NPR news.de german news 53+627 dutch abstract dutch mixed 2%2C147%2C483%2C647 french abstract french mixed 2%2C147%2C483%2C647 spanish abstract spanish mixed 2%2C147%2C483%2C647 italian abstract italian mixed 2%2C147%2C483%2C647 japanese abstract japanese mixed 2%2C147%2C483%2C647 grammar%23NPR www.yovisto.com%2Flabs%2Fner-benchmarks grammar%23NPR reduce our test set first subset provide abstract each language due evaluation platform limit display their characteristic table grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPR et+al.
result on english data+set this evaluation configure grammar%23NPR disambiguate named entity as+well+as common entity use acronym index popularity score according+to grammar%23NPR at letter level achieve good result when set trigram threshold be optimal at vary depth grammar%23NPR yield optimal choice therefore tune threshold d perform evaluation grammar%23NPR english result show first part analysis our result show that although acronym index be interesting addition potential improvement contribution amount only 0.05%25 f measure on+average over all data+set also popularity feature improve result almost every data+set explain by analysis demonstrate that most data+set create use more popular entity as mention thus this bias ease their grammar%23NPR hit show good result on+average than page grammar%23NPR however page rank show promise result some data+set e.g. spotlight corpus grammar%23NPR grammar%23NPR grammar%23NPR
use hit outperform other approach on data+set while achieve comparable result on e.g. grammar%23NPR grammar%23NPR cypriot+nationalist+organization+%28oke%29 data+set
performance grammar%23NPR easily adjust on each data+set by tune parameter for+instance grammar%23NPR achieve on microposts2014-test+english grammar%23ENT measure but result increase around 22%25 grammar%23ENT measure by disable context this improvement be due low entity count entity document thus missing+link among entity inside document
parameter configuration work not only huge data+set but also small data+set even disambiguate simple+sentence e.g. grammar%23NPH be basketball+player grammar%23NPH born grammar%23ENT be american researcher machine+learning artificial+intelligence if co- reference resolution turn+on both grammar%23NPR link same entity basketball+player but if this parameter turn+off grammar%23NPR be able find both correctly
sake clarity be no machine+learning grammar%23NPR thus be no cross validation set held out validation while choose parameter
multilingual result here show easy portability high+quality grammar%23NPR different language next german italian spanish french dutch choose japanese show promising potential grammar%23NPR different language+system grammar%23NPR preprocess nlp technique be multilingual thus be no additional implementation handle mention different character use same set parameter as english evaluation but exclude acronym as only collect english moreover perform page rank algorithm over each kb each respective language collect grammar%23NPR choose trigram threshold in+accordance+with work grammar%23NPR et+al. titan.informatik.uni-leipzig.de%2Frusbeck%2Fagdistis%2Fappendix.pdf grammar%23NPR gerbil.aksw.org%2Fgerbil%2Fexperiment%3Fid%3D201701230012 gerbil.aksw
gerbil experiment id grammar%23NPR result without popularity use hit gerbil.aksw.org%2Fgerbil experiment id grammar%23NPR gerbil.aksw.org%2Fgerbil%2Fexperiment%3Fid%3D201701240030 grammar%23NPR gerbil.aksw.org%2Fgerbil%2Fexperiment%3Fid%3D201701270035 popularity value their entity result display second part table show that grammar%23NPR use hit outperform publicly available state-of-the-art also page rank outperform hit score on dutch improved performance grammar%23NPR be due knowledge+base agnostic algorithm indexing model for+instance although mention grammar%23NPH have high popularity english have less popularity italian spanish kb study about generation proper+name support this observation
fine-grained evaluation this analysis use extension gerbil do by grammar%23NPH et+al. this extension provide fine-grained evaluation evaluation measure quality given el link different type entity this extension also consider assumption et+al. that corpus have tendency focus strongly on prominent popular entity entity cause evaluation problem hence extension evaluate capability given el system find entity different level popularity thus reveal degree bias towards popular entity
this evaluation use same set parameter as english evaluation fine-grained analysis show that grammar%23NPR be good at link person than other type entity explain by index create by grammar%23NPR offline phase collect last+name rare surface entity in+addition result show that grammar%23NPR be not biased towards link only popular entity as see table summary present grammar%23NPR kb agnostic deterministic approach multilingual el grammar%23NPR outperform state+of+the+art on all grammar%23NPR data+set in+addition grammar%23NPR achieve performance similar state+of+the+art on english data+set nevertheless average grammar%23ENT measure place grammar%23NPR first out annotation system as expect machine+learning based system retain their advantage due their tuned training on provided data+set however intrigue by our deterministic knowledge+base independent performance this paper analyze influence different indexing searching method as+well+as influence data+set structure fine-grained evaluation also provide context search without rely on machine+learning as previously done moreover show that current ml based el approach be strongly biased due their learned model this behavior see on multilingual data+set also deploy analyze influence acronym last+name best our knowledge no work investigate this influence on el provide fine-grained evaluation before in+the+future intend further investigate disambiguation algorithm grammar%23NPR performance biomedical earth+science domain use same data+set from grammar%23NPH al also aim evaluate grammar%23NPR on different language such as arabic using other data+set from export experiment configuration base on reproducibility purpose
detail see faturl.com%2Fmultilingualmag%2F%3Fopen grammar%23NPR information about other data+set find here gerbil.s16a.org gerbil experiment id grammar%23NPR multilingual entity linking grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america table micro f measure approach red entry be top score while blue represent second+best score
data+set ab el fy d b pe di d os er en ti ty cl as si fi er e u fr ed k ea be m g h it s m g pr en gl be h grammar%23NPR grammar%23NPR complete grammar%23NPR test aida grammar%23NPR test grammar%23NPR training grammar%23NPR spotlight grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR task evaluation set task example set task training set 700%2C000 ul til gu al grammar%23NPR news.de italian abstract spanish abstract japanese abstract dutch abstract french abstract average standard+deviation table fine-grained micro f1 evaluation
person page rank 10%25 page rank 10%25 55%25 page rank 55%25 100%25 grammar%23NPR 10%25 grammar%23NPR 10%25 55%25 grammar%23NPR 55%25 100%25 acknowledgment this work support by grammar%23NPR project hobbit ga no
as+well+as eurostar project diesel no. grammar%23NPR grammar%23NPR no. grammar%23NPR support by grammar%23NPR grammar%23NPR no. this work also support by german federal ministry+of+transport grammar%23NPR project no. grammar%23NPR grammar%23NPR no. grammar%23NPR as+well+as by german federal ministry+of+education research grammar%23NPR within grammar%23NPR grammar%23NPR f%C3%BCr die zivile grammar%23NPR in+particular grammar%23NPR f%C3%BCr die zivile grammar%23NPR project grammar%23NPR no. grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPR abstract large-scale open multilingual nlp training corpus proceedings on language resource evaluation grammar%23NPR grammar%23ENT
grammar%23NPH gathering alternative surface form grammar%23NPR entity nlp grammar%23NPR grammar%23ENT grammar%23NPR workshop proceeding
grammar%23NPH grammar%23NPH entity disambiguation web link
grammar%23ENT 145+156
grammar%23NPH grammar%23NPH use grammar%23NPH named entity resolution linking typing knowledge+base population semantic+web evaluation challenge second grammar%23NPR challenge at grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT springer
large-scale named entity disambiguation base on proceeding grammar%23ENT on empirical method natural+language+processing computational natural language+learning grammar%23NPR grammar%23NPR czech+republic 708+716
grammar%23NPH grammar%23NPH computational interpretation grammar%23NPR maxim generation refer expression on computational+linguistics cognitive+science 233+263
pooja comparative study hit based ranking algorithm computer+engineering communication+engineering grammar%23ENT from textual description through grammar based transformation indian+institute+of+technology madras kevin%40cse.iitm.ac p madras psk%40cse.iitm.ac abstract this paper propose new grammar%23NPR call grammar%23NPR textual description identifier grammar inspire by different way grammar%23NPR construct express english build system system transform grammar%23NPR sentence into corresponding grammar%23NPR axiom now ambiguity due different possible lexicalization sentence semantic ambiguity present sentence be challenge this context
find that good way handle challenge be construct axiom correspond alternative formalization sentence so that end-user make appropriate choice output compare against human authored axiom substantial number case human authored axiom be indeed alternative give by system
proposed system substantially enhance type sentence+structure use ontology authoring
computing methodology description logic ontology engineering lexical semantics theory computation axiomatic semantics keyword ontology learning knowledge extraction text+mining introduction authoring grammar%23NPR ontology intellectually challenge make this process simpler many system accept natural+language text as input now ambiguity natural+language text be major challenge context ontology authoring be difficult disambiguate input sentence without substantial background+knowledge domain
most current authoring+system produce one formalization sentence find that good way handle challenge be construct axiom correspond alternative formalization sentence so that end-user make appropriate choice
this paper propose novel ontology authoring+system authoring+system simplify authoring process user our system take english text text conform newly proposed association+for+computing+machinery acknowledge that this contribution be authored co- authored by employee contractor affiliate national government as+such government retain nonexclusive royalty-free right publish reproduce this article allow do so government purpose only
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148034 grammar%23NPR call grammar%23NPR as input generate corresponding owl axiom scope ontology authoring building schema grammar%23NPR ontology within this context focus on owl class expression axiom detailed version paper be available grammar%23NPR
outline architecture our system figure
five main module system be lexical ambiguity handler grammar%23NPR parser semantic ambiguity handler syntactic transformation axiom generation lexical ambiguity handler accept input sentence express grammar%23NPR generate possible lexicalization sentence use po tag pattern lexicalization be process break+down given sentence into token such that token identify as various ontology element namely class individual properties concept constructor then grammar%23NPR parser parse lexicalization on basis grammar%23NPR grammar rule only valid grammar%23NPR lexicalization process further sentence not conform grammar%23NPR indicate as+such user so that reformulate semantic ambiguity handler generate possible interpretation grammar%23NPR lexicalization use specific sentence pattern interpretation convert grammar%23NPR grammar%23NPR syntactic transformation module use various rule transformation finally grammar%23NPR sentence convert owl axiom by existing grammar%23NPR parser
output system compare against humanauthored axiom substantial number case humanauthored axiom be indeed alternative give by system our framework clearly outperform grammar%23NPR terms number type sentence system handle also employ grammar%23NPR as intermediate+language aid formalization reduce complexity system comparison existing system proposed system have streamlined approach generation axiom from text
system attempt convert any english sentence process either fail generate incorrect axiom several case however proposed approach since guide by grammar system identify sentence not handle give error+signal also existing system generate only one formalization sentence without take into account impact ambiguity formalization
remainder paper be structured as+follows section discuss related work section discuss grammar%23NPR section discuss transformation grammar%23NPR text grammar%23NPR section describe ambiguity formalization how handle by proposed system
arxiv.org%2Fabs%2F1709.08448k-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH p lexical ambiguity handler po tag pattern english text grammar%23NPR parser lexicalization semantic ambiguity handler sentence pattern syntactic transformation transformation rule axiom generation grammar%23NPR parser owl axiom lexicalization grammar%23NPR lexicalization interpretation grammar%23NPR text figure architecture section describe result evaluation section conclude paper
related work axiom from grammar%23NPR challenge due unrestricted ambiguous nature grammar%23NPR restrict unambiguous variant natural+language propose this context some grammar%23NPR develop specifically create owl ontology be grammar%23NPR rabbit so
that text-based ontology authoring approach need have corresponding solution problem extract ontology from text success authoring+tool depend on effective technique extraction extract ontology also term as ontology learn literature main technique ontology learn from text be linguistics base statistics base linguistics based technique involve exploitation lexico syntactic pattern utilization knowledge rich resource such as linked data ontology syntactic transformation grammar%23NPR technique involve relevance analysis co-occurrence analysis cluster formal concept analysis association rule mining deep learning grammar%23NPR et+al. present study on prominent ontology learning system current system employ combination aforementioned technique
text description identifier grammar grammar%23NPR only sentence have at+least one lexicalization lexicalization be valid according grammar%23NPR rule handle by our system
expressivity dl cover by language define by grammar%23NPR be grammar%23NPR develop grammar%23NPR keep+in+mind rich set primitive owl world+wide+web+consortium recommend widely adopt ontology language also mine brown corpus identify various way owl primitive express natural+language use various regular expression extract pattern brown corpus be prominent text corpus be rich enough contain various kind such pattern
pattern pattern correspond owl primitive encode as rule grammar for+instance grammar rule rule define owl class intersection be intersection concept grammar%23NPR grammar grammar%23NPR make+up about such rule
terminal grammar correspond owl primitive primary rule grammar%23NPR correspond class expression axiom class expression axiom define concept
textual description correspond class expression axiom consist concept class expression class expression either be atomic class complex class
atomic class denote single concept complex class construct various way be possible employ set operation union intersection complement property restriction existential restriction universal restriction cardinality restriction value restriction self restriction be also possible construct complex class by enumerate all instance class
natural+language indicator various owl primitive give production rule grammar brown corpus be beneficial identify indicator consider relative+pronoun as natural+language indicator intersection class
determiner indicate existential restriction determiner only indicate universal restriction
syntactic transformation proposed system employ syntactic transformation convert grammar%23NPR text i.e. sentence have at+least one grammar%23NPR lexicalization grammar%23NPR text implement syntactic transformation as action that attach grammar element grammar%23NPR although grammar%23NPR use ontology authoring grammar%23NPR have two main limitation syntactic transformation address limitation firstly many english sentence be not present grammar%23NPR only suitable transformation sentence become ace compliant table list few english sentence sentence be not grammar%23NPR compliant along reason their non-compliance also show correspond sentence that obtain through syntactic transformation
transformed sentence be valid grammar%23NPR sentence secondly some case even+though sentence be valid grammar%23NPR grammar%23NPR parser namely fail axiomatize because only subset grammar%23NPR sentence map well owl axiom table list few grammar%23NPR sentence sentence be not auckland+peering+exchange compliant along reason their noncompliance also show correspond sentence that obtain through suitable transformation transformed sentence successfully formalize into owl
ambiguity formalization investigate impact two type ambiguity namely lexical ambiguity semantic ambiguity following section describe each type in+detail how both handle by system extract ontological knowledge from textual grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america english sentence reason non-compliance battery produce electricity every noun prefix by determiner
battery produce some electricity
adenine be purine+base multi- term expression hyphenate
adenine be purine+base
kidnapper seize detain victim coordinated verb+phrase agree number verb form
kidnapper seize victim detain victim
make english sentence grammar%23NPR compliant through syntactic transformation grammar%23NPR sentence reason non-compliance kid play intransitive+verb not support by auckland+peering+exchange owl generator
kid play
person learn some maths modal+verb not support by auckland+peering+exchange owl generator
person should learn some maths
abacus efficiently perform some arithmetic
not support by auckland+peering+exchange owl generator
abacus efficiently perform some arithmetic table making grammar%23NPR sentence auckland+peering+exchange compliant through syntactic transformation lexical ambiguity ontology element i.e. class relation instance lexicalize more than one way due various modeling possibility depend on factor such as domain individual preference application as result be possible generate multiple formalization same sentence by combine different lexicalization element various way for+example consider following sentence every vegetable pizza be tasty pizza note adjective tasty tasty describe noun pizza while formalize sentence this adjective either lexicalize as separate concept associate corresponding noun this lexical ambiguity with+respect+to adjective result two different formalization as show below grammar%23NPR grammar%23NPR grammar%23NPR tasty pizza our system identify lexical ambiguity associate formalization generate all possible formalization sentence use syntactic pattern ontology identifier study by grammar%23NPH according+to study ontology identifier such as class identifier name individual identifier object property identifier data property identifier follow simple syntactic pattern each type identifier express through relatively few syntactic pattern
pos tag each word input text then use aforementioned syntactic pattern extract all possible identifier from sentence by properly combine identifier generate all formalization sentence
semantic ambiguity another type ambiguity associate formalization be semantic ambiguity due semantic ambiguity sentence have more+than one interpretation for+example consider following sentence every driver drive car be possible interpretation this sentence formalization correspond each interpretation be driver drive car driver drive car driver drive car drive car note quantification associate each axiom first axiom denote correct interpretation hence correct formalization given sentence existential quantification be appropriate due fact that person drive at+least one vehicle become driver universal quantification be inappropriate due fact that driver drive any vehicle not necessarily car interest note that structurally similar sentence different interpretation be appropriate e.g. every vegetable pizza make vegetable item
be necessary have access sufficient background+knowledge disambiguate such sentence as demonstrate above however absence background+knowledge good possible way be generate possible multiple interpretation semantically ambiguous sentence study various sentence pattern then manually identify one one be semantically ambiguous also record as how map corresponding interpretation give sentence as input system check whether sentence use any pattern pattern identify beforehand
such case system generate all corresponding interpretation sentence case sentence not use pattern i.e. semantically unambiguous generate only one interpretation sentence
result discussion details evaluation our framework give below use various type dataset our evaluation cover multiple domain facilitate concrete evaluation our approach also ensure that our approach grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH p dataset pizza ssn be 40+115 table comparison grammatical coverage grammar%23NPR grammar%23NPR on various dataset be refer number input sentence as refer number grammar%23NPR sentence ts refer number grammar%23NPR sentence neither dataset orient domain dependent dataset use evaluation be pizza ssn grammar%23NPR grammar%23NPR
dataset evaluation result be available grammar%23NPR
use grammar%23NPR parser namely translate grammar%23NPR text into owl axiom
grammatical coverage analysis evaluate quality grammar%23NPR by analyze grammatical coverage compare coverage grammar%23NPR grammar%23NPR
analysis show that grammar%23NPR have large grammatical coverage as compare ace analysis report table
system guarantee generate at+least one valid owl axiom from every grammar%23NPR sentence every dataset number grammar%23NPR sentence be significantly number grammar%23NPR sentence for+instance out sentence pizza dataset sentence be valid according ace whereas sentence be grammar%23NPR sentence observe similar pattern every+other dataset
formalization evaluate number formalization generate by proposed system choose dataset have at+least 50%25 grammar%23NPR sentence this evaluation as observe from table pizza ssn grammar%23NPR have at+least 50%25 grammar%23NPR sentence hence dataset use this evaluation
result report table as observe from table proposed system generate many lexicalization from given input sentence out be valid grammar%23NPR sentence base on various sentence pattern identify semantically ambiguous sentence sentence have more+than one interpretation major portion complete set interpretation be valid grammar%23NPR sentence convert equivalent owl axiom
also evaluate correctness axiom choose pizza dataset this evaluation manually author gold+standard set axiom compare axiom generate by system out sentence dataset sentence human authored axiom be indeed alternative give by system
kevinalexmathews.github.io%2Fsoftware dataset pizza ssn grammar%23NPR be lx 677+5964+560 grammar%23NPR 157+297 grammar%23NPR 260+594+186 206+180 table evaluation formalization on various dataset be denote number input sentence lx denote total number lexicalization grammar%23NPR denote total number valid grammar%23NPR lexicalization grammar%23NPR denote total number interpretation denote total number valid grammar%23NPR axiom conclusion this paper propose ontology authoring framework framework extract class expression axiom from natural+language sentence through grammar based syntactic transformation
input sentence that conform grammar transform into grammar%23NPR then use grammar%23NPR parser generate owl axiom from transformed text also explore effect ambiguity on formalization construct axiom correspond alternative formalization sentence
our future work include enhance grammar so that framework handle larger range sentence investigate impact other type ambiguity such as scope ambiguity on formalization
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH syntax towards controlled natural+language syntax owl proceedings vol.
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH rabbit language description syntax conversion owl ordnance+survey research lab grammar%23ENT
w grammar%23NPH grammar%23NPH brown corpus standard corpus grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPR control english knowledge+representation lecture note computer+science including subseries lecture note artificial+intelligence lecture note bioinformatics vol. grammar%23NPR 104+124
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH owl next step owl grammar%23ENT 309+322
grammar%23NPH grammar%23NPH grammar%23NPH ontology learn deep knowledge engineering knowledge+management 20th grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT proceeding grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1007%2F978-3-319-49004-5 grammar%23NPH grammar%23NPH acquisition owl dl axiom from lexical resource semantic+web research application grammar%23ENT 670+685
grammar%23NPH analysis pos tag pattern ontology identifier label http computing-reports.open.ac.uk%2F2013%2Ftr2013-02.pdf grammar%23NPH grammar%23NPH ontology learn from text look back into future
grammar%23NPR grammar%23NPR grammar%23ENT grammar%23ENT http doi.org%2F10.1145%2F2333112.2333115propagating data policy user study grammar%23NPH united+kingdom enrico.daga%40open.ac.uk grammar%23NPR mathieu.daquin%40insight-centre.org grammar%23NPH united+kingdom enrico.motta%40open.ac.uk abstract when publishing data data licence use specify action action permit prohibit duty that target data consumer comply however complex environment such as smart city data portal multiple data+source constantly combine process redistribute such scenario decide policy apply output process base on licence attach input+data be difficult knowledge intensive task this paper evaluate how automatic reason upon semantic representation policy data+flow support decision+making on policy propagation report on result user study design assess both accuracy utility such policy propagation tool comparison manual approach
concept information+system expert+system computing methodology knowledge+representation reasoning applied computing law keyword data licence data+flow policy propagation user study association+for+computing+machinery reference format grammar%23NPH grammar%23NPH propagate data policy user study proceeding knowledge capture united+states+of+america submit grammar%23ENT grammar%23NPR grammar%23ENT page
introduction large data infrastructure such as city data hub dataset publish specific terms condition specify action action permit prohibit duty that target user comply for+example duty attribute data publisher however while dataset browse their original form more often tailor meet needs application through complex manipulation process have new dataset as output such processing republication derived dataset permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page
third-party component this work honor
all use contact owner author s
grammar%23ENT submit grammar%23ENT grammar%23ENT copyright hold by owner author s
isbn 123+4567+24+567+08+06 priceless https%3A%2F%2Fdoi.org%2F10.475%2F123 can happen multiple times this scenario complex knowledge intensive task assess policy associate input process take+into+account when decide term condition applicable output for+example process aggregated statistics from two different input dataset duty attribute source data restriction on commercial application still apply result previous work develop policy propagation support data manager user make decision on policy associate generated dataset on basis one associate input by+means+of semantic representation policy data+flow system combine owl reasoning compute policy associate any data node involve process this work report on user study carry+out evaluate system term accuracy utility support task policy propagation as perform by data manager processor publisher participant confront problem decide policy need take+into+account when use dataset dataset derive from complex process reuse licensed data+source their decision then compare one our system insight into expected behaviour acquire as+well+as observation about accuracy utility
next section present background related work section illustrate knowledge+base rely+upon section describe setup user study methodological criteria how required resource acquire develop discuss feedback receive from participant user study section before go into detail result section discuss section section summarize contribution this study give direction future work
context numerous initiative investigate vision smart city where cutting-edge technology apply a+number+of sector include government+services transport traffic management water healthcare energy urban agriculture waste resource management city data hub emerge on as centralized node control monitor flow+of+information between variety system deploy given city region
research aim understand how govern life+cycle data city data hub diversity data+source owner licence associate data open new challenge namely problem data exploitability grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT et+al.
cluster grammar%23NPR size table precision base grammar%23NPR size table recall j base grammar%23NPR size table recall base grammar%23NPR figure cost grammar%23NPH base grammar%23NPR figure cost base grammar%23NPR terms cost figure best cluster algorithm as expect base on random selection be be due fact that number pair compare by this algorithm be always minimum give dataset simply randomly divide into m equal sized group where m be equal number topic i.e. dimension dataset since grammar%23NPR grammar%23NPR make comparison between document until their internal condition be be most approach grammar%23NPR involve high cost because compare all document centroid each iteration
our proposal main reason algorithm present high cost be due number group corpus divide into see figure e greater number group number later comparison make therefore lower cost algorithm
e behavior grammar%23NPR algorithm depend remarkably on similarity metric used think that be due way both measure satisfy triangle+inequality condition since base on divergence j on distance be property property distance b distance c distance c b be very important calculation that grammar%23NPR make discover group since only calculate distance between near icient clustering from distribution over grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america figure grammar%23NPR grammar%23NPH base grammar%23NPR figure grammar%23NPR base grammar%23NPR finally terms figure regardless+of similarity measure use algorithm algorithm yield good performance accord result obtain be overall grammar%23NPR demonstrate high accuracy low cost by improve performance by centroid base densitybased approach
also create synthetic dataset grammar%23NPR section compose grammar%23NPR distribution same dimensions than topic grammar%23NPR k unlike grammar%23NPR topic distribution randomly generate imply that similarity value be not so high min mean max follow same criteria than before section similarity threshold be now x figure result term figure show poor performance grammar%23NPR grammar%23NPR algorithm e reason be that base on fact that high weighted topic share between similar distribution
this condition be not when similarity value between be low
this behavior create third dataset grammar%23NPR same size but only dimensions topic e goal be achieve more similar distribution than drm even+though also randomly generate since similarity value range from min mean tomax similarity threshold be now x more detail section e result figure figure similarity value group by frequency drm figure grammar%23NPR grammar%23NPH base drm figure grammar%23NPR grammar%23NPH base grammar%23NPR show improvement accuracy both grammar%23NPR grammar%23NPR algorithm although score be still not as high as grammar%23NPR dataset increase compare drm dataset show that their precision recall improve when similarity threshold be high on+the+other+hand both grammar%23NPR grammar%23NPR algorithm show similar behavior both dataset mean that their performance be not by similarity cap grammar%23ENT grammar%23ENT grammar%23ENT et+al.
conclusion grammar%23NPR process continuously growing collection human generated document require technique technique divide space into small region contain potentially similar document some algorithm literature tackle this problem from unsupervised point+of+view but incur high temporal cost not suit domain study
ree novel unsupervised clustering algorithm grammar%23NPR grammar%23NPR grammar%23NPR describe this paper rely on distribution infer from topic modeling algorithm grammar%23NPR ey present as means identify small set document where only similarity function compute ey leverage on particular behavior grammar%23NPR distribution describe topic distribution where high weighted topic have high on rest topic be also mean that give topic distribution relation between their topic weight such as order trend between be more important than density value
initially think that use only x number topic high weight topic distribution grammar%23NPR take+into+account only trend change between weight consecutive topic grammar%23NPR be enough classify similar topic distribution result obtain show that properties be not result term cost show compare proposed algorithm existing centroid base density based clustering technique
ey reveal that obtain most representative topic topic distribution by compare sum their weight with+respect+to rest grammar%23NPR be promising approach improve obtain by other centroid base density based approach while grammar%23NPR take o nk logn grammar%23NPR take o n logn time classify n document collection proposed algorithm only take linear time o n because not require any other data except their own topic distribution assign cluster
hierarchical approach grammar%23NPR algorithm also consider but not produce good result hybrid method combine novel approach existing technique perform future work on same line
grammar%23NPH grammar%23NPH grammar%23NPH
text+mining task grammar%23NPR proceeding 17th association+for+computing+machinery symposium on document engineering grammar%23NPR doi.org%2F10.1145%2F3103010
grammar%23NPH grammar%23NPH grammar%23NPH
eval similarity calculation grammar%23ENT doi.org%2F10.5281%2Fzenodo
grammar%23NPH grammar%23NPH grammar%23NPH scalable grammar%23NPR proceeding vldb endowment grammar%23NPR grammar%23ENT 622+633 doi.org%2F10.14778%2F2180912.2180915 grammar%23NPH grammar%23NPH grammar%23NPH unsupervised organization image collection taxonomy beyond institute+of+electrical+and+electronics+engineers transactions on grammar%23NPR analysis machine+intelligence grammar%23ENT doi.org%2F10
tpami.2011.79 grammar%23NPH introduction probabilistic topic modeling grammar%23NPR association+for+computing+machinery grammar%23ENT doi.org%2F10.1145%2F2133806.2133826 grammar%23NPH grammar%23NPH grammar%23NPH probabilistic topic model
grammar%23ENT doi.org%2F10.1109%2Fmsp
grammar%23NPH grammar%23NPH grammar%23NPH probabilistic topic model focus on graphical model design application document image analysis grammar%23ENT doi.org%2F10
msp.2010.938079 grammar%23NPH grammar%23NPH modeling science grammar%23ENT
grammar%23NPH correlated topic model science e grammar%23NPR grammar%23ENT doi.org%2F10.1214 grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH latent grammar%23NPR allocation grammar%23ENT 993+1022 doi.org%2F10.1162%2Fjmlr.2003.3.4-5.993 grammar%23NPH grammar%23NPH holistic sentiment analysis language grammar%23NPR supervise latent grammar%23NPR allocation proceedings grammar%23ENT conference on empirical method grammar%23ENT doi.org%2F10.1017%2Fcbo9781107415324.004 grammar%23NPR d hakkani-tur grammar%23ENT grammar%23NPR based similarity modeling estion answering proceeding on semantic grammar%23NPR
grammar%23NPH grammar%23NPH similarity grammar%23NPR model machine+learning grammar%23ENT
doi.org%2F10.1023%2Fa s grammar%23NPR s t grammar%23NPR g w grammar%23NPR t k r grammar%23NPR
by latent semantic analysis journal grammar%23ENT doi.org%2F10.1002 grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPH grammar%23NPH peter grammar%23NPR grammar%23NPR s grammar%23ENT density based algorithm discover cluster large spatial database noise grammar%23ENT 226+231 doi.org%2Fciteulike-article-id grammar%23NPH grammar%23NPH grammar%23NPH on cluster validation technique grammar%23ENT 107+145 doi.org%2F10.1023%2Fa grammar%23NPR astro-ph grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH study history idea using topic model proceedings grammar%23ENT conference on empirical method natural+language+processing grammar%23ENT october grammar%23ENT 363+371 doi.org%2F10.3115%2F1613715.1613763 grammar%23NPR south grammar%23NPH untangle text+data+mining 37th annual meeting
omas grammar%23NPR grammar%23NPR learn by probabilistic latent semantic analysis machine+learning grammar%23ENT 177+196
grammar%23NPH grammar%23NPH tao mei author topic model based collaborative ltering personalized poi recommendation
transactions on multimedia grammar%23ENT 907+918 doi.org%2F10.1109 tmm.2015.2417506 grammar%23NPH short text similarity word embeddings category subject descriptor proceedings 24th on conference on information knowledge+management grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT grammar%23ENT doi.org%2F10.1145%2F2806416.2806475 grammar%23NPR li grammar%23NPH building using institute+of+electrical+and+electronics+engineers computer society conference on computer+vision grammar%23NPR recognition
grammar%23ENT divergence measure base on institute+of+electrical+and+electronics+engineers transactions on information eory grammar%23ENT 145+151 doi.org%2F10.1109 automatic topic discovery multi- object tracking proceeding on grammar%23NPR intelligence
grammar%23NPH grammar%23NPH inference population structure use multilocus genotype data genetics grammar%23ENT 945+959
c grammar%23NPH diversity measurement decomposition apportionment analysis grammar%23NPR e series grammar%23ENT
grammar%23NPH similarity measure base on latent grammar%23NPR allocation computational+linguistics intelligent text+processing 459+470
m grammar%23NPR s t. grammar%23ENT probabilistic topic model t. grammar%23NPR d. grammar%23NPH s. grammar%23NPR w. editor latent semantic analysis road meaning grammar%23NPH tang z. grammar%23NPR j grammar%23ENT
w grammar%23NPH grammar%23NPH measure similarity similarly grammar%23NPR human perception association+for+computing+machinery transactions on intelligent+systems technology association+for+computing+machinery reference format association+for+computing+machinery trans. grammar%23NPR grammar%23ENT grammar%23NPR multilingual knowledge+base agnostic grammar%23NPH moussallem%40informatik.uni leipzig.de ricardo.usbeck%40upb.de michael.roeder%40upb.de axel.ngonga%40upb.de abstract recently be subject significant body research currently good performing approach rely on trained mono lingual model port approach other language be consequently difficult endeavor as require corresponding training data retraining model address this drawback by present novel multilingual knowledge+base agnostic deterministic approach dub grammar%23NPR grammar%23NPR base on combination context based retrieval on structured knowledge+base graph algorithm evaluate grammar%23NPR on data+set language our result show that good approach train on english dataset grammar%23NPR achieve micro f measure measure be up time worse on dataset other language grammar%23NPR on+the+other+hand achieve state-of-the-art performance on english dataset reach micro f measure measure be up high than grammar%23NPR on non- english language
concept information system information extraction computing methodology natural+language+processing keyword multilingual association+for+computing+machinery reference format grammar%23NPH grammar%23NPR grammar%23NPR grammar%23NPR multilingual knowledge+base agnostic proceeding grammar%23NPR grammar%23ENT knowledge united+states+of+america grammar%23ENT page
permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT knowledge grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148024 introduction more than grammar%23ENT data add every automatic extraction knowledge from this data demand use efficient natural+language+processing nlp technique such as text aggregation text summarization knowledge extraction
most important nlp task be also+known+as goal here be as+follows give piece text reference knowledge+base k set entity mention that text map each entity mention corresponding resource k
challenge address when deal
entity have large+number also+known+as label due synonymy acronym typo for+example new+york new+york new+york be all label same entity moreover multiple entity share same name due homonymy ambiguity for+example both state city new+york call new+york despite complexity endeavor approach achieve increasingly good result over past few year by rely on trained machine+learning model see overview portion approach claim be multilingual rely on use cross lingual dictionary
our experiment see section show that underlying model train on english corpus make prone failure when migrate different language for+example while grammar%23NPR achieve average micro f measure on corpus english only achieve average micro f measure on other language
alleviate this problem by present grammar%23NPR novel multilingual approach grammar%23NPR multilingual base on concepts similar underlying but go beyond this approach by rely on+time efficient graph algorithm combine language independent feature link entity given reference knowledge+base hence grammar%23NPR be knowledge+base agnostic i.e. deploy on any reference knowledge grammar%23NPR
approach be also deterministic not rely on any trained model hence deploy on virtually any language
main contribution this paper summarize as+follows grammar%23NPR tinyurl.com%2Fkx6baxxk-cap grammar%23ENT knowledge grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
present novel multilingual deterministic approach combine lightweight easily extensible graph based algorithm new context based retrieval method
grammar%23NPR feature innovative candidate generation method method rely on various filter method search type good candidate selection
provide thorough evaluation our overall system on data+set use gerbil platform our result show that grammar%23NPR achieve state-of-the-art performance on english in+addition grammar%23NPR outperform all state-of-the-art approach on non- english data+set
implement within version grammar%23NPR use this paper also all experimental result be publicly grammar%23NPR
approach subdivide into different class grammar%23NPR approach approach base evaluate on english data+set multilingual approach
only grammar%23ENT et+al. propose approach dub nerd-ml entity recognition tailor extract entity from tweet this approach rely on entity type classification over rich feature vector compose set linguistic component next plethora other approach including develop approach mostly rely on graph algorithm and%2For on machine+learning technique however not offer webservice implementation implementation be publicly available be thus difficult compare
large+number multilingual approach develop over years combine model base on vector+space representation entity use cosine similarity perform disambiguation task grammar%23NPR et+al. present grammar%23NPH base on
et+al. implement grammar%23NPH other language
base on fine granular context model model take+into+account heterogeneous text source as+well+as text create by automated multimedia analysis grammar%23NPR et+al. present entity entity base on hypernym grammar%23NPR entity classification+system classification+system identify salient word
input transform low dimensional representation keep same quality output all size input text grammar%23ENT grammar%23NPR et+al. present three step pipeline base on cross lingual linked data lexica harness multilingual grammar%23NPR et+al. propose grammar%23NPR base on random+walk dense subgraph algorithm rely on grammar%23NPR semantic+network as background+knowledge grammar%23NPR et+al. present agnostic entity disambiguation approach base on string similarity measure graph based hit algorithm grammar%23NPR be successor grammar%23NPH include re- design all grammar%23NPH component namely spotter grammar%23NPR github.com%2Faksw%2Fagdistis grammar%23NPR github.com%2Faksw%2Fagdistis%2Freleases%2Ftag%2Fv1.0-mag grammar%23NPR faturl.com%2Fmagexp%2F%3Fopen%26selected%3D0 disambiguator pruner grammar%23ENT grammar%23NPR grammar%23NPR present grammar%23NPH novel machine reader reader extend grammar%23NPH entity typing capability grammar%23NPR et+al. present doser approach akin but base on entity embeddings
grammar%23NPR et+al. introduce grammar%23NPR probabilistic graphical model model use pairwise grammar%23NPR random field our related work+study suggest that only three related approach be multilingual deterministic without domain restriction be vinculum grammar%23NPR however vinculum grammar%23NPR other recent supervised approach not include our evaluation because neither their code webservice make available publicly approach not reconstruct base on respective paper only
grammar%23NPR approach this section present grammar%23NPR in+detail throughout this work rely on following formal definition
let e be set entity from d be document contain potential mention entity m grammar%23NPR mn goal system be generate assignment f mention entity f m e n document d where stand entity be not
process implement by grammar%23NPR consist two phase
index generate during offline phase per+se carry+out during online phase consist two steps candidate generation disambiguation overview found figure
offline index creation grammar%23NPR rely on following five index person name rare reference acronym context
grammar%23NPR rely exclusively on structured data generate entity so as remain agnostic each entity reference our approach harvest all label say entity as+well+as type index additional sf collect from different source
name this index account variation name reference person language domain person refer by different portion their name for+example artist grammar%23NPH grammar%23NPH often refer as grammar%23NPH grammar%23NPH also+known+as grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH moreover language such as chinese japanese put family+name in+front+of given+name contrast english where name write reverse order address problem label person by generate all possible permutation word within known label person add index name
reference this index create if textual description be available resource interest e.g. if resource have rdf comment property large+number textual entity description provide type information pertain resource at+hand as example grammar%23NPH be american singer grammar%23NPH our implementation rely on predicate such as rdf label rdf type adaptive approach interlink grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america road centreline barycentre monument b grammar%23NPR grammar%23NPR b grammar%23NPR c figure learning feature geometry capture rule table learning result classify algorithm method precision recall f measure 91.6%25 91.3%25 91.3%25 grammar%23NPR 96.3%25 96.3%25 96.3%25 decision+table 96.4%25 96.3%25 96.2%25 random forest 98.8%25 98.8%25 98.7%25 plot data on base map intuitively distinguish three principal representation monument location near building center near building facade near road centerline assumption be actual intended geometric representation not verify because+of open nature this source however take as hypothesis in+order+to automatically learn geometry capture rule estimate positional accuracy geometry
use two reference geographic data+source about grammar%23NPR road grammar%23NPR investigate some possible learning feature compute state+of+the+art gis tool present grammar%23NPR prepare training set by manually label monument each class then apply grammar%23NPR learning algorithm validate result manually see result table resulting classification interpret on level geometry by adding metadata about their capture rule
estimate absolute positional accuracy each point by sum two value distance between point intended location building facade building barycenter road centerline planimetric accuracy geographic feature represent this intended location more detail on population approach describe this section present adaptive approach geometry similarity evaluation this section present how use parameter about threshold confidence value function infer use grammar%23NPR semantics ontology automatically adapt in- progress geometry similarity evaluation process
general description see section that geometry similarity measure usually base on some main choice one more similarity function behavior their normalization function their parameter grammar%23NPR grammar%23NPR grammar%23NPR land parcel database
topo grammar%23NPR topographic database
www.cs.waikato.ac.nz%2Fml%2Fweka such as threshold way combine through aggregation operator weight choice make base on knowledge provide by metadata about geometry capture process represent consistently grammar%23NPR semantics vocabulary
show fig. suggest choose geometry distance function customize confidence function every comparison geometry depend on their metadata for+example threshold confidence function weight spatial criterion adapt decide through some decision rule take as input metadata two geometry give as output parameter their comparison decision rule define by data match expert decision concern different case impact distinct parameter value distance threshold
behavior confidence function
weight spatial criterion
potential neutrality spatial criterion
when two geometry have different capture rule expect considerable gap between this case be less strict distance value i.e. define high distance threshold thus provide high confidence value same distance moreover when two geometry have bad absolute positional accuracy when capture base on some vague geographic entity increase decrease weight spatial criterion even remove from aggregation depend on estimated reliability
implementation silk be very+well known maintained data link tool many interesting feature in+order+to capitalize on asset silk implement our approach as describe fig. make silk compatible our adaptive approach geometry similarity evaluation
case silk confidence grammar%23NPR obtain as describe equation where d be distance compute between property value be threshold choose by user
d f d %C3%97 else as explain decision rule base on metadata metadata describe geometry impact confidence function behavior thus suggest change silk default confidence function replace by following function d f d d f d %C3%97 else where d be distance compute between geometry be parameter affect by decision rule represent threshold compute by sum absolute positional accuracy value geometry be convexity concavity factor respectively positive negative part confidence function both be positive value c.f.
when decision rule affect one three parameter confidence function become more+or+less strict grammar%23NPR github.com%2Fsilk-framework%2Fsilk%2Fblob%2Fmaster%2Fdoc%2Flinkagerules.mdk-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
global approach self adaptive comparison geometry figure implementation self adaptive linking approach distance grammar%23NPR idence figure variation confidence function behavior while keep monotony two other parameter be eventually necessary neutrality spatial criterion weight case weighted aggregation
parameter use by geometry comparison process define by+means+of decision rule base define in+advance by data linking expert rule base write grammar%23NPR piece knowledge about geometry capture process planimetric accuracy format as grammar%23NPR fact resulting value requested parameter represent as grammar%23NPR goal
backward reasoning execute by grammar%23NPR rule engine on whole rule base infer comparison parameter thus give as output from reasoning engine
then use together+with spatial distance compute confidence value spatial criterion respect formula grammar%23NPR markup+language wiki.ruleml.org grammar%23NPR describe by equation backward top-down reasoning work similarly most system give answer set goal by reasoning on base rule fact in+order+to make+sure that our approach work safely rule base be decidable since have fixed+number goal complexity reasoner be linear number rule declare rule base use grammar%23NPR silk every comparison result total quadratic complexity this implementation be scalable though because remain compatible grammar%23NPR version silk
grammar%23NPR monument evaluate our adaptive geometry similarity measure apply on dataset on historical monument first be grammar%23NPR be national+monument registry produce maintain by french ministry culture communication m%C3%A9rim%C3%A9e database contain instance provide as csv file monument locate by textual address transform data rdf grammar%23NPR platform then select only monument locate coded monument by link their address their corresponding feature second come from present https%3A%2F%2Fwww.data.gouv.fr%2Ffr%2Fdatasets%2Fimmeubles-proteges-au-titre-des-monumentshistoriques grammar%23NPR datalift.org grammar%23ENT address database grammar%23NPR french national mapping agency adaptive approach interlink grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america metadata about absolute positional accuracy geometry capture rule m%C3%A9rim%C3%A9e resource be quite straightforward indeed their geometry come from database be well-documented traditional geographic database rule provide information about four different geometry capture rule apply address at address sign by projection on corresponding street centerline by interpolation on corresponding section street centerline more rarely center addressing zone depend on geometry capture rule three different positional accuracy value be possible grammar%23ENT grammar%23ENT structure metadata accord vocabulary present add m%C3%A9rim%C3%A9e geometry grammar%23NPR update query case monument use approach present section learn their metadata
comparative matching test in+order+to evaluate our approach perform two spatial matching task on dataset describe above silk default spatial distance operator our adaptive geometry similarity measure
silk default spatial distance operator confidence value each comparison compute accord formula perform run different distance threshold find+out value give good result top table grammar%23NPR outline result good f measure obtain grammar%23ENT runtime this approach be around grammar%23ENT
our approach use two rule base define parameter see section since be mono criterion match task parameter not need since geometry two dataset be point only choose use euclidean distance measure between with+respect+to usually do geographic data matching define threshold as sum positional accuracy two geometry here name grammar%23NPR grammar%23NPR when geometry capture rule target different spatial characteristic grammar%23NPR grammar%23NPR real+world entity intend represent e.g. first geometry be barycenter building while second be point facade building add bias named deltac when target different type real+world entity grammar%23NPR grammar%23NPR add another bias name deltah our use case set deltac at 10%2C000%2C000 deltah at 15%2C000%2C000 rule base grammar%23NPR summarize below result get show on grammar%23NPR t h e t x ho t grammar%23NPR cha race l grammar%23NPR grammar%23NPR x grammar%23NPR grammar%23NPR
h e t x ho t grammar%23NPR cha race l grammar%23NPR charace grammar%23NPR x grammar%23NPR grammar%23NPR d e l t c t h e t x ho t grammar%23NPR x grammar%23NPR grammar%23NPR d e l t h
s t n c e e u c l d n
l pha lambda f l s e
on experience test perform rule base grammar%23NPR define second rule base by add more fine-grained rule namely grammar%23NPR this rule base set value deltac at 20%2C000%2C000 deltah at grammar%23NPR also change convexity confidence function when geometry capture rule be different when real+world geographic entity be too vague too wide neutralize spatial criterion additional rule grammar%23NPR deltah estimate by investigate bias some case where two geometry different geometric modeling locate two equivalent resource table instance match result compare reference link set produced grammar%23NPR information complete manually precision recall f measure 84.55%25 20.9%25 33.51%25 74.15%25 39.33%25 51.4%25 64.15%25 51.46%25 57.11%25 57.96%25 58.88%25 58.42%25 50.09%25 63.15%25 55.86%25 44.75%25 65.17%25 53.06%25 40.43%25 66.97%25 50.42%25 36.76%25 68.31%25 47.8%25 33.99%25 69.44%25 45.64%25 31.68%25 70.34%25 43.68%25 using learning result rule base precision recall f measure grammar%23NPR 70.43%25 58.88%25 64.14%25 grammar%23NPR 71.43%25 62.92%25 66.91%25 use learning result correction rule base precision recall f measure grammar%23NPR 73.46%25 59.1%25 65.5%25 grammar%23NPR 70.99%25 62.7%25 66.59%25 describe below result also present grammar%23NPR
rule replace last line grammar%23NPR lpha ho t grammar%23NPR cha race l grammar%23NPR charace l grammar%23NPR
lpha ho t grammar%23NPR cha race l grammar%23NPR charace l grammar%23NPR
lpha ho t ho t
t r u e ho t lambda t r u e ho t grammar%23NPR
t r u e ho t commune lambda t r u e ho t commune
f l s e ho t grammar%23NPR ho s t grammar%23NPR ho s t commune ho s t commune
runtime matching task use rule base be grammar%23NPR
discussion knowledge about cause heterogeneity between geometry by use supervise learning method show promise result nonetheless choice adequate learning feature condition by context data have important impact on result for+instance run another learning test on address data city this case geometry have also three possible geometry capture rule center building at entrance building on+the+street centerline
this city building size their distance road be more homogeneous than this case simple learning feature such as grammar%23NPR be sufficient obtain very good result choice training set be also crucial entity be clearly representative different learning class classification error induce by learning step show low effect on final linking result compare improvement bring by linking approach
matching result our approach show clearly good fmeasure score than default approach adapt parameter geometry comparison measure ensure grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
both small big distance threshold compare good result classical approach avoid 50%25 false+positive link use grammar%23NPR 40%25 use grammar%23NPR not significantly add new true positive link use grammar%23NPR but increase their number by 6%25 use grammar%23NPR
our approach have clearly high runtime complexity implementation depend on size rule base be why try define minimum number decidable rule sufficiently adapt parameter confidence function more detailed rule base provide good result but be much less efficient runtime user find good trade-off between efficiency performance as+a+matter+of+fact our approach be good suited instance matching task data+source have high spatial density instance describe by geometry lot internal geometric heterogeneity
conclusion grammar%23NPR this work tackle problem geometry similarity evaluation georeference resource linking propose ontology represent knowledge about geometry positional accuracy capture rule approach extract from considered spatial data geographic reference data by use automatic supervised learning also define data match approach that rely on this knowledge adapt comparison geometry during runtime matching result show good performance than classical adaptive approach
main downside our approach be time complexity current implementation improve do by add cache system reasoning result in+order+to reduce workload reasoning engine further test bigger more heterogeneous dataset especially dataset different type geometry also bring new insight this proposal future test also include two remaining aspect grammar%23NPR semantics namely geometry resolution vagueness populate interlink approach
grammar%23NPH formalisation acquisition en oeuvre connaissances pour virtuelle base donn%C3%A9e g%C3%A9ographiques les sp%C3%A9cifications coeur du processus int%C3%A9gration ph.d. dissertation universit%C3%A9 est.
grammar%23NPH grammar%23NPH grammar%23NPH general framework conflation extended abstract volume grammar%23NPR grammar%23ENT
grammar%23NPH l grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH efficiently computable metric compare polygonal+shape
new+york+state
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR using schema translator bidirectional interpreter
grammar%23ENT https%3A%2F%2Fwww.w3.org%2F2004%2F12%2Frules-ws%2Fpaper%2F49 geometrical matching polygon gis assessment geometrical quality polygon grammar%23ENT
grammar%23NPH partial matching planar polylines similarity+transformation 8th annual grammar%23NPR symposium on discrete algorithm 777+786
match old hydrographic vector data from grammar%23NPR map
grammar%23ENT page
grammar%23NPH grammar%23NPH grammar%23NPH on spatial database integration grammar%23ENT 335+352
grammar%23NPH flexible approach planning schema match algorithm on+the+move grammar%23NPR grammar%23ENT grammar%23ENT 249+264
grammar%23NPH grammar%23NPH learning concise pattern interlink extended version space grammar%23NPR institute+of+electrical+and+electronics+engineers grammar%23NPR joint conference on vol. institute+of+electrical+and+electronics+engineers
grammar%23NPH grammar%23NPH assess planimetric accuracy georeference data on grammar%23NPR case+study on grammar%23NPR
grammar%23NPR grammar%23ENT workshop conjunction grammar%23NPR conference
grammar%23NPH data link semantic+web semantic+web ontology knowledge+base enable tool service application grammar%23ENT
jahanara+foundation grammar%23NPR grammar%23NPR estimation donn%C3%A9e g%C3%A9ographiques application longueur et surface
universit%C3%A9 grammar%23NPR grammar%23ENT
s grammar%23NPR d grammar%23NPR connect linkedgeodata name spatial semantic+web 6th
grammar%23NPR tutoring system dedicate ontology matching enterprise interoperability ii springer 377+388
grammar%23NPH grammar%23NPH efficient multidimensional blocking link discovery without lose recall grammar%23NPR
international+standards+organization geographic information data quality international standard
www.iso.org
grammar%23NPH grammar%23NPH grammar%23NPH semantics similarity geographic information+retrieval grammar%23ENT grammar%23ENT
grammar%23NPH theory geographic concept ontological approach semantic integration
grammar%23NPH towards rule based matcher selection international conference on knowledge engineering knowledge+management springer 109+119
grammar%23NPH grammar%23NPH matching network different level detail grammar%23NPR grammar%23ENT 435+453
orchid reduction ratio optimal computation geo- spatial distance link discovery springer 395+410
grammar%23NPH grammar%23NPH grammar%23NPH integration semantically data by grammar%23NPR architecture grammar%23NPR
265+274
ana- grammar%23NPH olteanu-raimond grammar%23NPH grammar%23NPH knowledge formalization vector data matching using belief theory grammar%23ENT page
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
reference model grammar%23ENT rap.opengeospatial.org%2Form.php j grammar%23NPR grammar%23NPH find spatial equivalence ac cross multiple rdf dataset proceeding on foundation technology application geospatial 114+126
lt grammar%23NPR conceptual model generalisation multiple representation generalisation geographic information cartographic modelling application grammar%23ENT
s grammar%23NPR computer tractable translation geospatial data revue en ligne publi%C3%A9e par le european+commission grammar%23ENT
md grammar%23NPR grammar%23NPH et+al. grammar%23ENT efficient metric automatic weight generation properties instance match technique
preprint grammar%23NPR grammar%23ENT
grammar%23NPH systematic survey point set distance measure link discovery semantic+web grammar%23NPR cite on page grammar%23ENT
grammar%23NPH grammar%23NPH ontology matching state+of+the+art future challenge on knowledge data engineer grammar%23ENT 158+176
grammar%23NPH grammar%23NPH grammar%23NPH interlink geospatial information data bridge geographic information+science grammar%23ENT 119+139
grammar%23NPH iterative approach match multiple representation street data international archive photogrammetry remote+sensing spatial information+science part grammar%23NPR grammar%23ENT 101+110
grammar%23NPH grammar%23NPH match spatial data+set statistical approach geographical information+science grammar%23ENT 445+473
grammar%23NPR grammar%23NPH fu grammar%23NPH grammar%23ENT grammar%23NPR neural+network grammar%23NPH approach multi- represented feature match update propagation transaction gis grammar%23ENT 964+993
pattern grammar%23NPH approach matching node heterogeneous urban road+network transaction gis grammar%23ENT 718+739 forecast spreading technology research community open+university grammar%23NPR grammar%23NPR milton+keynes united+kingdom francesco.osborne%40open.ac.uk open+university grammar%23NPR grammar%23NPR milton+keynes united+kingdom andrea.mannocci%40open.ac.uk open+university grammar%23NPR grammar%23NPR milton+keynes united+kingdom enrico.motta%40open.ac.uk such as algorithm application format be important part knowledge produce reuse research process typically technology expect originate context research area then spread contribute several other field for+example semantic+web technology successfully adopt by variety field e.g. information+retrieval human computer interaction biology many unfortunately spreading technology research area be slow inefficient process since be easy researcher be unaware potentially relevant solution produce by other research community this paper that be possible learn typical technology propagation pattern from historical data exploit this knowledge anticipate where technology adopt next ii alert relevant stakeholder about emerging relevant technology other field do so propose technology topic framework novel approach approach use semantically enhanced technology topic model forecast propagation technology research area formal evaluation approach on set technology semantic+web artificial+intelligence area produce excellent result confirm validity our solution information+system information+retrieval computing methodology artificial+intelligence keyword scholarly data semantic+web technology ontology introduction every new piece research no+matter how grind break adopt previous knowledge reuse tool methodology from past as emphasise by grammar%23NPH researcher stand on shoulder giant constantly reuse idea method material today as number paper available scientific+knowledge grow rapidly become increasingly hard keep+track all relevant knowledge methodology facilitate research initiative trigger research idea spark collaboration between expert from different field availability research material on presence academic search+engine e.g. system system support exploration research environment e.g. semantic grammar%23NPR grammar%23NPR saffron grammar%23NPR alleviate only marginally this issue indeed while system be effective at processing keyword based query on literature at produce variety analytics about research landscape not attempt represent explicitly knowledge describe scholarly publication vision underlie work present here be researcher assist by software capable apply datadriven methodology machine readable description research knowledge aim be expand conceptual horizon researcher combine human creativity data+mining ability computer semantic+web community already start work this direction by foster paradigm create bibliographic repository generate knowledge+base biological data formalise research workflow implement system manage nano- publication micropublications organise relevant workshop e.g. linked science grammar%23NPR at grammar%23NPR grammar%23NPR at grammar%23NPR save-sd at challenge e.g. challenge create variety ontology describe scholarly data e.g. grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR recently grammar%23NPR propose even more ambitious vision suggest development artificial intelligence+system able make major scientific discovery biomedical+science win technology such as algorithm format application be very important part knowledge produce reuse https%3A%2F%2Fwww.semanticscholar.org ontoware.org%2Fswrc bibliontology.com purl.org%2Fspar%2Fbido purl.org%2Fspar%2Ffabio grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery association+for+computing+machinery grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148030k-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
t
s.
t
s.
t
s.
t
s.
c c c grammar%23ENT t
s.
note figure dissonance example dissonance grammar%23NPR po grammar%23NPR po not con interval grammar%23NPR po grammar%23NPR po con interval grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR con interval grammar%23NPR po grammar%23NPR po dissonance rule use as macro define five subtype model canonical suspension for+instance be dissonance such that dissonant interval occur on accentuated beat voice grammar%23NPR remain place while voice grammar%23NPR move downward suspension grammar%23NPR po grammar%23NPR po grammar%23NPR grammar%23NPR po grammar%23NPR po grammar%23NPR po grammar%23NPR grammar%23NPR grammar%23NPR po grammar%23NPR po weight po weight grammar%23NPR example show advantage rule based approach clear concise unambiguous expression expression serve as definition dissonance concept generation fact clearly some analytic procedure fall beyond rule language expressiveness case pure algorithmic approach use
conclusion develop generic methodology knowledge modeling extraction exploitation rely on core grammar%23NPR ontology specialized analytic model create one such model renaissance counterpoint analysis implement our methodology our digital grammar%23NPR
first achievement now extend enable collaborative annotation system open external contribution this endeavor hope contribute context music notation semantic+web vision open digital space knowledge document
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR do reusable musical data vol. ceur-ws.org
les dissonance dan la grammar%23NPR indication en vue une analyse assist%C3%A9e par ordinateur grammar%23ENT
m%C3%A9lodique harmonique polyphonie vers les ph.d. dissertation universit%C3%A9
grammar%23NPH grammar%23NPH grammar%23NPR toolkit computer-aided musicology grammar%23NPR intl. conf. on music information+retrieval 637+642
neuma.huma-num.fr%2Fquality grammar%23NPH grammar%23NPR metadata vocabulary grammar%23ENT http www.ldodds.com%2Fprojects%2Fmusicbrainz%2Fschema%2Fmb.html grammar%23NPH grammar%23NPR notation analysis w. b
grammar%23NPH 113+124
grammar%23NPH grammar%23NPH grammar%23NPH said grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR semantic+web rule language combining owl grammar%23NPR world+wide+web+consortium member submission
grammar%23ENT www.w3.org%2Fsubmission%2Fswrl grammar%23NPH music information+processing use humdrum toolkit concept example lesson computer+music journal grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1162%2F014892602760137158 counterpoint polyphonic vocal style sixteenth century third ed.
music vocabulary grammar%23ENT www
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR semantic annotation indexing retrieval
semantics science service agent on grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH
computer music grove music online grammar%23ENT www

grammar%23NPH grammar%23NPH et+al. grammar%23ENT owl web ontology language overview world+wide+web+consortium recommendation grammar%23ENT
grammar%23ENT music-encoding.org grammar%23ENT
grammar%23ENT
grammar%23NPR grammar%23ENT grammar%23NPR proposal overview https%3A%2F%2Fw3c.github.io%2Fmnx overview grammar%23ENT accessed grammar%23ENT
grammar%23NPH grammar%23NPH semantic import approach partial ontology reuse grammar%23NPR int. conf. on ceur-ws. org
grammar%23NPR intl. conf. on music information+retrieval

grammar%23NPH grammar%23NPR intl.
on musical
r grammar%23NPH p grammar%23NPH b grammar%23NPH annotation protocol
grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH annotation data+model grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH schematize treatment dissonance grammar%23NPR counterpoint grammar%23NPR intl.
on music information+retrieval 645+651
grammar%23NPH grammar%23ENT grammar%23ENT melisma music analyzer www.link.cs.cmu.edu%2Fmusic-analysis grammar%23ENT grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH semantic annotation knowledge+management requirement survey state+of+the+art semantics science service agent on grammar%23ENT
grammar%23NPH ontology based information extraction introduction survey current approach
information+science grammar%23ENT 306+323 adaptive approach interlink data univ. grammar%23NPR grammar%23NPR saint-mande feliachi%40ign.fr grammar%23NPR grammar%23NPR saint-mande nathalie-f.abadie%40ign.fr grammar%23NPH grammar%23NPR faycal.hamdi%40cnam.fr abstract resource publish on data often describe by spatial reference such as coordinate common data link approach mainly base on hypothesis hypothesis spatially close resource be more likely represent same thing however this assumption be valid only when spatial reference that compare produce same positional accuracy when actually represent same spatial characteristic resource capture unambiguous way otherwise spatial distance based matching algorithm produce erroneous link
this article first suggest formalize acquire knowledge about spatial reference namely their positional accuracy their geometric modeling their level detail vagueness spatial entity represent then propose interlinking approach approach dynamically adapt way spatial reference compare base on this knowledge
information system grammar%23NPR description language geographic+information+system data extraction integration data+description+language keyword spatial reference instance matching link data association+for+computing+machinery reference format grammar%23NPH adaptive approach interlink data grammar%23NPR grammar%23ENT grammar%23NPR grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery new+york new+york united+states+of+america https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148025 introduction interlinking data on be crucial step data publication process this step aim identify create link between resource represent same real-world entity relate each by some kind relationship existing approach use data link on mainly inspire by previous works on data+integration entity resolution data match subtask thus generally perform by compare value similar property use by resource from heterogeneous data+source describe real-world entity order publication right license association+for+computing+machinery association+for+computing+machinery acknowledge that this contribution be authored co- authored by employee contractor affiliate national government as+such government retain nonexclusive royalty-free right publish reproduce this article allow do so government purpose only
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148025 estimate degree similarity between resource high similarity score be between two resource more be likely represent same real-world entity tool like grammar%23NPR grammar%23NPR implement such approach allow one compare property value value describe resource by+means+of various distance measure each property comparison confidence value compute base on distance value parameter define by data linking expert then function apply aggregate confidence value into one value use decide whether create link not this pair resource
be typical multicriteria decision problem
data many resource associate via spatial reference location geographic space spatial reference be direct such as geographic+coordinates geometric primitive point linestrings polygon indirect such as postal address name administrative+unit like any other property spatial reference use evaluate similarity resource data match process field geographic data matching many measure propose evaluate similarity grammar%23NPR progressively implement data linking tool however design traditional geographic database matching not provide good result when directly reuse georeference resource open nature data+source mainly produce by crowdsourcing pose indeed new challenge geometric similarity evaluation due heterogeneity geometric quality within between data+source
this work follow intuition intuition improve spatial data match result require one adapt each pair geometry similarity evaluation characteristic tested geometry remainder this paper organize as+follows section present related work about geometry similarity evaluation on data section present vocabulary describe geometry characteristic characteristic take+into+account geometry comparison section detail our adaptive geometry similarity evaluation approach section detail experiment carry+out validate our approach
comparing work field geographic data matching focus on approach evaluate similarity geographic feature mainly base on evaluation their geometry similarity as two geographic feature locate far from each space grammar%23NPR silkframework.org grammar%23NPR aksw.org%2Fprojects%2Flimes.html grammar%23NPR sake brevity use term geometry instead direct spatial reference throughout this grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
not likely represent same real+world phenomenon geometry similarity prevail indeed over any other property similarity decide whether two geographic feature match not this section present approach propose field geographic data match geometric similarity evaluation in+order+to adapt georeference resource linking
geometry similarity measure geographic data match geographic database create through process abstraction real+world phenomenon geometry use provide quantitative description spatial characteristic real+world entity such as their dimension position size shape orientation
quality raw data+source discrete nature geometrical primitive spatial database spatial characteristic real+world entity capture only simplified way have impact on quality data+capture rule resulting geometry
similarity measure design geographic database matching be base on one more geometry similarity function s evaluate s geometry similarity with+respect+to some particular descriptor function choose depend on type geometry criteria with+respect+to compare distance function base on euclidean orthodromic elliptic curve spatial distance like min distance function deal location point set function base on inclusion geometry some buffer build around given geometry use compare geometry location surface distance use by compute similarity between polygon regard their location area their overlapping surface propose two measure compare polygon shapes respectively base on distance angle value grammar%23NPR grammar%23NPR distance deal both location shape linestrings widely use linestring polygon similarity evaluation see for+example use function compare linestrings orientation function compare polygon orientation area length also use function compare geometry neighborhood i.e. their topologically related spatially close geometry
geometry similarity measure use at+least one location-based similarity function this function combine function base on other descriptor that end standardize value between by means various normalization function eventuallyweighted aggregationmethods use compute overall standardized geometry similarity value
such as buffer size normalization function threshold use define extent difference between geometry regard give descriptor consider acceptable set such parameter usually assign expert expert define their values base on their knowledge about database match function behavior parameter relate evaluation location-based geometry similarity be most intuitive time represent maximum acceptable spatial distance between two geometry consider as potentially represent same real+world entity above this value geometry consider too far from represent same thing this parameter be thus closely related absolute positional planimetric accuracy database define by international+standards+organization standard as closeness reported coordinate value value accept as be true also affect by geometry capture rule geographic feature boundary vagueness for+example postal address represent by point capture various way within extent building locate by address at entrance building on centerline street in+front+of building etc. detail information need configure confidence function use geometry similarity evaluation base on location neighborhood orientation
geometry similarity measure design complex geometry such as linestrings polygon combine several geometric similarity function usually do overcome some data+integration conflict due difference level detail dataset i.e. degree geometric semantic abstraction use represent real+world entity geographic dataset geographic database integration conflict cause by difference level detail thoroughly describe by road+network match have different level granularity i.e. road segment be more detailed database than this conflict solve by similarity function base on spatial neighborhood road edge node
approach also apply geometry+transformation operation before computing geometry similarity in+order+to reduce gap between geometry compare for+example perform conflation lower location difference between road segment match due each database positional planimetric accuracy use generalisation algorithm harmonise level detail database
approach thus be propose compute geometry similarity geographic database matching progressively introduce georeference resource linking
heterogeneity geometry on data unlike geometry geographic database geometry use on data be not main piece information resource describe be not necessarily available description resource besides spatial data on have various origin extract transform from geographic database provide by traditional data producer such as national mapping agency e.g. ordnance+survey data geo.linkeddata.es but also be fusion many data+source as produce by crowdsourcing e.g. grammar%23NPR such case inside same data+source spatial reference capture differently lead call internal heterogeneity
effort specifically recommendation grammar%23NPR world+wide+web+consortium working+group on good practice when publishing spatial grammar%23NPR tend solve syntactic heterogeneity geometry grammar%23NPR www.dbpedia.org grammar%23NPR www.geonames.org grammar%23NPR www.w3.org%2Ftr%2Fsdw-bp%2Fan adaptive approach interlink grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america figure internal geometric heterogeneity grammar%23NPR
data however crowdsourced geometry produce different capture rule at different level detail fig. show example internal heterogeneity geometry grammar%23NPR hotel locate by point capture on building while capture on roadway
grammar%23NPR be open voluntary data+source this discrepancy be due some difference positional accuracy geometry capture choice
geometry similarity measure present are design compare homogeneous geometry set set produce at same level detail same positional planimetric accuracy same capture rule thus be inadequate geometry use georeference linked data
geometry similarity function selection combination tuning choose automatically adequate setting matching process be subject work both ontology matching data link field still pose challenge approach such as propose take+advantage alignment vocabulary that structure data select properties properties compare distance measure use that purpose other approach address question tune parameter matching process such as approach propose automatically compute comparison criteria weight challenge tune ontology match process address by various approach propose approach choose comparison criteria compute decision+tree aggregate propose classification matching algorithm use set decision rule assess each ontology context adequate matching algorithm propose also approach base on decision rule tune matching process use metadata describe ontology describe matching algorithm self tuning matching process provide adaptation setting context matching while reduce intervention expert in+fact technique such as use decision rule provide materialization expert knowledge about good setting matching process accord different context
this work our intuition be use decision rule automatically select tune adequate similarity measure between geometry while take+into+account heterogeneity heterogeneity exist between every pair geometry
see previous section geometry use represent real+world geographic feature produce by different capture process therefore be different from one data+source another in+addition human material input error collaborative open nature some data+source on accentuate geometric heterogeneity within single dataset geometric level detail well-defined data specification allow understand meaning each geometry represent how capture how model how accurate be and+so+on in+other+words semantics carry by this geometry heterogeneity between geometry be therefore but difference their semantics thus define semantics as set geometry characteristic relate geometric level detail capture process name our ontology as so motivate by definition geographic data semantics give by as relationship between data real+world phenomenon represent
heterogeneity face geographic database match approach challenge face context data present section identify following characteristic characteristic be more likely affect setting spatial data match process absolute positional accuracy geometry geometry capture rule geometric modeling vagueness spatial characteristic geographic entity represent by geometry level detail data+source
vocabulary description propose grammar%23NPR call grammar%23NPR semantics semantics describe characteristic thus enable use as knowledge through interconnection process choose only four characteristic since assume that although be most important understanding heterogeneity only way take+advantage be make explicit indeed other geometric feature such as orientation elongation area etc. be implicitly present geometry therefore be not difficult extract on+the+fly
grammar%23NPR semantics ontology base on international+standards+organization standard on geographic database metadata international+standards+organization geographic data quality international+standards+organization also include work relate spatial entity vagueness geometry capture rule fig. show excerpt grammar%23NPR semantics ontology
grammar%23NPR semantics ontology enable associate each geometry element describe four characteristic geometry semantics list above as example positional planimetric accuracy describe by method evaluation result evaluationmethod specify whether evaluation result derive from another quality element assess from data this latter be most often employed data-based evaluation carry+out by look sample data their genealogy this reason use entity class from grammar%23NPR ontology
data.ign.fr%2Fdef%2Fxysemantics grammar%23NPR www.w3.org%2Ftr%2Fprov-o%2Fk-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
excerpt grammar%23NPR semantics ontology describe planimetric accuracy geometry
evaluation result describe by numerical+value as+well+as unit measure define from grammar%23NPR ontology
grammar%23NPR semantics ontology be task be very complicated case geographic data produce by map agency descriptive record metadata including geometric metadata be often providedwith dataset moreover dataset guarantee internal homogeneity geometrical characteristic same class geometry often have often same geometrical modeling same planimetric accuracy etc. even when geometry same class have different characteristic additional indication often provide explain different situation e.g. different possible planimetric accuracy address contrast context data metadata about method use georeference quality rarely provide in+addition dataset build collaboratively not necessarily provide guidance on how represent spatial reference even if do not necessarily guarantee that contributor comply guideline
describe following how populate grammar%23NPR semantics ontology presence absence geometric metadata
populating ontology when geometric metadata provide dataset metadata geographical data often provide descriptive file provide authoritative data mapping agency make populate our ontology much easy according+to metadata grammar%23NPR address database address point capture various location at address sign at entrance building 4%2C500%2C000 from axis street by projection from centroid plot by interpolation arbitrarily addressing area center city moreover different planimetric accuracy value geometry provide metadata easily translate into rdf data+structure accord our grammar%23NPR semantics ontology associate each geometry through grammar%23NPR data insertion query
qudt.org%2Fschema%2Fqudt grammar%23NPR french national mapping agency populating ontology when geometric metadata not provide identify geometric characteristic when not describe metadata be laborious task if perform manually each geometry data+source thus populate grammar%23NPR semantics ontology become complicated task deal this issue propose two-step approach that automatically identify geometry capture rule first find each resource within same dataset characteristic element form choose when coordinate use locate enter identify this characteristic allow then evaluation planimetric accuracy spatial reference this latter carry+out by adapt direct estimation method absolute planimetric accuracy geographical data collaborative data
propose use reverse+engineering mode identify different geometric modeling spatial reference start from main assumption that geometry result from intentional choice geometric modeling by contributor propose formulate different hypothesis on choice make by contributor when enter geometry resource hypothetical choice determine by visually compare spatial reference resource geometry use represent geographical entity semantically similar equivalent type within geographic dataset this empirical visual analysis allow mainly identify various hypothetical pattern trend geometric modeling choice emerge from data
next step be associate each spatial reference identified geometric modeling pattern formalize our problem as+follows have geometry grammar%23NPR set geometric modeling class grammar%23NPR therefore define set relevant descriptor d select set learning s grammar%23NPR grammar%23NPR s g in+order+to define classification function grammar%23NPH descriptor d be descriptive indicator value combination allow discriminate different class define propose analyze geometry resource comparison geometry geometry represent geographical entity semantically similar equivalent type within reference geographic dataset
descriptor therefore be distance relationship between analyzed geometry characteristic element shape geographical feature represent by reference geometry can for+example consider distance between analyze geometry closest linestring use represent road centerline reference dataset selection learning set consist find each class geometric modeling representative+sample easily recognizable geometry analyzed dataset then apply learning algorithm assign each geometry geometric modeling class
evaluate our approach apply resource from french grammar%23NPR describe historical monument monument grammar%23NPR spatially reference by grammar%23NPR longitude prop-fr latitude property grammar%23NPR extract from encyclopedia where location resource provide without any metadata about their geometric modeling their positional accuracy though by grammar%23NPR version grammar%23ENT
property grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH evaluation following evaluate fused multi modal embeddings on standard word similarity evaluation dataset compare single modal embeddings grammar%23NPR grammar%23NPR grammar%23NPR thereafter knowledge+transfer method apply improve grammar%23NPR embeddings word outside shared concept space this work pre- trained model utilize in+case+of inception-v34 train on 1%2C300%2C000 image grammar%23NPR train on 100%2C000%2C000%2C000 word since no kg entity representation complete kg be readily available train grammar%23NPR representation by running grammar%23NPR on result over grammar%23ENT concept representation
concept similarity evaluation consider extended list word pair similarity relatedness task therefore construct each word similarity dataset relevant subset concept representation be available all three modalities due low sample size some subset only men grammar%23NPR use validation our tri- modal embeddings subset be also available grammar%23NPR
fusion method present sec. use dimensions as discover that dimensions be sufficient encode relevant information align representation word similarity task further weighting modality when combine optimize over all evaluation subset be intermediate step learn weighting proportion also following knowledge+transfer resulting optimal weight triple report table exhibit similar modality proportion all combination approach
weight combination method wt ext grammar%23NPR west+virginia isual table performance weighted multi modal combination method report weighted combination method substantially outperform single modal grammar%23NPR overall weighted fusion modality successfully improve single modal embeddings tri- modal concept space
evaluate our augmented grammar%23NPR embeddings as describe sec. data instance not cover by multi modal concepts include list grammar%23NPR grammar%23NPR 287 grammar%23NPR rw-stanford grammar%23NPR grammar%23NPR not use validation fused tri- modal concepts therefore subdivide dataset into in- sample word pair cover concepts tri- modal shared space word pair as set remaining word pair
storage.googleapis.com%2Fdownload.tensorflow.org%2Fmodels%2Finception_dec_ grammar%23ENT zip grammar%23NPR code.google.com%2Farchive%2Fp%2Fword2vec grammar%23NPR wiki.dbpedia.org%2Fservices-resources%2Fdatasets%2Fdata-set-39 grammar%23NPR people.aifb.kit.edu%2Fsto%2Ftransfer table spearman rank+correlation score on subset evaluation dataset combined embeddings normalize weight
grammar%23NPR inception-v3 grammar%23NPR grammar%23NPR grammar%23NPR dragunov+svd grammar%23NPR grammar%23NPR artificial similarity set feature mask method create via concatenation embeddings from all three modality fusion grammar%23NPR single modal embeddings normalize weight weight triple wtext grammar%23NPR wvisual derive from in- sample experiment see table concepts from shared tri- modal space cosine similarity compute over 2%2C300%2C000 different concept pair scaling factor approximate feature wise correlation grammar%23NPR multivariate linear+regression grammar%23NPR as introduce sec. obtained scaling factor s adjust s s median s max s min s
performance grammar%23NPR grammar%23NPR knowledge+transfer approach sigmoid activation function report table on in- sample subset grammar%23NPR outperform grammar%23NPR on all evaluation set also improve original grammar%23NPR embeddings on all subset except+for grammar%23NPR however both approach exhibit relative small performance increase on full evaluation set compare grammar%23NPR em bedding while knowledge+transfer via grammar%23NPR work entity within common concept space neither grammar%23NPR grammar%23NPR extrapolate information well unseen concepts
rebuilding multi modal representation feature rebuild approach construct via concatenation embeddings from inception-v3 grammar%23NPR normalize weighted wvisual grammar%23NPR transform dimensional em bedding via grammar%23NPR including grammar%23NPR weight grammar%23NPR improved result compare visual feature only textual domain when learn transfer function grammar%23NPR embeddings show superior result textual information incorporate by concatenate grammar%23NPR artificial embeddings reconstruction
feature approximation this dimensional multi modal em bedding space two different approach examine linear+regression apply each artificial feature neural+network hidden unit train each artificial feature result report table
approach successfully improve grammar%23NPR embeddings on in- sample subset thus effectively encode knowledge from grammar%23NPR grammar%23NPR produce linearly decorrelated feature feature facilitate feature wise approximation
perform grid search step size on in- sample evaluation modal knowledge+transfer improve by look+at grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america table spearman rank+correlation knowledge+transfer approach on word similarity evaluation set thereby refer complete evaluation set in- sample out subset rw-stanford only covered subset word pair grammar%23NPR evaluate grammar%23NPR grammar%23NPR no in- sample word pair exist
grammar%23NPR mc- grammar%23NPR grammar%23NPR grammar%23NPR verb grammar%23NPR 999+771 30+287 stanford+university 143+130 grammar%23NPR all grammar%23NPR grammar%23NPR all out grammar%23NPR all out all out all out multi modal concept space when apply transfer function unknown data overcome limitation small multimodal concept space method extrapolate better than also dominate linear+regression approach full evaluation set except+for rw-stanford grammar%23NPR grammar%23NPR furthermore improve performance initial grammar%23NPR embeddings on all evaluation set but grammar%23NPR 287 rw-stanford grammar%23NPR also this enhancement grammar%23NPR embeddings not cause by local improvement concept embeddings cover tri- modal space since observe consistent performance improvement on subset interestingly show large performance dip initial grammar%23NPR embeddings on grammar%23NPR indicate that representation verb grammar%23NPR be systematically different from representation wordnet noun visual feature be available tri- modal shared space add+up effect that verb maintain dependency relation syntactic argument subject object can therefore not as easily model as noun also grammar%23NPR 287 rw-stanford knowledge+transfer fail due high abstraction complexity grammar%23NPR 287 be collection entity from grammar%23NPR distant relation while rw-stanford contain rare complex+word exclusively
semantic segmentation illustrate improvement embeddings provide entity segmentation plot fig. two grammar%23NPR entity type land vehicle bird calculate first two grammar%23NPR component respective embeddings two type
pure textual em bedding space grammar%23NPR fig. grammar%23NPR different type land vehicle bird be not as+well separated as other three plot fig. grammar%23NPR show good separability tri- modal space only show entity have representation all three modality text image kg red x blue fig. grammar%23NPR show that good separability tri- modal space transfer entity type land vehicle bird bird show black x green grammar%23NPR grammar%23NPR omit corresponding plot since be inferior when extrapolate instance
look+at outlier cluster see that harrier get close land vehicle turkey albatross get farther away from bird cluster this phenomenon cause by ambiguity word i.e. harrier turkey albatross be also respective name military jet country plane so be only natural that harrier as air vehicle get closer other vehicle turkey albatross get farther away from bird cluster
summary indicate that tri- modal fused embeddings exhibit good entity cluster separation than grammar%23NPR alone this structural improvement successfully transfer uni modal concept approach so concept representation be this example indeed augment more intuitive way
concept abstractness further investigate word pair benefit most from transfer performance measure by rank difference predicted rank true rank similarity given word pair evaluation set relative performance improvement compare grammar%23NPR achieve if predicted rank word pair come closer human provided gold+standard ranking example word pair high performance gain relative grammar%23NPR see table while word pair pair not benefit even decrease performance show table grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH rover turkey harrier albatross grammar%23NPR albatross albatross albatross albatross b tri- modal space rover turkey harrier albatross rover turkey harrier albatross c rover turkey harrier albatross rover turkey harrier albatross rover turkey harrier albatross d first two grammar%23NPR component various land vehicle red black x bird blue green red x blue represent tri- modal space while black x green do not have tri- modal representation segmentation two different concept type improve tri- modal space b improvement successfully transfer back by both c d
good bad change due transfer find indication that more concrete word pair better transfer result
make concreteness quantifiable calculate proxy abstractness base on wordnet hierarchy given word compute depth subtree hyponym all word+sense that list this particular word grammar%23NPR hierarchy weight depth score by grammar%23NPR sense number depths frequently word+sense be more important result average abstractness score each word high score mean that word aggregate many+more specific meaning be thus proxy abstraction+level word our investigation word type i.e. verb adjective noun and+so+on be inconclusive
example word pair performance gain
word pair abstractness men bloom rise dragonfly underwater guy rusty rw-stanford angry huffy kingship rank 1%2C234 princedom rank 1%2C234 grammar%23NPR strike say affect apply since abstractness seem be good indicator transfer success analyze how different modality deal different level abstractness illustrate effect calculate mean abstractness score most little improved word pair relative grammar%23NPR each evaluation grammar%23NPR table see abstractness score within tri- modal space table textual space no tri- modal representation be available
average abstractness score both word in+a+word pair
example word pair performance decrease
word pair abstractness men animal zoo cute mammal nest reptile rw-stanford regionalism address membership relationship brandish expose grammar%23NPR happen produce provide show observe that augment word embeddings deal very+well word pair high abstractness within tri- modal space especially representation abstract concept benefit from this more holistic modelling through complementary information source cause by ability transfer function incorporate multi modal information since textual grammar%23NPR embeddings struggle abstract concept performance gain be most prominent case visual+space be good capture similarity abstract concept presumably partly cause by use wordnet hierarchy grammar%23NPR 1%2C000 dataset while kg result be inconclusive
evaluate augment word embeddings concepts outside tri- modal concept space interesting shift observe as depict table overall concrete word pair improve even evaluation set similar concepts transfer training space e.g. word pair men relation between concrete word improve by knowledge extrapolation make+sense since representation abstract concept be presumably incomplete grammar%23NPR thus learned transfer function confront inaccurate concept representation representation impose additional noise apart+from approximation error outside tri- modal space
multi modal embeddings be+well structured grammar%23NPR modal knowledge+transfer improve by look+at grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america table mean abstractness score word pair shared tri- modal space respective evaluation dataset text visual kg embeddings good bad performing word pair average in+case+of relative improvement through multi modal information capture by average abstractness score most little improved word pair ranking improvement measure relative grammar%23NPR i.e. whether ranking come closer human provided gold+standard ranking
visual top bottom top bottom top bottom top bottom top bottom men grammar%23NPR table mean abstractness score grammar%23NPR improve word pair respective evaluation dataset ranking improvement measure relative grammar%23NPR
top bottom top bottom men grammar%23NPR grammar%23NPR grammar%23NPR 287 grammar%23NPR rw-standford grammar%23NPR grammar%23NPR abstract concept this information be difficult transfer grammar%23NPR em bedding restructure grammar%23NPR space most challenge scenario high abstractness score
summary key finding our key finding provide interesting insight knowledge+representation concept general concept embeddings in+particular concept similarity concept representation fuse three modality come closer human notion similarity than single modal embeddings less self-evident be that word embeddings without matching concepts other modality improve by transfer abstract knowledge from other modality
segmentation tri- modal space improve semantic segmentation concept specifically homonym again not self-evident be that also extrapolate transfer space
abstractness abstract word improvemost by combine embeddings from different modality when transfer multi modal knowledge embeddings grammar%23NPR more concrete word benefit
grammar%23NPR recent year two line research representation learn exploit information modality emerge on+the+one+hand fusion embeddings achieve independent training each modality on+the+other+hand embeddings each modality fused em bedding space jointly optimize our approach be first category best our knowledge first approach approach exploit available embeddings more than two modality extrapolate cross-modal knowledge from concept not cover by all modality
first category include construct bimodal concept representation by concatenate independently train visual textual representation instead visual representation et+al. use textual text corpus embeddings hierarchical+structure learn grammar%23NPR embeddings then concatenate transform various statistical method e.g. grammar%23NPR obviously fusion approach above not include visual textual structural knowledge at+the+same+time approach approach combine three source i.e. multiple language image be however since text represent as polylingual topic task reduce learning bi modal embeddings bandar+seri+begawan et+al. extend word representation knowledge from image same tag while tag based image dataset cover large number concepts than grammar%23NPR also introduce noise training alignment step still not cover enough concepts so that our knowledge+transfer approach become obsolete
example joint optimization approach from second category be word embeddings enrich information from knowledge graph another approach joint learning word embeddings kg embeddings represent same vector+space investigate link word embeddings knowledge graph embeddings during training alignment function but joint learning approach mention so+far be able deal instance missing information some modality joint optimization approach approach be related our approach this aspect be since address bi modal information transfer instance not cover by all modality use deep visual semantic em bedding model model learn image tag embeddings simultaneously
be able improve visual embeddings unseen image category through information encode co-occurrence grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH text corpus combination embeddings audio video source examine apply encoder decoder system joint learning embeddings be also able handle missing data from modality during training
contrast joint optimization approach deal missing information bi modal embeddings through jointly learned common em bedding space instead infuse knowledge training impose restriction on learning objective em bedding dimension in+addition all joint learning approach so+far restrict two modality not exploit available pre- trained embeddings another drawback joint optimization approach real-world application be that embeddings learn from aligned corpus unstructured content be hard interpret by human since not be related concept explicit semantics for+instance entity segmentation plot like fig. can not directly construct
conclusion grammar%23NPR this work contribute area knowledge+transfer between multi modal concept representation our approach be able fuse unstructured information concept from text image structured information from kg meaningful way aim towards make computational knowledge+representation get close human like perception concept
overcome restricting number concepts available certain modality like visual+modality novel transfer approach multi modal information present extrapolate information from small common concept space only concepts large space another modality our case 3%2C000%2C000 word embeddings
word embeddings benefit from extrapolate information modality even concept not represent other modality our study indicate that apply most concrete concepts homonym
be confident that our finding spawn more research cross-modal knowledge capture both in+order+to extend quality expressiveness coverage cross-modal representation but also obtain fine-grained understanding knowledge benefit how why
near future like broaden our empirical evaluation task other modality like link prediction knowledge graph also extend number concepts tri- modal space highly improve transfer result including visual representation image tag help validate that
grammar%23NPH grammar%23NPH unsupervised model instance level proceedings grammar%23ENT conference on empirical method natural+language+processing grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT 278+289
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH translate embeddings modeling multi relational data advance neural information+processing+system
grammar%23NPH multimodal distributional semantics grammar%23ENT
grammar%23NPH grammar%23NPH community evaluation exchange word vector at wordvectors.org proceedings 52nd annual meeting system demonstration
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH place search context proceedings 10th international conference on

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al. grammar%23ENT devise deep visual semantic grammar%23NPR model advance neural information+processing+system
single multiple combine from text wordnet on artificial+intelligence
grammar%23NPH represent verb as argument concept grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR learn word relatedness constraint proceeding 18th association+for+computing+machinery grammar%23NPR international conference on knowledge discovery data+mining
grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPR evaluate semantic model genuine similarity estimation computational+linguistics grammar%23ENT 665+695
grammar%23NPH use convolutional neural+network improved multimodal semantics grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH knowledge graph semantic+web grammar%23ENT
640+655
thang grammar%23NPR grammar%23NPH grammar%23NPH good word representation recursive neural+network morphology proceedings seventeenth conference on computational natural language+learning
104+113
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
representation word phrase their grammar%23NPR
advance neural information+processing+system
grammar%23NPH grammar%23NPH contextual correlate semantic similarity language cognitive+process grammar%23ENT
multi modal grammar%23NPR lingual cross-modal retrieval advance information+retrieval 37th on ir research grammar%23NPR grammar%23ENT cham https%3A%2F%2Fdoi.org%2F10.1007%2F978-3-319-16354-3_9 grammar%23NPH grammar%23ENT multimodal deep proceedings 28th international conference on machine+learning grammar%23NPR 689+696
grammar%23NPH grammar%23NPH holographic embeddings knowledge graph proceedings on artificial+intelligence grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPR grammar%23ENT grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH glove global vector word representation grammar%23NPR grammar%23ENT grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
word at time computing word relatedness use temporal semantic analysis grammar%23NPR 20th international conference on 337+346
grammar%23NPH grammar%23NPH contextual correlate synonymy grammar%23NPR association+for+computing+machinery grammar%23ENT 627+633
grammar%23NPH grammar%23NPH hao su grammar%23NPH grammar%23NPR grammar%23NPR grammar%23NPH grammar%23NPH
grammar%23NPR large scale visual recognition challenge
grammar%23NPR grammar%23ENT 211+252 http doi.org%2F10.1007%2Fs11263-015-0816-y grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH rethink inception architecture computer+vision grammar%23ENT on computer+vision pattern+recognition grammar%23NPR grammar%23ENT las+vegas united+states+of+america grammar%23ENT
grammar%23NPH knowledge fusion via embeddings from text knowledge graph grammar%23NPR grammar%23NPR preprint grammar%23NPR grammar%23ENT
grammar%23NPH towards holistic concept representation relational knowledge visual attribute distributional word semantics semantic+web grammar%23ENT
knowledge graph grammar%23NPR grammar%23ENT grammar%23ENT
yangtze+river xu grammar%23NPR bai bin gang grammar%23NPH red+cross national+educational+television general framework incorporate knowledge into word representation proceedings 23rd on conference on information+management knowledge+management grammar%23ENT grammar%23ENT
grammar%23NPH verb similarity on taxonomy wordnet proceedings grammar%23NPR grammar%23ENT best term recall first topic score 47.2%25 significantly high than value 44.7%25 random grammar%23NPH p value 42.5%25 decision+tree p first topic grammar%23NPH recall be 35.1%25 again significantly high p than 32%25 random grammar%23NPH 31.5%25 decision+tree figure average recall six machine+learning approach on first n topic f1 distribution six algorithm result significantly different when compare grammar%23NPR test multiple correlated distribution p zoom again on three top performer also base on result obtain precision recall carry+out three one-to-one direct comparison random grammar%23NPH versus grammar%23NPH random grammar%23NPH versus decision+tree grammar%23NPH versus decision+tree first comparison obtain international+criminal+court grammar%23NPR excellent agreement between two algorithm confirm by grammar%23NPH test p no significant difference decision+tree be instead significantly different from both random grammar%23NPH grammar%23NPH p low value international+chamber+of+commerce grammar%23NPR international+criminal+court respectively grammar%23NPR discussion evaluation confirm our initial hypothesis be indeed possible learn from historical spreading pattern forecast technology propagation at+least set topic more involve technology propagation events in+particular good value precision indicate be able safely produce sound suggestion researcher field recall seem confirm intuition intuition be not possible forecast all propagation events only on basis previous propagation pattern our study suggest that random grammar%23NPH perform best term precision while grammar%23NPH offer advantage term recall be possible that high number example favour neural+network approach other similar approach approach need very large example set however currently be still challenge generate large dataset technology data since main aim grammar%23NPR be forecast set propagation high precision adopt random grammar%23NPH as default table show good research area term f1 score when adopt random grammar%23NPH naturally grammar%23NPR tend work best on topic associate large set publication technology however performance also depend on inherent nature topic their behaviour term adopted technology for+example research area about computer+network sensor peer-to-peer system perform well since usually associate coherent set technology similarly research area associate image+processing field also yield good result receive constant flow approach format elaborate describe image table performance random grammar%23NPH on first topic at+least positive label order by f1 score table example topic correctly forecast by grammar%23NPR table show as example seven technology take consideration grammar%23ENT grammar%23ENT period set topic where propagate few year as correctly forecast by grammar%23NPR for+example grammar%23NPR correctly suggest that novel at that time extreme learning machine machine be feedforward neural+network single layer hidden node deem grammar%23NPR apply topic such as speech+recognition computer+vision robotics similarly be able forecast that adopt by semantic+web area as+well+as by community work on data+reduction image+compression data security and+so+on grammar%23NPR also anticipate strong propagation semantic+web format such as grammar%23NPR semantic+web rule language several research area such as bioinformatics social+network electronic+commerce e-learning so conclusion this paper present technology topic framework novel approach approach characterise technology term relevant research topic over time forecast technology propagation research area implementation system evaluate on set 1%2C118 technology field semantic+web artificial+intelligence yield precision 74.4%25 recall 47.7%25 first research area when use random grammar%23NPH result confirm that be possible use historical propagation pattern forecasting technology spreading nonetheless grammar%23NPR present some limitation intend address future work first train each classifier sequentially therefore be not very scalable this issue solve by parallelising training phase by adopt multi class multilabel classification second instance current method associate technology research paper be purely syntactic while this solution use by several technology forecasting system think that semantic characterisation research technology possibly yield good result therefore intend create ontology technology exploit map technology paper third instance grammar%23NPR only focus on technology spread pattern not take account other potentially significant factor such as person champion technology cost adoption presence usable implementation socio- political context plan enrich forecasting model by considering text generate feature possibly derive additional feature from external knowledge+base social+media aim be combine knowledge source forecast technology propagation events even when not match previously observed pattern also intend expand scope our work by including analysis more varied set research field such as biology social+science engineering finally intend create web+application enable researcher explore technology pattern receive tailored suggestion technology support their research work acknowledgment thank provide grammar%23NPR dump reference tang grammar%23NPH grammar%23NPH yao grammar%23NPH li grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPR extraction mining academic social+network proceeding 14th association+for+computing+machinery grammar%23NPR international conference on knowledge discovery data+mining 990+998 association+for+computing+machinery grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPH samp k. grammar%23NPR p. explore your research sprinkling some saffron on semantic+web dog+food semantic+web challenge at grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR e. grammar%23NPR p. explore scholarly data grammar%23NPR semantic+web grammar%23ENT 460+477 springer grammar%23ENT grammar%23NPR d. semantic publishing coming revolution scientific journal publishing learned publishing grammar%23ENT grammar%23NPR a.g. gentile grammar%23NPH grammar%23NPR v. grammar%23NPR a. semantic web+conference ontology re factoring solution grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR m.a. grammar%23NPR n. grammar%23NPR p. grammar%23NPR grammar%23NPH grammar%23NPR towards mashup build bioinformatics knowledge system journal biomedical informatics grammar%23ENT grammar%23NPR grammar%23NPR r. fellow d. grammar%23NPH a. grammar%23NPR d. grammar%23NPH s. soiland-reyes s. grammar%23NPR i. grammar%23NPR a. fisher p. grammar%23NPH grammar%23NPH taverna workflow suite design execute workflow on desktop cloud nucleic+acid research grammar%23ENT grammar%23NPR p. grammar%23NPR a. grammar%23NPR grammar%23NPH anatomy nanopublication information+services use grammar%23ENT grammar%23NPR t. c. grammar%23NPR m. queralt-rosinach n. grammar%23NPR r. grammar%23NPR grammar%23NPH grammar%23NPR grammar%23NPR grammar%23NPR r. grammar%23NPR m. decentralized provenance aware publishing nanopublications grammar%23NPR computer+science grammar%23ENT grammar%23NPR grammar%23NPH grammar%23NPR p. grammar%23NPH t. grammar%23NPR grammar%23NPR use grammar%23NPR ontology open annotation data+model represent evidence within drug drug interaction knowledge+base 4th international conference on linked science vol grammar%23ENT ceur-ws. org. grammar%23ENT grammar%23NPR h. artificial+intelligence win prize beyond create engine scientific discovery ai magazine grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR h. grammar%23NPR e. grammar%23NPR technology from academic publication knowledge engineering knowledge+management 20th international conference grammar%23NPR grammar%23ENT 463+479 grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR h. october dr. inventor framework extract structured information from scientific publication international conference on discovery science 209+220 springer grammar%23ENT grammar%23NPR a. brush m. grammar%23NPR grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPR capitol+hill s. puerto+rico grammar%23NPR pol m. tan south+carolina washington+state n. a cultural shift publishing grammar%23ENT grammar%23NPR c. grammar%23NPR grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPR s. grammar%23NPR c. grammar%23NPR r. grammar%23NPR s. a crystallization point data journal grammar%23ENT grammar%23NPR m. grammar%23NPR r. forecasting emerging technology aid science technology database technological forecasting social change grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR a. grammar%23NPR a. grammar%23NPR e. automatic classification springer nature proceeding smart topic miner 383+399 springer grammar%23ENT grammar%23NPR grammar%23NPR grammar%23NPR e. grammar%23NPR integrate multiple source generate semantic topic network 408+424 grammar%23ENT grammar%23NPR v. grammar%23NPR m. grammar%23NPR r. grammar%23NPR k. grammar%23NPR h.a. grammar%23NPR c. porter grammar%23NPH on future technological forecasting technological forecasting social change grammar%23ENT i. forecasting technology within their socioeconomic framework technological forecasting social change grammar%23ENT grammar%23NPH grammar%23NPH park grammar%23NPH technology forecasting use topicbased patent analysis grammar%23ENT grammar%23NPR t.u. grammar%23NPR grammar%23NPH grammar%23NPH h. grammar%23NPR p. forecasting emerging technology use metrics patent analysis technological forecasting social change grammar%23ENT grammar%23NPR grammar%23NPR south+carolina technology forecasting patent strategy hydrogen energy fuel+cell technology grammar%23ENT jun s. s. d. technology forecasting use matrix map patent cluster industrial grammar%23ENT grammar%23NPR grammar%23NPR forecasting technology diffusion grammar%23NPR model technological forecasting social change grammar%23NPR a. grammar%23NPR d. grammar%23NPR p. providing access monitoring contextualizing open+access publication grammar%23ENT grammar%23NPR grammar%23NPH grammar%23NPH s. grammar%23NPH q. grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH fan grammar%23NPH topic orient community detection through social object link analysis social+network knowledge-based+system grammar%23ENT grammar%23NPR p. grammar%23NPR c. social+network perspective tourism research collaboration grammar%23NPR tourism research grammar%23ENT cross-modal knowledge+transfer improve apple by look+at grammar%23NPR faban.both%40student.kit.edu steffen.thoma%40.kit.edu rettinger%40.kit.edu abstract knowledge via learned latent vector representation word image entity show performance computer+vision computational+linguistics kg task recent result demonstrate that learning such representation modality be beneficial since each modality capture complementary information however approach limit concepts cross-modal alignment training data be only available just few concepts
visual object exist far fewer embeddings than word kg entity investigate whether word em bedding e.g. apple still capture information from other modality even if no match concept within other modality i.e. no image kg entity apple but orange as picture title analogy empirical result our knowledge+transfer approach demonstrate that word embeddings benefit from extrapolate information modality even concept not represent other modality interestingly apply most concrete concepts e.g. dragonfly while abstract concept e.g. animal benefit most if align concepts be available other modalities
concept information system multimedia information+system knowledge+transfer grammar%23NPH cross knowledge+transfer improve apple by look+at orange grammar%23NPR grammar%23ENT grammar%23NPR grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery new+york new+york united+states+of+america page
introduction unsupervised learning latent vector representation embeddings set concept become key technique different permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148026 research community capture raw information computable representation computer+vision visual object feature learn from large image collection like grammar%23NPR computational+linguistics word embeddings extract from huge text corpus semantic+web community embeddings entity obtain from large knowledge graph kg existing em bedding approach become increasingly sophisticated implementation extensively optimize train on huge dataset however well perform model only integrate subset all available knowledge e.g. only visual information consider when train image classifier reason be limitation available labeled aligned training data
used raw encoding method dataset be inherently different eachmodality learned embeddings fixed sized real valued vector such specialized model capture different knowledge about represented concept pose question if complement knowledge transfer between concept embeddings from different modality separately train
scalable solution this problem facilitate multi modal enrichment procedure without adapt domain specific model training task obviously have great potential since model benefit from advance other field e.g. by increased availability training data improved learning method
schematic illustration knowledge+transfer from fused tri- modal space arrow on right different circle depict concept space quantity represented concept single modal embeddings
transfer knowledge between em bedding space information about same concept align modality
key limitation be that intersection concept cover by all em bedding space be very small within today training data 1%2C000 our experiment while word embeddings be available millions word our data 3%2C000%2C000 see fig. there exist only small sample few thousand image embeddings reliably map word level 2%2C000 thus when try enhance large em bedding space like text information from much small space like image embeddings challenge become extrapolate concept not cover smaller image cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH this paper aim exploit existing embeddings from three modality image text kg in+order+to augment space word embeddings complement knowledge from other two modality see fig. achieve first construct trimodal concept space alike be able capture information concept be available all modalities sec. next propose two approach call transfer knowledge back into single modal em bedding space word thus circumvent limitation small multi modal concept space see sec.
sec. show that single modal embeddings e.g. word em bedding apple 3%2C000%2C000 word space benefit from transferred cross-modal knowledge even concept not explicitly represent tri- modal space since be no training sample available other modalities
finding be quite remarkable conduct more experiment shed some light on obvious question type concept do this extrapolation knowledge work outcome indicate that homonym benefit since sharedmeanings get disambiguate as show semantic segmentation plot sec. ii that abstractness concept appear be good indicator whether concepts benefit see sec.
sec. briefly summarize our finding before discuss related work sec. conclude sec.
grammar%23NPR grammar%23NPR modal concept space following em bedding model textual kg visual domain introduce subsequently method alignment modality fusion method embeddings within shared cross-modal space be alike be foundation our knowledge extrapolation approach learn em bedding augmentation from common multi modal em bedding space
single modal embeddings our work use following well-known em bedding grammar%23NPR textual domain grammar%23NPR model propose successfully apply large scale text corpus state+of+the+art textual representation nlp processing this work grammar%23NPR model use latent vector representation word phrase construct from their context
kg concept embeddings construct grammar%23NPR type constraint objective+function base on link prediction task show be scalable knowledge graph millions vertices good result
visual representation inception-v3 use construct classify image into multiple available category therefore convolutional filter apply multiple layer abstract local feature this grammar%23ENT please+note our transfer method be not limited three approach but also applicable any other em bedding approach like glove hole
last neural layer before function use as latent representation visual data
alignment multimodal concept space share knowledge between different embeddings alignment between establish therefore map previously mention single modal representation consolidated tri- modal shared concept space this work choose wordnet lexeme word as common concepts since numerous evaluation dataset word similarity task be available
textual representation grammar%23NPR not adjust since be already on word level
alignment grammar%23NPR representation perform by map kg entity addressable unique word therefore each kg concept map most commonly used surface form word refer kg concept
align visual embeddings word level single image representation from grammar%23NPR 1%2C000 aggregate wordnet synset representation via featurewise max operator alike do all image certain synset category additionally more abstract synset representation build by combine representation child node from wordnet hierarchy alignment synset representation lexeme perform wordnet directly since wordnet lexeme assign at+least one synset
modality fusion shared space obtain consolidated tri- modal shared space space capture knowledge all modality one representation modality fusion use aligned corpus n different lexeme represent three matrices contain latent vector representation three modality textual t knowledge graph g visual v
normalization each concept vector unit length representation weight individually grammar%23NPR wg grammar%23NPR
grammar%23NPR grammar%23NPR t wg g grammar%23NPR v grammar%23NPR wg grammar%23NPR besides simple concatenation grammar%23NPR concept vector from different modality modalities fuse use dimensionality reduction method like pca dragunov+svd transformation method compute on stacked thus dimension multi modal concept vector reduce statistical smoothing effect contribute overall performance improvement fuse embeddings see evaluation performance gain within tri- modal concept space
multimodal knowledge single modality when fuse multi modal information source size common multi modal space be limiting+factor useful application
our case aligned tri- modal concept space have only concepts hereby visual domain present major bottle grammar%23NPR modal knowledge+transfer improve apple by look+at grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america since human annotate object image accord classification task in+order+to overcome limitation small common concept space introduce our approach transfer knowledge from multi modal space back into single modal space utilized model neither complex multi modal learning from aligned corpus extensive multi modal training on+hand labeled sample need instead rearrange em bedding space single model mimic superior multi modal em bedding
following transfer approach subdivide into fitting transfer procedure fit all n sample shared concept space utilize afterwards transfer perform any em bedding fitted single feature mask feature mask approach factor correct feature single modal em bedding approximate from common concept space feature from original em bedding multiply learned feature mask in+order+to re- scale feature see grammar%23NPR grammar%23NPR
artificial similarity dataset compute all possible concept pair common concept space thereby similarity define by cosine vector similarity em bedding pair shared concept space construction fused multi modal embeddings any combination technique mention section can utilize set similarity score sim r n grammar%23NPR then use as supervise training set
em bedding grammar%23NPR fm from single modality m denote number feature fi r this modality each concept pair common concept space two single modal embeddings know difference vector d between vector pair compute d be element wise multiplication result n grammar%23NPR difference vector stack matrix d r n grammar%23NPR grammar%23NPR where di refer i-th column matrix d. difference vector scaling factor si estimate each feature fi high scaling factor indicate that feature be useful differentiate concepts accord similarity score sim vector such scaling factor s rm approximate via pair wise correlation so that si corr di sim another approach use coefficient multivariate linear+regression form sim d s
transformation all feature single modal em bedding compute by use scaling factor activation function s standard activation function be sigmoid function also work negative values si test many alternative sigmoid function turn+out be most reliable yielding grammar%23NPR presented transfer approach also test information transfer via fitted pca dragunov+svd transformation on multi modal concept space method however yield poor result information abstraction concept outside shared space
feature mask multiplication b multi modal feature rebuilding
information transfer approach single modal em bedding
good result note that value range s adjust accord activation function
feature rebuilding let consider simplistic example fruit only differentiate one specific feature if single modal em bedding assign feature value from high low apple coconut pear entity apple can by simply re scaling fruit feature never become more similar pear with+regard+to cosine similarity than coconut coconut stay between apple pear such case reordering need achieve by add additional feature single modal em bedding our feature rebuilding approach additional feature approximate feature from shared concept space but only construct feature single modal em bedding see grammar%23NPR grammar%23NPR
start point be common concept space k feature dimension dimension obtain by any combination technique from section transfer function then learn input from single modal em bedding m feature therefore all feature from common concept space rebuild input single em bedding reconstruct feature further refer as artificial feature each artificial feature k reconstruction function ri rm r learn
function ri fit via any regression method this work use linear+regression neural+network
given em bedding all artificial feature compute subsequently combine artificial em bedding ear grammar%23NPR rk transformed em bedding then obtain by concatenate original em bedding artificial grammar%23NPR ear t f icial ear t f icial grammar%23NPR that normalization be important since difference vector dimension value range not be meaningful otherwise see more detail on effect cap grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
comparison between paragraph vector entropy cluster
evaluation on project distance term un clustered problem confirm that proposed method successfully form several cluster grammar%23NPR
baseline method quantitative comparison fig. show relationship between cosine similarity number edge relationship between degree centrality number node i.e. project case cosine similarity
result confirm that edge high cosine similarity node high degree increase reason this result be because through use high entropy concept be significant scientific technological context scientifically unimportant word as element between paragraph vector paragraph vector be able comprise meaningful group simultaneously newly unknown synonym closely related word word not define thesaurus be unified cluster vector if be same cluster take centroid vector as representative vector cluster involve separate each cluster vector as+much+as+possible form clear difference vector+space
terms accuracy content similarity evaluation encounter difficulty since best our knowledge be no gold+standard evaluate distance semantics among document therefore first evaluate degree similarity base on sampling method randomly extract pair project cosine similarity make distribution similar entire distribution each pair have two project title description cosine value value divide into three level weak cos. middle cos. strong cos. some example two project their cosine value show table then three member our organization funding agency figure cosine similarity artificial data partial replacement
similarity each pair member provide prior explanation intended use graph some example evaluation member receive same data their background be bioscience psychology computer+science as result confirm that 78%25 project similarity i.e. distance graph match majority+vote member opinion example misjudge include e.g. two project including many synonym high cosine value two project project accidentally have similar sentence and+so+forth by+contrast accuracy distance baseline method be 21%25 evaluation result determine be fair agreement grammar%23NPR kappa
evaluate accuracy content similarity use artificial data part randomly replace other project replace 100%25 project description sentence randomly select from other project description then measure cosine similarity between vector generate from artificial project description vector original project description project randomly select from all project then evaluate 1%2C000 pair original project artificial project relationship replacement+ratio cosine similarity show grammar%23NPR
as result confirm that be obvious correlation between content similarity project their cosine similarity grammar%23NPR paragraph vector without entropy cluster also have same trend but vector entropy clustering have high similarity on+average this result match relationship between cosine similarity number edge show grammar%23NPR
evaluation on project semantics as describe above try extract semantics project relationship base on three approach however property relationship divide into commonality
semantics offset vector whereas middle point vector common concepts obviously represent commonality project offset vector represent more general relationship such as entail transition contrast science graph characterize recent scientific landscape use paragraph grammar%23ENT grammar%23ENT united+states+of+america figure largest cluster correspond area a1 fig.
effect part obtained relationship show grammar%23NPR grammar%23NPR column table for+example third line show that property neutrino neutrino oscillation facility connect by concept vector grammar%23NPR measurement thus link for+example by grammar%23NPR example interest further extension science graph but percentage such result be very low at 14.6%25 evaluation conduct same member previous section reason this poor result be that be not sufficient relational concepts concepts properly represent project relationship grammar%23NPR thesaurus since grammar%23NPR thesaurus construct mainly indexing research article most concepts represent research subject except+for low number general broad concept moreover project relationship cosine similarity such as last line be too close have meaningful relationship except+for grammar%23NPR
semantics middle point vector middle point vector between two project vector assume represent commonality two project however result show that even grammar%23NPR grammar%23NPR have cosine similarity middle point vector thus most concept vector not represent grammar%23NPR be both method show on left right grammar%23NPR since paragraph vector correspond project compose many concept vector be difficult one two concept vector represent commonality paragraph vector as result accuracy remain at 43%25
semantics common concepts part result show c entropy value column table be many common concepts between two project title description but entropy successfully select significant concept among result achieve 92%25 accuracy accord member evaluation result determine be correct when at+least three concept be correct thus as commonality relationship extraction common concepts entropy be more productive than common concept vector
example example how result from this science graph use practical manner compare graph category grammar%23NPR project i.e. security+insurance+company code then focus on middle cluster be center research project edge cluster be figure robot biotechnology cluster correspond area grammar%23NPR fig.
project because+of long distance from center bridge gradient between cluster bridge project provide information about relationship interdisciplinary approach between different subject suggest potential area future project fund agency since new cluster create on bridge in+the+future
describe four example finding from obtained science graph grammar%23NPR correspond area a1 grammar%23NPR
middle cluster be largest cluster more than 5%2C000 project correspond approximately 20%25 graph although require in-depth analysis inside cluster confirm that be three obvious bridge between cluster biotechnology grammar%23NPR medical biotechnology med social aspect soc information media inf information+processing information+systems ips
fig. correspond area grammar%23NPR fig. project about robot human sensor inf related studies grammar%23NPR med e.g. insert chip into human brain locate left right cluster respectively furthermore sociological ethical research soc run two cluster as bridge suggest possibility future collaboration such studies in+addition although need consensus novelty there seem be novel project on bridge such as see ear several biomimetic studies further investigate extraction such project
compare industrial project such as industrial manufacture reveal that such project increase six fold second half grammar%23NPR confirm that subject be initially limited but finally span wide range graph by+contrast some project life+science lif code code be obviously about aerospace technology grammar%23NPR in+fact cluster as grammar%23NPR project graph
case support certainty graph
conclusion future work navigate scientific landscape expect from enterprise research+laboratory policy analyst grammar%23NPR grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
example two project relationship fund agency however technique base on citation linkage be not applicable characterize most up-todate scientific landscape since project description not have reference also recently published article not have enough citation yet therefore assess relationship use content based method instead citation analysis improve existing paragraph embed technique entropy based clustering method successfully form latticelike graph graph link by string aligned project subject be relevant each other confirm good face validity in+addition evaluate three approach find semantics relationship finally retrieve some relationship particular interest from graph resulting science graph grammar%23NPR be publicly accessible expect have feedback from people people be related grammar%23NPR project
back+end graph bibliographic information store as linked data project article connect by grammar%23NPR similarity value by grammar%23NPR common concept class
next step plan have comparison by any quantitative measure other content base citationbased method on concrete scenario in+the+future use paragraph vector several statistical application rather than drawing cosine similarity network result raise discoverability research era open+access
also plan perform additional comparison other project data+set such as grammar%23NPR grammar%23NPR funding project
concept vector share different language english japanese document compare through grammar%23NPR thesaurus english japanese notation furthermore visualize overlay heterogeneous data+set such as fund project conference paper be interest detect emerging research area
latent grammar%23ENT 993+1022
grammar%23NPH grammar%23NPR tool science science research practice

tutorial grammar%23NPR grammar%23ENT
grammar%23NPH map backbone science grammar%23NPR grammar%23ENT
synopsis linguistic theory grammar%23ENT grammar%23ENT studies linguistic analysis grammar%23ENT grammar%23ENT grammar%23ENT
find scientific topic
proceedings national+academy+of+sciences vol. suppl 1%29
grammar%23NPR herr ii grammar%23NPH grammar%23NPR burn g
national+institutes+of+health visual browser interactive visualization biomedical research proceedings grammar%23NPR grammar%23ENT
505+509
grammar%23NPH grammar%23NPH grammar%23NPH j-global knowledge be largest linked data science technology proceeding grammar%23NPR grammar%23ENT
on information sufficiency grammar%23NPR mathematical+statistics grammar%23ENT
distributed representation sentence document proceeding grammar%23NPR grammar%23ENT vol.

grammar%23NPH efficient estimation word representation vector+space proceeding workshop at grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH
distributed representation word phrase their grammar%23NPR proceeding nip
automatic classification springer nature proceeding smart topic miner proceeding grammar%23NPR grammar%23ENT 383+399
network scientific paper science grammar%23ENT 510+515
map grammar%23NPR exercise map network excellence internet science proceeding grammar%23NPR grammar%23ENT
grammar%23NPH chase hypernym vector+space entropy proceeding grammar%23NPR grammar%23ENT
mathematical+theory communication
grammar%23ENT 379+423 623+656
probabilistic topic model
grammar%23NPH grammar%23NPH grammar%23NPR herr ii grammar%23NPH grammar%23NPR burn
national+institutes+of+health grant use machine learned category graphical clustering nature method grammar%23ENT 443+444
word representation via gaussian grammar%23NPR proceeding grammar%23NPR grammar%23ENT detection contextual identity link knowledge+base grammar%23NPH grammar%23NPR grammar%23NPR joe.raad%40lri.fr grammar%23NPH grammar%23NPR grammar%23NPR nathalie.pernelle%40lri.fr grammar%23NPR fatiha.sais%40lri.fr abstract linked data application currently rely on use owl grammar%23NPR link ontology instance however several studies notice multiple misuse this identity link misuse misuse mainly cause by lack other well-defined link alternative lead erroneous statement inconsistency propose this paper new contextual identity link grammar%23NPR serve as replacement owl grammar%23NPR link identical instance specified context detect contextual link define algorithm name decide that test on scientific knowledge+base describe transformation processes
concept information system grammar%23NPR description language computing methodology knowledge+representation reasoning applied computing bioinformatics keyword context identity link discovery scientific data association+for+computing+machinery reference format grammar%23NPH grammar%23NPH detection contextual identity link knowledge+base association+for+computing+machinery new+york new+york united+states+of+america page
introduction over recent year scientist increasingly start use ontology in+order+to formalize knowledge about their data
formalization allow scientist browse data collect process by other scientist purpose facilitate data+integration task scientific+knowledge discovery identity link that declare between class instance be importance since use fusion data describe different data+source predict missing value evaluate reliability scientific result base on frequency represent identity link people increasingly rely on use owl grammar%23NPR
relationship define grammar%23NPH et+al. have very strict semantics owl grammar%23NPR statement indicate that two uri reference actually refer same thing i.e. statement grammar%23NPR permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148032 owl grammar%23NPR indicate that every property assert grammar%23NPR infer grammar%23NPR vice+versa many situation owl grammar%23NPR use link two similar but distinct individual grammar%23NPR et+al. study implication such erroneous use owl grammar%23NPR link author grammar%23NPR dataset present grammar%23NPR conduct study choose name common forename surname from 491+796 author available grammar%23ENT grammar%23NPR dataset this study show that 92%25 chosen name have incorrect publication affiliate cause by erroneous inference dataset describe scientific experiment data collect by different scientist experiment circumstances participant e.g. product material etc. tend change even slightly from one experiment another therefore individual rarely declare be furthermore this type genuine identity not always require as notion identity change depend on context for+instance some application fact two drug share same name be sufficient consider as equivalent while other application be also necessary that drug share same chemical structure likewise two lemonade different quantity but equal proportion lemon water sugar consider gustatory context different context energetic nutritional study however be not easy scientific expert enumerate all context interest be relevant given task our discussion grammar%23NPR expert show that be easy declare constraint constraint respect by semantic context in+order+to consider relevant for+instance expert declare that if quantity sugar consider corresponding measure unit also consider then when identity link detect all context that respect expert constraint be possible focus on different link depend on considered task
this paper propose new contextual identity link name grammar%23NPR this link express identity between two class instance be valid context define domain ontology define algorithm link grammar%23NPR detect most specific global context couple instance be identical this algorithm also guide by set semantic constraint provide by expert
test our approach on scientific data issue from two different project relate stabilization micro-organism transformation dairy gel
rest paper be structured as+follows next section present related work section present our objective preliminary section present algorithm grammar%23NPR detect contextual identity link knowledge+base
national institute agricultural grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH section before concluding present experiment conduct on two scientific dataset test our approach
grammar%23NPR data linking linked open data cloud initiative there be great interest development rwanda+defense+force data linking approach see survey existing data linking approach class into different category firstly supervised unsupervised approach depend on whether approach use set labelled data learn parameter e.g. weight property similarity threshold and%2For function e.g. aggregation function elementary similarity measure secondly local global approach depend on whether not approach explore property type owl while measure similarity finally informed uninformed approach depend on whether approach consider expert knowledge declare as ontology axiom e.g. key functionality constraint on property as data linking rule
this classification approach propose be unsupervised global inform however our aim be not detect owl grammar%23NPR link but discover identity link link be only valid specific explicit context
link assessment identity link generate by automatic approach mainly represent by owl grammar%23NPR constructor
relationship have strict semantics require in+particular identity all property related individual i.e.
grammar%23NPR grammar%23NPR grammar%23NPR p grammar%23NPR v p grammar%23NPR v several approach focus on detect existing erroneous owl grammar%23NPR statement such as some approach base on structural properties large graph identity link other approach be constraint base logical base
approach aim invalidate owl grammar%23NPR link while our proposed approach aim qualify specific context where two object consider as identical
identity similarity representation some approach focus on representation weak identity link
et+al. propose introduce eight new relations such as so similar so grammar%23NPR
prefix word claim express subjective identity similarity+relation their veracity depend on contextual interpretation user newly introduce relations organize hierarchy where existing identity property such as rdf grammar%23NPR owl grammar%23NPR grammar%23NPR predicate also describe
this hierarchy each predicate characterize by reflexivity transitivity symmetry property in+addition this extend domain specific relation
be difficult reliably deploy distinction open-ended domain this representation not allow explicit context identity link be valid therefore author propose use named graph represent context identity link link be valid context
et+al. author define new predicate genuine identity lvont grammar%23NPR aim be distinguish correct identity link from existing possibly erroneous owl grammar%23NPR statement whenever lvont grammar%23NPR use user know that this link intend strict sense identity additionally this ontology provide two near identity predicate lvont grammar%23NPR lvont grammar%23NPR intentionally leave vague e.g. relation grammar%23NPR define as property be at+least somewhat same+as else city los+angeles be somewhat same+as
identity link discovery grammar%23NPR et+al. propose approach approach allow represent possible context identity link be valid context represent by subset property two individual have same value all possible subset property organise lattice use set inclusion+relation however proposed representation not rely on ontology class not allow selection property depend on considered ontology class
represent set instance describe rdf their corresponding shared description extension framework recently introduce handle graph description grammar%23NPR et+al. iterative process infer new attribute propositionalized relation between individual from relation that explore at several level depth rdf graph formal concept intent make original attribute dl role restriction existential universal restriction exploit concepts concepts compute at previous step haspublished where belong concept lattice grammar%23NPR et+al. intent formal concept project graph pattern however approach not consider ontology class class pre-exist guide construction shared intent describe formal concept
contextual identity this paper present new approach discover contextual identity relationship rdf knowledge+base approach aim at detect identity link link be valid context define as sub- ontology domain ontology this section introduce basic notion definition definition need define contextual identity link first present considered data+model problem statement then define notion global context contextual identity relationship grammar%23NPR
knowledge+base consider knowledge+base where ontology represent grammar%23NPR data represent grammar%23NPR knowledge+base b define by couple o f where ontology o c dp define by set class c set owl set owl set axiom property domain range subsumption
f be collection triple subject property object property express that some relationship indicate by property hold between subject object triple between two resource between resource literal value
www.w3.org%2Fowl grammar%23NPR www.w3.org%2Frdf grammar%23NPR not consider blank node this grammar%23NPR research process typically technology first appear particular research community then spread contribute variety other research area for+example semantic+web technology e.g. rdf owl first create by research community field artificial+intelligence knowledge+base system formal ontology subsequently contribute variety other research area e.g. information+retrieval human computer interaction biology many be however easy miss interesting piece knowledge from different field therefore transfer technology from one research area e.g. semantic+web different possibly conceptually distant other area e.g. digital humanities take several year potentially slow+down research process thus need develop new method foster this process possibly predict spreading technology research field unfortunately standard technology diffusion forecasting model not tackle this issue since design assess general potential technology associate good number document conversely want predict that technology technology have no very few initial publication topic t adopt by researcher work on grammar%23NPH that technology technology exhibit similar spreading pattern multiple research topic tend adopt by similar topic follow this intuition introduce novel approach approach use semantically enhanced technology topic model predict technology technology adopt by research field characterise technology term set topic draw from large-scale ontology research area over given time+period apply machine+learning on data forecast technology spreading our goal be suggest promise technology scholar field thus help accelerate knowledge flow pace technology propagation main contribution this paper be definition implementation novel approach characterise forecast technology propagation dataset associate technology research topic throughout time use perform further analysis technology field semantic+web artificial+intelligence evaluation on 1%2C118 technology grammar%23ENT grammar%23ENT period show that our methodology forecast technology spread high precision remainder this paper organise as+follows section discuss detail input knowledge+base section discuss state+of+the+art current method forecasting technology section evaluate our approach by compare six machine+learning algorithm conclude section by outline future direction research technology topic figure show architecture take as input dataset research paper list technology research topic ontology then characterise technology accord their propagation through research topic use this representation forecast future propagation novel technology of+course technology propagate topic as result events not anticipate by knowledge previous spreading pattern indeed adoption technology new research topic foster by creation multidisciplinary workshop by scientific collaboration by inclusion technology commercial application by intuition researcher by many other events goal be therefore focus on technology propagation event event follow some extent previously observe pattern forecast high enough precision reliably suggest new technology researcher naturally adoption technology not hinge only upon awareness existence but depend on variety technological social political factor analysis be out scope this paper figure architecture input knowledge take as input three knowledge+base dataset research paper describe by+means+of their title abstract keyword list input technology associate relevant publication research paper dataset ontology research area scribing topic their relationship following discuss specific knowledge+base adopt study 1%2C118 technology semantic+web artificial+intelligence present this paper also suggest some alternative data+source data+source use implement dataset use dump grammar%23NPR database period contain about 16%2C000%2C000 paper mainly field computer+science grammar%23NPR be very large high+quality database peer review literature each paper describe by title abstract set keyword similar available dataset dataset contain title abstract scholarly publication be grammar%23NPR grammar%23NPR grammar%23NPR https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fproject%2Fmicrosoft-academic-graph3 technology list general be possible generate set technology by extract from research paper by+means+of automatic method ii obtain from manually curated software repository e.g. portal iii get from general knowledge+base e.g. grammar%23NPR since focus this study be on analysis technology not on their identification create manually curated set technology field semantic+web artificial+intelligence first select initial set about 2%2C000 technology by run grammar%23NPR on set 3%2C000 paper semantic+web in+order+to find technology technology originate adopt by this field then manually clean enrich resulting dataset by discard incorrect result also add other technology source from page list artificial+intelligence machine+learning algorithm method specifically focus on three category technology algorithm approach e.g. support vector machine particle swarm optimisation latent semantic analysis format e.g. owl system modeling+language tool application e.g. grammar%23NPR taverna grammar%23NPR then use grammar%23NPR instance map each technology all paper grammar%23NPR dump contain technology name title abstract as do previous technology forecasting study finally select set 1%2C118 technology appear more than publication topic ontology as reference topic ontology use create represent topic grammar%23NPR system currently be trialled by springer nature classify proceeding field computer+science such as well-known grammar%23NPR series cso create by apply grammar%23NPR algorithm 16%2C000%2C000 publication our grammar%23NPR derived dataset grammar%23NPR algorithm combine semantic technology machine+learning knowledge from external source e.g. grammar%23NPR call paper web+page automatically generate fully populated ontology research area use grammar%23NPR data grammar%23NPR be extension in+turn build on top grammar%23NPR include three semantic relationship grammar%23NPR indicate that two topic treat as equivalent purpose explore research data e.g. ontology matching ontology mapping skos grammar%23NPR indicate that topic be sub another one e.g. linked data semantic+web grammar%23NPR indicate that research output topic contribute another e.g. ontology engineering semantic+web generation technology topic matrices https%3A%2F%2Fcore.ac.uk https%3A%2F%2Fwww.openaire.eu citeseerx.ist.psu.edu technologies.kmi.open.ac.uk%2Frexplore%2Fontologies%2Fbiboextension.owl purl.org%2Fontology%2Fbibo aim this phase be build each year matrix matrix characterise technology terms their number publication different research topic this end first map each paper associate at+least one technology set topic classic way do so be adopt keyword as proxy research topic apply probabilistic topic model however as extensively discussed previous work solution ignore rich network semantic relationship between research topic be often unable distinguish research area from other terms use annotate publication therefore exploit topic ontology by associate paper all concept label found either title abstract keyword set as+well+as ii all skos grammar%23NPR iii all grammar%23NPR area initial set topic extract from scholarly dataset for+example publication associate term grammar%23NPR tag also high level topic such as rdf linked data semantic+web computer+science finally each technology count number paper each topic each year result be sequence matrices one matrix each year row represent technology column represent topic cell contain number publication technology e.g. grammar%23NPR topic e.g. semantic+web yield given year figure technology topic matrices naturally this representation rest on assumption that research topic not change radically in+time recognise that be not necessarily true be possible research area shift as new paradigm idea emerge be however very common assumption adopt by most system explore research tend work well most case our implementation sequence matrices materialise as json file feed forecasting module module use predict technology emerge topic years grammar%23NPR when analyse spreading technology through research landscape be useful know most frequent pattern technology topic matrices code be available at rexplore.kmi.open.ac.uk%2Fttf%2Fdownloads%2Fcode_and_data.zip.4 do so implement script script extract frequent technology spread pattern from matrices since focus this paper be on forecasting only describe briefly define technology spread pattern set topic as ordered tuple n topic associate number technology traverse topic during time+interval say that topic be subsequent topic grammar%23NPR pattern certain technology only if technology technology traverse enter grammar%23NPR within n year time window n our implementation our method extract pattern iterate over years technology count number publication associate most common spreading pattern even if this technique simply find most common sequence technology spread pattern without any assumption on causality propagation link allow identify valuable interesting pattern for+example pattern link artificial+intelligence information+retrieval field medical imaging bioinformatics show variety technology adopt by field over time such as support vector machine neural+network finite+state+machine gesture+recognition application grammar%23NPR and+so+on some+other pattern highlight how variety technique first adopt image analysis e.g. grammar%23NPR grammar%23NPR machine conditional random neural+network non- negative matrix factorisation then use also speech+recognition vice+versa full analysis pattern be not within scope this paper present future work technology propagation forecasting forecasting technology propagation treat as separate classification problem each topic interest pseudocode highlight step process topic sequence technology topic matrices process extract example feed machine+learning model each example consist pair represent respectively light+blue pink figure where vector characterise term topic technology one more years be indicating whether technology example spread topic interest window future years equal input parameter set by+default each m classification problem example organise into training set test set include all example certain year define by parameter evaluate precision recall f1 score classifier in+order+to produce suitable example classification problem sequence technology topic matrices short scan by year within input range by technology each element locate precisely one+and+only one topic distribution about technology year within sequence technology topic matrices each compute cumulative topic distribution british+columbia grammar%23NPH where rh be first year period analysis i.e. grammar%23ENT be number component pertain topic distribution i.e. number topic consider problem statement by+default be in+principle different from number classification problem give topic example create from if only if number publication that topic j be less+than h h threshold accounting presence absence technology topic set empirically this work this condition ensure that machine+learning algorithm focus only on case technology not yet propagate topic feature vector obtain by concatenate year previous years according+to variable set by+default grammar%23NPR loyalist+volunteer+force whenever past year miss beyond dataset boundary zero padding strategy adopt by use grammar%23NPR grammar%23NPR rh relate example build around compute by look+at future value technology topic figure construction example topic within v grammar%23NPR grammar%23NPR if at+least j be equal great than h h then label associate example be positive otherwise be negative namely h h j%3A%5Cpqqraqb grammar%23NPR v where j be component i.e. cumulative number publication about technology associate topic up year h h function yield when argument be greater than h h otherwise analogy window window define maximum number future years approach check whether technology propagate topic use random grammar%23NPH as default since yield good performance this task as discuss evaluation however current implementation allow choose between six different machine+learning classifier important point be that not apply indiscriminately all research area tend adopt specific technology thus be more predictable while be inherently more erratic this reason realistic setting be necessary evaluate method on historical data return only most reliable prediction involve subset topic yield good result pseudocode pseudocode forecasting step related work technology forecasting be established research area track all+the+way back 1930-1939 currently adopt variety modern data drive method traditionally focus on detect emerge vacant technology assess their potential most approach technology forecasting aim at support human expert comprehensive model represent technical social political information about technology recent year also witness emergence semi- automatic method identify assess promising technology since be fully automatic method focus on this second category approach detect assess technology by analyse scientific literature patent for+example grammar%23NPH et+al. use cluster patent document detect promising technology similarly jun et+al. forecast vacant technology by exploit k medoids cluster method base on support vector clustering applied on patent et+al. present approach approach integrate metric patent analysis into logistic growth curve model hydrogen energy fuel+cell technology aim identify optimal patent strategy fuel+cell industry grammar%23NPR grammar%23NPR use scientometric approach determine number publication patent emerge technology assess their trend several analysis on diffusion technology show that their growth approximate by s-shaped usually model predict use bass grammar%23NPR grammar%23NPR grammar%23NPR other statistical model however as discuss introduction method not apply on task address by this paper since focus on forecast growth technology rather than predict adoption by researcher work on specific research topic latter case technology not associate enough publication topic interest allow application statistical technique characterise research environment through scientific artefact epistemological concept e.g. claim hypothesis motivation background experiment other research concepts become increasingly important analyse research ensure reproducibility for+example portal be manually curated archive collect assign id a+number+of scientific object including application system prototype grammar%23NPR be initiative provide large network linked data life+science include information about biological compound drug gene taverna be workflow management system design represent execute scientific workflow in+addition linked data repository such as scholarly.org project foster open such as grammar%23NPR provide variety data about scientific paper author venue and+so+on another way express scientific+knowledge be by+means+of nano- publication be unit information contain scientific claim claim uniquely identify attribute their author similarly micro publication usually serialise represent scientific claim how support and%2For challenge another force software citation group also contribute this endeavour by produce set principle foster traceability software research initiative be complementary potentially provide input information exploit output better characterise flow knowledge research characterise technology distribution research topic from ontology represent entity such as author venue community citation accord their associated https%3A%2F%2Fwww.force11.org%2Fgroup%2Fsoftware-citation-working-group function grammar%23NPR ttm lookback lookforward topic topic topic feature label year do technology ttm year do examplefeatures ctd ttm year lookback technology ctd ttm year technology examplelabel threshold ctd ttm year technology
ctd ttm year lookforward technology if example be valid then examplefeatures examplelabel end end end xtrain xtest ytrain ytest split feature label if enough example then train xtrain ytrain prediction predict xtest evaluate prediction ytest end end return prediction end topic prove be very useful generate analytics on research for+example grammar%23NPR grammar%23NPR model use treat author as probability+distribution over topic conference journal grammar%23NPH et+al. propose topic oriented community detection approach combine both topic cluster link analysis similarly grammar%23NPH grammar%23NPH identify topic oriented community by exploit topic similarity matrix assign research topic document author differently from approach characterise topic through formal ontology since this solution allow generate structured set unambiguous research topic link by semantic relationship evaluation evaluate on 1%2C118 technology topic field computer+science during grammar%23ENT grammar%23ENT grammar%23NPR evaluation have two main purpose first confirm initial hypothesis i.e. that be possible forecast technology propagation at+least certain subset technology topic by learning how technology spread in+the+past secondly compare performance several machine+learning algorithm on this task experimental setup select as training set example grammar%23ENT grammar%23ENT period as test set example grammar%23ENT grammar%23ENT period choose interval as allow label example test set use window five years grammar%23ENT grammar%23ENT set h h threshold years since want focus on predict relatively new technology consider only example about technology exist no more than year technology have two fewer publication topic simulate realistic situation by assume grammar%23ENT as current year not use any information successive that year label example training set select topic associate at+least positive example both training test set period analysis train classifier each topic classifier train on+average on 5%2C136 example total 888%2C633 example evaluate on example total 117%2C516 example as discuss previously be not possible apply this task standard model forecast potential technology thus test six machine+learning algorithm on technology topic model model characterise propagation pattern logistic regression random grammar%23NPH decision+tree support vector machine neural+network gradient tuning hyper parameter use each single model perform by twofold cross validation over training set as support by scikit learn grammar%23NPR evaluation data be available at rexplore.kmi.open.ac.uk%2Fttf scikit-learn.org measure performance classifier by computing precision recall f1 score overall performance over a+number+of topic determine by compute micro average metrics significance result on categorical variable precision recall assess by use chisquare test table cross categorised frequency data rxc grammar%23NPR correction grammar%23NPR table when more appropriate grammar%23NPR test correlated proportion recall existence statistical difference among n f1 not independent distribution explore parametric+test grammar%23NPR be n grammar%23NPR be n similarity behaviour two f1 distribution investigate use within subject grammar%23NPH compute intra- class correlation+coefficient international+chamber+of+commerce measure item reliability grammar%23NPR coefficient also+known+as grammar%23NPR coefficient measure tendency correlated member two distribution have approximately same value close international+chamber+of+commerce grammar%23NPR high similarity result figure figure show respectively precision recall obtain by six algorithm on first n topic order by number positive label test set classifier perform better on topic associate high number technology propagation event so performance decrease number topic figure average precision six machine+learning approach on first n topic first part analysis consider ensemble six algorithm arrange two grammar%23NPR contingency table grammar%23ENT freedom value precision recall chi-square test evidence highly significant difference among approach both precision recall p then zoom analysis on three top performer random grammar%23NPH decision+tree gradient random grammar%23NPH yield good result terms precision first topic precision be over 74.4%25 significantly high p than value 69.4%25 obtain decision+tree 67.2%25 gradient also consider first topic random grammar%23NPH score good 70.2%25 versus 62.9%25 decision+tree 64.4%25 gradient p conversely grammar%23NPR grammar%23NPR containment benchmark generation grammar%23NPR grammar%23ENT grammar%23ENT united+states+of+america si m r it y e r r o r super query feasible grammar%23NPR grammar%23NPR grammar%23NPR random feasible exemplar grammar%23NPR benchmark similarity error si m r it y e r r o r super query feasible grammar%23NPR grammar%23NPR grammar%23NPR random feasible exemplar b grammar%23NPR benchmark similarity error d iv er si ty s c o r e super query feasible grammar%23NPR grammar%23NPR grammar%23NPR random feasible exemplar c grammar%23NPR benchmark diversity score d iv er si ty s c o r e super query feasible grammar%23NPR grammar%23NPR grammar%23NPR random feasible exemplar d grammar%23NPR benchmark diversity score figure comparison benchmark generate by our framework term similarity error diversity score table comparison number containment test t benchmark generation time g second benchmark size grammar%23NPR grammar%23NPR km grammar%23NPR db grammar%23NPR grammar%23NPR feasible ex exemplar q number super- query benchmark size db km random grammar%23NPR ex grammar%23NPR km db km random grammar%23NPR ex q t g t g t g t g t g q t g t g t g t g 7+906 5+408 6+179 7+400 7+496 9+126 2+105 1+102 1+117 1+123 1+100 12+163 2+140 1+111 1+151 1+160 1+125 15+197 2+146 1+146 1+180 1+195 as show figure feasible exemplar generate most diverse benchmark figure compare diversity grammar%23NPR exemplar selected feature be analysis base on super query benchmark be be because grammar%23NPR contain total unique super query generate super query benchmark use feasible exemplar as overall diversity score i.e. last bar figure our approach generate more diverse benchmark vs. diversity score compare individual feature be more diverse than feature only number projection variable number bgp standard+deviation be high than our approach
containment solver evaluation result figure show comparison selected query containment solver benchmark super- query generate from grammar%23NPR by use grammar%23NPR cluster algorithm overall grammar%23NPR clearly outperform other solver as be only isomorphism based one handle all test+case e grammar%23NPR grammar%23NPR be about twice grammar%23NPR suggest quickly check query containment test+case
since t grammar%23NPR grammar%23NPR only handle at most test case grammar%23NPR rate rather suggest how fast grammar%23NPR grammar%23ENT grammar%23ENT et+al.
n o rm ed s d
grammar%23NPR exemplar grammar%23NPR figure diversity analysis selected feature number projection variable tp number triple pattern grammar%23NPR mean join vertex degree solver rule+out support containment check instead indicate actual performance on containment check
comparison qc bench our result be rather surprising solver i.e. grammar%23NPR algebra grammar%23NPR be able handle majority qc bench test+case be not able handle more+than generated test case consequently mean that grammar%23NPR query containment containment occur in+reality query log be from query containment present qc bench in+practice isomorphic containment turn+out be much+more scarce
q m p h algebra figure ery mix hour grammar%23NPR
conclusion this paper present grammar%23NPR containment benchmark generation framework our framework allow user generate benchmark suit particular use case case be important test query containment solver pertain given application allow generate benchmark use approach compare quality benchmark generate use approach solver the+halo+trust grammar%23NPR algebra table containment solver comparison total test+case the+halo+trust number handled test+case number correct test+case number time+out test case
term their composite error diversity score turn+out that grammar%23NPR grammar%23NPR generate most representative grammar%23NPR exemplar generate most diverse benchmark compare other method use our framework also compare existing grammar%23NPR query containment solver use benchmark generate by our framework turn+out that grammar%23NPR accuracy be than other selected solver in+addition grammar%23NPR be able check containment test reasonable amount+of+time future work generate benchmark from other lsq dataset also consider add cluster method into framework further milestone be identify other query feature e.g. result size query consider while generate query containment benchmark
grammar%23NPR ery github.com%2Faksw jena-sparql-api tree develop jena-sparql-api-query-containment grammar%23ENT online access grammar%23ENT

grammar%23NPR query containment rdf entailment regime on automated reasoning springer 134+148

evaluate benchmarking grammar%23NPR query containment solver springer 408+423
containment expressive grammar%23NPR navigational query
86+101
grammar%23NPH grammar%23NPH conjunctive query containment revisit eoretical computer+science grammar%23ENT 211+229
grammar%23NPH grammar%23NPR static analysis xml path type notice grammar%23ENT 342+351
grammar%23NPR v grammar%23NPR grammar%23NPH
property path
grammar%23NPH grammar%23NPH grammar%23NPH static analysis optimization semantic+web query tod grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH ery containment data+integration system system sci. grammar%23ENT
grammar%23NPH grammar%23NPH containment equivalence well designed grammar%23NPR proceedings 33rd association+for+computing+machinery sigmod-sigact-sigart symposium on principle database system association+for+computing+machinery
grammar%23NPH grammar%23NPH grammar%23NPR e linked grammar%23NPR eries dataset
grammar%23NPH
feature based grammar%23NPR benchmark generation framework

grammar%23NPH hibiscus grammar%23NPR grammar%23NPR
grammar%23NPH decision procedure alternation free twoway modal calculus international conference on automated reasoning analytic tableau related method springer 277+291
grammar%23NPH
query containment grammar%23NPR axiom proceedings 26th on grammar%23NPR intelligence ontology grammar%23NPR annotation music score grammar%23NPH grammar%23NPH grammar%23NPR samira.cherfi%40cnam.fr grammar%23NPH christophe.guillotel%40cnrs.fr grammar%23NPR faycal.hamdi%40cnam.fr grammar%23NPH grammar%23NPR philippe.rigaux%40cnam.fr grammar%23NPH grammar%23NPR nicolas.travers%40cnam.fr abstract digital music score be way present music notation lack semantic information useful musicology purpose in+order+to manipulate music concept propose general approach extend score encoding semantic annotation rely on ontology music notation design integrate semantic music element either extract produce by knowledge process illustrate whole mechanism by extract rdf fact base on identification dissonance renaissance counterpoint
concept information+system owl resource+description+framework rdf computing methodology ontology engineering keyword music notation ontology knowledge extraction annotation grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH music score grammar%23NPR grammar%23ENT grammar%23NPR grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery new+york new+york united+states+of+america https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148038 introduction many digital music score now produce grammar%23NPR format such as grammar%23NPR permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148038 grammar%23NPR format share common approach encode complete content sheet score including render instruction while initial motivation be mostly address interoperability concern each note describe attribute pitch octave alteration duration exciting perspective allow emergence new application automatic transformation e.g. transposition audio synchronization multimodal interaction of+course computer analysis music
musical+score encode semantic category music be subject analysis identify linguistic structures phrase cadence dissonance highlevel concepts be implicit score encode thus need extract by analytic process link score fragment relate this completion notation help figure+out score organization specificity musical language
first immediate application be visualization mechanism for+instance want see occurrence given harmonic sequence propose score representation relevant fragment large digital+library music score this knowledge also leverage meaningful search such as for+instance find piece feature particular cadence formula constitute fruitful framework digital musicology enhancement
present paper propose framework manage knowledge extraction process from music score aim at supply generic approach represent extract link search music language concept concept infer from music notation illustrate analysis dissonance late renaissance counterpoint more specifically propose following contribution ontology music notation content focus on core semantic information present score subject analytic process modeling high-level analytic concepts such way that concept instance associate as annotation fragment score section review state+of+the+art present notation ontology section section detail knowledge+management process section conclude grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
state+of+the+art digital musicology software+system implication begin humdrum toolkit by grammar%23NPH
system grammar%23NPR extend as independant module combine analytical task draw on various heuristic approach while rather target fully automated algorithm structured score encoding
strict counterpoint be first musical idiom investigate by+means+of computer software grammar%23NPR program design pedagogical purpose perform detailed analysis two-part species counterpoint first species counterpoint module implement grammar%23NPR cint extension module grammar%23NPR drawing on grammar%23NPR distinction five main dissonance category strict counterpoint grammar%23NPR propose fully implementable model allow unambiguous identification dissonance type
paper take this model dissonance schema develop by grammar%23NPR et+al. as point+of+departure
knowledge extraction annotation ontology characterize by their ability describe concepts at fine-grained level structured manner
be as subfield ontology play central role general idea be use retrieve instance value relate class property ontology
annotation be process add information digital content according+to several factor take+into+account when choose annotation strategy
developing knowledge be heavy investment semantic+web technology thus recommend publishing share syndicate distribute web annotation suggest formally identify concept relation document help information+retrieval inference integration source
annotation be generally usage orient annotate music score harmony merge ontology performance analysis like grammar%23NPH second solution prefer evolution as annotation preserve consistency exhibit modularity our approach core notation ontology ontology capture semantic information from notation analytic ontology ontology cover specific concepts particular application
ontology propose ontology on musical+work but very high level concepts without notation
music ontology describe more content orient usage basic information about musical+work artist integrate whole score dedicate many music oriented web+service grammar%23NPR work propose music vocabulary vocabulary describe classical+music performance ontology describe music at work level be suitable provide general information such as music category e.g. chamber+music performance e.g. musician more content orient ontology be suitable
believe that such model be invaluable help let community formalize discussion proposal address issue relate topic hope that present work although focus on scope application serve as encouragement initiate such endeavour
grammar%23NPR ontology our core notation ontology call grammar%23NPR aim at capture part music notation be necessary sufficient support music analysis process therefore abstract semantic music content content explicitly found score encode without graphical layout score
structural aspect grammar%23NPR ontology rely on structural aspect score essentially score model as hierarchical+structure where leave consist voice inner node part voice be sequence events occurrence abstract+class event be refined several sub- class
score make part class part represent abstract concept concept be refined group part consist set subpart mostly serve organizational aspect score e.g. two group wind+instruments string+instrument concerto ii single part encapsulate music notation element assign individual performer instrument vocal fig. show for+instance part piano violin cello etc. single part contain one several voice
voice events voice be essentially time+series event where event denote production sound artifact at specific timestamp onset thus voice model as function from time+domain where be exactly one event at each instant in+other+words events not overlap
show voice time+series event overlapping constraint imply that duration event cover time range between onset include onset next event series exclude
event concept be refined sound grammar%23NPR text syllable sing grammar%23NPR actually any piece information attach time+period grammar%23NPR decompose note decomposable chord compose at+least two note same duration
measure represent at part level level mean that voice from part share same temporal range frommeasures property measure be time+signature value vary from one measure another
event link measure from onset fig.
cedric.cnam.fr%2Fisid%2Fontologies%2Fmusicnote.owlontology-based annotation music score grammar%23NPR grammar%23ENT top-level score piano soloist orchestra string wind violin cello oboe flute voice figure structure score voice grammar%23NPR time rational grammar%23NPR grammar%23NPR tn events part grammar%23NPR measure grammar%23NPR grammar%23NPR en figure voice as time+series event conceptual space storage+space analytic process fragment annotation map analytic fact notation fact figure annotation mechanism modeling analytic concepts grammar%23NPR now that aim at develop analytic model some musical language expert analyst express this model as concept need be integrated our core ontology as new module call analytic ontology
better understand this modular aspect consider generic analytic process illustrate by start from score encode such as ontological space populate notation fact thanks+to mapping extract semantic part score be instance core notation ontology cover necessary information carry+out analytic procedure process produce analytic fact instance concept take from analytic ontology qualify fragment notation e.g. as dissonance any other semantic artifact
that formalization analysis act as proof completeness core ontology happen that concept qualify some aspect music notation not capture by ontological representation moreover algorithm need information information omit illustrate idea concrete example dissonance modeling grammar%23NPR counterpoint
dissonance grammar%23NPR counterpoint our dissonance typology derive grammar%23NPR grammar%23NPR distinction five main dissonance type grammar%23NPR counterpoint suspension anticipation passing+note neighbor+note escape note
resume model pattern two voice counterpoint take+into+account harmonic melodic metric criteria at three distinct moment contrapuntal stream event immediately precede dissonance dissonance impact event immediately follow dissonance dissonant note highlight red example
suspension imply that voice remains place at impact while voice move subsequently then conversely anticipation constitute exact mirror this pattern weak beat constant melodic movement at voice while voice remains place imply different dissonance passing+note unidirectional parsimonious neighbor+note bidirectional parsimonious escape note unidirectional parsimonious
counterpoint module dissonance analysis express term our ontology concept as+follows dissonance pattern be relationship between two events grammar%23NPR from voice grammar%23NPR from voice by consider each pair event grammar%23NPR grammar%23NPR such that their time range overlap
grammar%23NPR ontology give events voice concept support rule requirements however event concept need extend metric weight property consonant property interval concept
create counterpoint module module contain concept this analytic model import on+demand consist dissonance concept identify dissonance pattern occurrence as relationship between two events distinct voice dissonance subtype this analytic ontology be integrated our notation ontology thanks+to standard mechanism
knowledge extraction score annotation implementation analytic process rely on knowledge based approach rule based language as example benefit express following our dissonance model as rule standard language semantic+web grammar%23NPR rdf fact extract by grammar%23NPR process compute every concept relation event position from file
grammar%23NPR concept be specified as+follows if consider list event pair ei ej take from two distinct voice grammar%23NPR grammar%23NPR such that their time range overlap then grammar%23NPR grammar%23NPR be instance canonical dissonance pattern if interval grammar%23NPR grammar%23NPR be dissonant pair em en ep eq respectively precede follow grammar%23NPR grammar%23NPR be both consonant intuitively following rule express three interval pair dissonance successor predecessor recall that con property introduce our ontology qualify interval
cedric.cnam.fr%2Fvertigo%2Fmei2musicnote.xsldetection contextual identity link knowledge+base grammar%23NPR grammar%23ENT united+states+of+america problem statement problem detect contextual identity link define as+follows give knowledge+base b o f set grammar%23NPR instance target class ontology o find set all instance pair grammar%23NPR grammar%23NPR most specific global context grammar%23NPR grammar%23NPR be identical
global context be sub- ontology o represent vocabulary on two instance consider as identical for+instance example depict figure two instance target class grammar%23NPR see as identical when all ontology properties class consider with+the+exception+of property name drug similarly two instance consider as identical two distinct context first context consider all product compose drug every product consider weight however this context description weight reduce measure unit not consider quantity property grammar%23NPR second context instance be identical be context where take+into+account weight paracetamol describe by value measure unit but only consider presence lactose drug without consider weight some context be more relevant than e.g. value weight without measure unit not have sense
also aim take+into+account some expert knowledge knowledge represent as set constraint on class and%2For properties properties should not involve considered context
context global context represent as connected sub- ontology ontology o compose set class property o set axiom be limited constraint on property domain range set class involve global context be subset class denote by grammar%23NPR that instantiate b see definition moreover automatically choose abstraction+level class involve global context by select from class direct type most general
follow first introduce set class grammar%23NPR involve context then formally define global context contextual identity relation name grammar%23NPR express that two instance be identical given global context
selected class grammar%23NPR set selected class grammar%23NPR involve contextual identity link be subset class ci b such grammar%23NPR ci c c j c s t. x grammar%23NPR x c j ci c j example figure grammar%23NPR contain all class graph except product product not instantiate therefore grammar%23NPR grammar%23NPR uniquely consider as type paracetamol lactose respectively
global context global context be subontology grammar%23NPR cu grammar%23NPR grammar%23NPR au o such that cu grammar%23NPR grammar%23NPR dp grammar%23NPR op au be set domain range constraint be more specific than describe grammar%23NPR op grammar%23NPR domainu op grammar%23NPR op op op dp grammar%23NPR domainu dp grammar%23NPR dp
figure there exist many possible global context present one grammar%23NPR c grammar%23NPR paracetamol lactose grammar%23NPR op grammar%23NPR dp domain grammar%23NPR grammar%23NPR grammar%23NPR lactose paracetamol domain lactose paracetamol grammar%23NPR definition order relation between global context let grammar%23NPR cu grammar%23NPR grammar%23NPR au grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR av be two global context context grammar%23NPR be more specific than grammar%23NPR note grammar%23NPR grammar%23NPR if grammar%23NPR cu grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR op grammar%23NPR domainv op domainu op op op dp grammar%23NPR domainv dp domainu dp dp dp
filter+out irrelevant context consider take consideration expert knowledge when be available expert supply three type constraint refer properties that expert want discard detection contextual identity link
constraint use when property value correspond unstructured free text know be particularly heterogeneous when property subject object be insignificant compare two instance given task such case expert declare that property p be unwanted given domain ci particular range c j by add+up constraint up ci p p c j grammar%23NPR when property be unwanted all domain range constraint p use such case p op dp
necessary property be constraint noted np ci p p c j when such constraint add np only consider global context where property p op p dp such+that ci domain p resp.
j p
co-occurrence constraint cp ci grammar%23NPR ci pn declare guarantee that certain class ci either declare as domain range all properties indicate constraint
declare that weight value have no meaning without measure unit expert add constraint grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR
contextual identity link our approach two instance consider as identical given global context when all properties contain global context instantiate when their instance value be equal
firstly define contextual description description consider one instance one context then define condition condition hold consider that two instance description refer same entity
contextual instance description accord global context give set rdf triple f global grammar%23NPR grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH extract ontology o four instance target class grammar%23NPR
cu grammar%23NPR grammar%23NPR au instance contextual description gi grammar%23NPR be maximal set triple describe f such gi form connected graph graph contain at+least one triple where be subject object t s p o gi then p grammar%23NPR grammar%23NPR type s domainu p type o p j class instance gi dp grammar%23NPR such as type j domain dp then ta j p v gi v type literal j class instance gi op grammar%23NPR such as type j domain op grammar%23NPR grammar%23NPR op then ta j op k tb j op l gi type k grammar%23NPR type l grammar%23NPR from two contextual description two class instance define given context define if consider as identical this work consider that properties be local complete if property p instantiate given class instance consider that all property instance know i. since local completness assume two instance consider as identical when contextual graph form by contextual description be isomorphic up re naming instance grammar%23NPH
note since some class remove from global context this constraint can in+fact consider class by class
identity global context give global context grammar%23NPR pair instance grammar%23NPR grammar%23NPR be identical grammar%23NPR note grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR only if two labelled graph grammar%23NPR grammar%23NPR represent contextual description grammar%23NPR grammar%23NPR respectively be isomorphic up rewriting uri class instance literal be equal
consider as identical according global context grammar%23NPR define example
i.e. grammar%23NPR grammar%23NPR
contextual identity relation only be specified most specific global context s but infer more general use order relation between global context give grammar%23NPR grammar%23NPR two global context grammar%23NPR grammar%23NPR then grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR
grammar%23NPR detect contextual identity before present algorithm sub- section introduce terminology terminology use throughout algorithm
preliminary definition local context local context class c be context be limited datatype object property that define c. algorithm note grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR local context where p grammar%23NPR grammar%23NPR domain p c grammar%23NPR grammar%23NPR op inu dp inu ainu local context where dp inu op op inu op c
identity graph identity graph grammar%23NPR grammar%23NPR grammar%23NPR v e pair individual grammar%23NPR grammar%23NPR be connected labelled directed graph where v be set node e be set edge each node ni represent set pair local context grammar%23NPR c grammar%23NPR c c generalize most specific local context grammar%23NPR c grammar%23NPR c pair consider as identical
node grammar%23NPR represent set pair link node grammar%23NPR represent set pair by edge e grammar%23NPR grammar%23NPR label as p if grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR such grammar%23NPR p grammar%23NPR grammar%23NPR p grammar%23NPR f if p grammar%23NPR c grammar%23NPR p grammar%23NPR grammar%23NPR p grammar%23NPR f if p grammar%23NPR c
identity graph grammar%23NPR grammar%23NPR grammar%23NPR graph path pi be sequence distinct node grammar%23NPR grammar%23NPR nm root by grammar%23NPR describe grammar%23NPR grammar%23NPR respect following condition nk nl pi k l grammar%23NPR c grammar%23NPR c
present identity graph grammar%23NPR grammar%23NPR pair drug detection contextual identity link knowledge+base grammar%23NPR grammar%23ENT united+states+of+america figure two possible identity graph pair grammar%23NPR grammar%23NPR simplicity reason c op grammar%23NPR not represent this figure all local context
grammar%23NPR input target class k np grammar%23NPH expert constraint f set rdf triple considered knowledge+base output grammar%23NPR set most specific global context each pair instance grammar%23NPR grammar%23NPR f list instance type f each grammar%23NPR grammar%23NPR %C3%97 do grammar%23NPR grammar%23NPR construct grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR k f each grammar%23NPR grammar%23NPR do grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR n grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR n ig grammar%23NPR add grammar%23NPR each library+of+congress grammar%23NPR do grammar%23NPR grammar%23NPR add library+of+congress grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR n ig if grammar%23NPR grammar%23NPR such as grammar%23NPR grammar%23NPR then grammar%23NPR add grammar%23NPR if grammar%23NPR grammar%23NPR such as grammar%23NPR grammar%23NPR then grammar%23NPR r emove grammar%23NPR grammar%23NPR add grammar%23NPR grammar%23NPR grammar%23NPR return grammar%23NPR algorithm goal algorithm grammar%23NPR detection contextual identity be determine each pair instance grammar%23NPR grammar%23NPR %C3%97 target class give by user set most specific global context identity relation grammar%23NPR be true grammar%23NPR require have set fact f considered knowledge+base target class as input in+addition grammar%23NPR consider different constraint lists up np cp give by expert this paper restrict description this algorithm two main+function nonetheless more detailed description different use case be available
algorithm grammar%23NPR describe algorithm collect selected class definition in+order+to indicate level abstraction consider build identity graph generate most specific global context
generate grammar%23NPR input n identity graph node as axiom indicate type node source property source grammar%23NPR current global context grammar%23NPR set unused local context n list node grammar%23NPR identity graph output grammar%23NPR current most specific global context if n n then n add n grammar%23NPR c n grammar%23NPR c grammar%23NPR c if grammar%23NPR c null grammar%23NPR c grammar%23NPR c then grammar%23NPR add grammar%23NPR c if not exist grammar%23ENT grammar%23NPR n each e op n nd en do ad domain op c op type nd grammar%23NPR grammar%23NPR nd ad grammar%23NPR grammar%23NPR n grammar%23NPR else if grammar%23NPR c grammar%23NPR c then grammar%23ENT grammar%23NPR n each e op n nd en do ad domain op c op type nd if ad grammar%23NPR c then grammar%23NPR grammar%23NPR nd ad grammar%23NPR grammar%23NPR n grammar%23NPR else c as grammar%23NPR library+of+congress c grammar%23NPR c library+of+congress c r emove as grammar%23NPR r lace library+of+congress c replace existing lc c grammar%23NPR add grammar%23NPR c if not exist grammar%23NPR add intersect grammar%23NPR c grammar%23NPR c if not exist return grammar%23NPR then each pair individual target class construct identity graph s definition use depth first search+algorithm when different mapping between instance same class consider new identity graph identity construct
generate most specific global context s by rely on identity graph global grammar%23NPR construct use set local context insure presence no more than one local context class same global context grammar%23NPR grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH most specific global context generate use function grammar%23NPR traverse identity graph grammar%23NPR use also depth first search+algorithm this function describe algorithm aim add most specific outgoing local context grammar%23NPR c c already calculate grammar%23NPR current global context grammar%23NPR i.e. most specific global context be three case grammar%23NPR not contain local context grammar%23NPR c class c if grammar%23NPR contain grammar%23NPR c grammar%23NPR c equal local context grammar%23NPR c n then grammar%23NPR c add grammar%23NPR this function then recursively recall each node nd grammar%23NPR such as be edge from n nd
grammar%23NPR contain local context grammar%23NPR c class c grammar%23NPR c be more specific than grammar%23NPR c then this function recursively recall each destination node nd grammar%23NPR such as be edge from n nd label op have axiom grammar%23NPR c domain op c type nd op
grammar%23NPR contain local context grammar%23NPR c class c grammar%23NPR c be not more specific than grammar%23NPR c then this function not recall this graph node domain represent type node source range represent c object property op lead this graph element remove from grammar%23NPR c
both grammar%23NPR c most specific local context context generalize grammar%23NPR c grammar%23NPR n add list grammar%23NPR in+order+to guarantee presence local context other global context therefore result several most specific global context same pair
time complexity this algorithm be o n n number pair target class number instance f. grammar%23NPR implement use triple store be available at github.com%2Fraadjoe%2Fdecide_v2
apply on pair grammar%23NPR result two global context grammar%23NPR grammar%23NPR represent most specific context two drug be identical grammar%23NPR c grammar%23NPR paracetamol lactose grammar%23NPR op grammar%23NPR dp grammar%23NPR domain grammar%23NPR grammar%23NPR grammar%23NPR lactose paracetamol domain lactose paracetamol grammar%23NPR domain grammar%23NPR grammar%23NPR grammar%23NPR xsd str grammar%23NPR c grammar%23NPR paracetamol lactose grammar%23NPR op grammar%23NPR dp grammar%23NPR alue grammar%23NPR domain grammar%23NPR grammar%23NPR grammar%23NPR lactose paracetamol domain paracetamol grammar%23NPR domain grammar%23NPR alue grammar%23NPR grammar%23NPR alue xsd f loat domain grammar%23NPR grammar%23NPR grammar%23NPR xsd str experiment dataset description our approach evaluate on two scientific dataset exploit use grammar%23NPR ontology aim at model transformation process each process conduct over several itinerary each itinerary represent sequence transformation step dry heating etc. this ontology as most knowledge+base use model scientific experiment distinction make between actual experiment include steps their participant between observation conduct at end each step observation contain large+number missing information since not every measure e.g. temperature ph consistently observe each experiment step distinction between experiment observation see ontology core grammar%23NPR
first dataset test our approach describe process micro-organism stabilization conduct different itinerary context project
dataset contain 1+721+979 statement selected class 415+136 individual properties object properties
second dataset describe process dairy gel transformation conduct itinerary context project this dataset contain 237+838 statement selected class individual properties object properties
test algorithm grammar%23NPR separately on dataset in+order+to detect most specific global context individual target class mixture be identical
mixture similarly class grammar%23NPR figure compose set products transform during different step process grammar%23NPR execute on grammar%23ENT ram machine core grammar%23ENT process
discovered contextual identity link table present result grammar%23NPR apply on two scientific dataset without consider their observation i.e. property related observation declare as unwanted properties dataset instance target class mixture mixture form pair result contextual identity link valid global context total while pair mixture dataset result identity link valid different global context total on+average dataset each identity graph each pair mixture compose node respectively each pair be identical most specific global context respectively
global context represent as named graph original dataset name graph contain detected identity statement contextual identity statement between two instance grammar%23NPR grammar%23NPR indicate that this context represent most specific global context two instance be identical definition each contextual identity statement be grammar%23NPR core ontology be available at agroportal.lirmm.fr%2Fontologies%2Fpo2 grammar%23NPR github.com%2Fraadjoe%2Fdecide_v2%2Fblob%2Fmaster%2Fpo2_model.jpg contextual identity link knowledge+base grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america table result grammar%23NPR on dataset target grammar%23NPR individual target class 210+619 possible pair 21+945+191+271 dependant class total class graph node pair different global context 28+231 identity link 31+092+239+410 identity link pair execution+time approx. minute symmetric transitive reflexive detected context contain up class properties while less specific contain only one class one property
repeat experiment on each dataset while take+into+account constraint cp express that weight value not consider without unit measure vice+versa
number distinct most specific global context remain unchanged both dataset notice change around 40%25 generated most specific global context more precisely each global context contain properties without replace by another new global context where two properties not consider
use contextual identity link prediction goal this experimentation be test if contextual identity link exploit prediction task more precisely want find+out probability two experiment be identical certain context have similar observation therefore be able predict certain degree certainty some experiment unobserved measure table indicate that individual target class mixture connect dataset class out out class grammar%23NPR thus show that identity between two mixture also indicate identity between experiment step two mixture exist
grammar%23NPH principle genuine identity between two object e.g. experiment indicate that every property e.g. observed measure assert assert x y p x z p y z p op dp this prediction task aim detect each context grammar%23NPR set properties grammar%23NPR pn where grammar%23NPR grammar%23NPR x y p x grammar%23NPR p y grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR such rule write as r grammar%23NPR grammar%23NPR x y same m m represent certain measure e.g. ph measure since detected contextual identity link only state most specific context each pair exploit global context order relation definition obtain complete set contextual identity link each global context
evaluate quality rule r calculate rule average error+rate each pair x y identical grammar%23NPR calculate error+rate their m measure value if exist both x y. for+instance error+rate example detected rule dataset rule error+rate support grammar%23NPR grammar%23NPR x y same adhesiveness 2.2%25 23%25 grammar%23NPR grammar%23NPR x y same sweetness 4.5%25 13%25 grammar%23NPR grammar%23NPR x y same bitterness 7.1%25 29%25 grammar%23NPR grammar%23NPR x y same acidity 8.2%25 21%25 pair x y measure ph grammar%23NPR x y ph x ph y grammar%23NPR ph max ph min ph max ph min represent maximum minimum value take measure ph dataset from sum this measure error+rate pair obtain rule average error+rate
rule support represent number pair identical grammar%23NPR have measurem divide by total number pair grammar%23NPR
generate rule dataset average rule context rule dataset average rule context on+average rule average error+rate be 7.3%25 rule support be 0.4%25 while rule average error+rate be 20%25 rule support be 1%25 this low support both dataset show large+number observational measure miss each experiment
test all rule each global context deduct that on+average error+rate rule decrease by 22%25 when global context replace by more specific global context dataset decrease by 31.5%25 dataset this decrease show that rule discover more specific global context be more precise than one discover more general context that contextual identity link can for+example exploit predict missing property value different confidence+level ask domain expert evaluate plausibility good detected rule terms error+rate support combine on scale expert strongly agree on plausibility rule agree on rule strongly disagree on plausibility rule expert be not sure plausibility remaining rule various reason table present rule strongly agree as plausible dataset for+instance first rule indicate that be high probability that mixture same weight rennet sardine sodium+chloride contain i.e. not necessarily same weight lipid water protein have similar adhesiveness
discussion our collaboration domain expert experiment result conduct on scientific dataset show that use genuine identity link such as owl grammar%23NPR link rarely require scientific dataset since experiment environment tend change even slightly from one experiment another result propagation grammar%23NPR graph characterize recent scientific landscape use paragraph science represent structure science help understand science technology development
research scientometrics develop technique analyze research activity measure their relationship however navigate recent scientific landscape still challenge since conventional inter citation co- citation analysis have difficulty apply ongoing project recently published paper therefore in+order+to characterize attempt current scientific landscape this paper propose content based method locate research project multi dimensional space use word paragraph em bedding technique specifically address un clustered problem associate paragraph vector introduce cluster vector base on information entropy concepts thesaurus in+addition propose three approach find semantics project relationship experimental result show that proposed method successfully form clustered graph from 25%2C607 project description from 7th framework programme european+union grammar%23ENT grammar%23ENT finally evaluate distance semantics project relationship identify significant relationship from graph
computing methodology information extraction semantic+network keyword scientometrics map science word em bedding thesaurus permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
isbn grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148018 association+for+computing+machinery reference format grammar%23NPH grammar%23NPH science graph characterize recent scientific landscape use paragraph vector
grammar%23NPR grammar%23ENT grammar%23NPR grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery new+york new+york united+states+of+america https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148018 introduction grammar%23ENT price propose study science use scientific+method since then research scientometrics develop technique analyze research activity measure their relationship map science major topic scientometrics provide bird%27s+eye+view scientific landscape science be useful tool understand structure science their spread interconnection discipline by know such information science technology enterprise anticipate change especially initiate their immediate vicinity research+laboratory university that organize accord established standard disciplinary department understand organization environment furthermore such map be important policy analyst funding agency since research funding base on quantitative qualitative scientific metrics usually perform several analysis on map use statistical+analysis careful examination by human grammar%23NPR
conventional quantitative approach understanding research activity focus on author tell about past accomplishment through inter citation co citation analysis published research paper thus most map science from literature characterize accomplish certain period+of+time collect a+number+of citation therefore this paper focus on researcher currently want work on thier research project
main contribution this paper be construction novel science map characterize attempt recent scientific landscape in+order+to provide new insight
not advocate using map science alone as basis fund decision but use concert other well-established metrics process such as h index impact factor top 10%25 most cite paper peer cap grammar%23ENT grammar%23ENT united+states+of+america t. grammar%23NPH et+al.
description however not have reference not analyze use citation analysis thus propose analyze use content based method use natural+language+processing nlp technique recently word paragraph em bedding propose find relationship between unstructured description such em bedding technique represent word paragraph as real valued vector several hundred dimensions distance semantics between description calculate from difference similarity between vector
construct science graph graph be knowledge graph represent recent scientific trend where node represent research project that link by certain distance content similarity their grammar%23NPR
address un clustered problem associate current paragraph em bedding technique introduce cluster vector base on information entropy concepts thesaurus in+addition propose three approach find semantics relationship
remainder this paper organize as+follows section discuss related work introduction our experimental data+set section describe baseline method proposed method extract relationship between research project experiment evaluation describe section conclusion suggestion future work provide section
related work funding agency publisher generally have their own classification+system 7th framework programme european research technological development grammar%23NPR most project have three digit code code represent academic subject some project have more+than one code thus interdisciplinary project found by search multi label project however even if two project assign same category distance semantics their relationship not find
fund agency publisher use different category be no comprehensive scheme characterize project article thus not compare between different agency publisher for+example compare article association+for+computing+machinery grammar%23NPR springer nature classification require taxonomy exchange
project fund grammar%23NPR visualize base on article publish by project member science bibliographic database by examine co- authorship co- citation pattern in+addition be website provide grammar%23NPR visualization tool map journal document such studies similarity between journal article calculate use cosine and%2For grammar%23NPR similarity inter citation co citation map promote interdisciplinary research collaboration but project description not have citation grammar%23NPR edge have different semantics call graph
www.acm.org%2Fpublications%2Fclass-2012 grammar%23NPR mapofscience.com reference therefore citation analysis not utilize ongoing project recently publish grammar%23NPR although project description eventually include article their research result
several content based method propose related literature previous study examine automatic topic classification use probabilistic latent semantic analysis grammar%23NPR use find five most probable word topic each document view as mixture topic this approach classify document different agency publisher however relationship between project such as involve their distance semantics not compute directly in+this+regard national+institutes+of+health visual browser similarity between project as mixture classification probability each topic base on grammar%23NPR use average symmetric kullback-leibler divergence function however this similarity be combination probability be not derive from actual content semantics another study hierarchically structured set topic springer nature classification tag create from springer nature proceed paper base on ontology however source be limited computer+science similarity distance between pairwise article not compute
word paragraph vector vector be distributed representation word paragraph attract attention nlp assume that context determine meaning word word appear similar context consider have similar meaning basic form word vector represent as matrix element be co-occurrence frequency between word w certain usage frequency corpus word within fixed window size c from w. popular representation word vector be grammar%23NPR grammar%23NPR create word vector use two layered neural+network obtain by skip gram model negative sampling specifically word vector obtain by calculate maximum likelihood objective+function l likelihood eq. where t be number word certain usage frequency corpus grammar%23NPR cluster word similar meaning vector+space
t grammar%23NPR t c j c j log p wt j wt additionally le grammar%23NPR propose paragraph vector vector learn fixed length feature representation use two layered neural+network from variable length piece text such as sentence paragraph document paragraph vector consider another word paragraph grammar%23NPR recently publish article have reference but not cite enough from other article
nihmaps.org%2Findex.phpscience graph characterize recent scientific landscape use paragraph grammar%23ENT grammar%23ENT united+states+of+america share all context generate from same paragraph but not paragraph context be fixedlength sample from sliding+window over paragraph paragraph vector compute by fix word vector train new paragraph vector until convergence as show eq.
grammar%23NPR t log p wt wt c di where di be vector paragraph include wt
word vector share paragraph paragraph vector be unique among paragraph represent topic paragraph by consider word+order paragraph vector also address weakness bag-ofwords model grammar%23NPR therefore paragraph vector consider more accurate representation semantics content then input result vector into analysis use machine+learning clustering technique find similar project different academic subject as+well+as relationship between project from different agency
measurement project relationship this section introduce our experimental data+set then describe baseline method proposed method extract relationship between research project
project description data+set analyze grammar%23NPR project description description freely obtain from european+commission primary portal european+union funded research project following reason grammar%23NPR include several research area from bioscience computer+science
most project complete therefore entire structure include
project classify by more than subject terms i.e. security+insurance+company code
our experimental data+set title description 25%2C607 grammar%23NPR project grammar%23ENT grammar%23ENT including 305%2C819 sentence total all word sentence be tokenized lemmatized by before create vector+space our content based approach handle project description paper abstract same way but in+order+to characterize recent scientific landscape this paper present science graph research project as case+study
distance relationship before introduce proposed method present problem apply paragraph vector research project grammar%23NPR cordis.europa.eu%2Ffp7 grammar%23NPR stanfordnlp.github.io%2Fcorenlp description implement paragraph em bedding technique use grammar%23NPR then construct paragraph vector 25%2C607 grammar%23NPR project although need more systematic way but this+time hyperparameters set empirically as+follows dimension establish 66%2C830 word appear more+than five times window size c be learning rate minimum learning rate be respectively adaptive gradient algorithm learning model be distributed memory+model hierarchical
find that project scatter not cluster by any subject discipline vector+space
project slightly connect low number project thus be difficult grasp trend compare ordinary classification+system such as security+insurance+company code closely observe vector+space reveals reason this un clustered problem each word nearly same meaning have slightly different word vector shared but unimportant word consider commonality paragraph in+fact le grammar%23NPR report classification accuracy multiple category less+than 50%25 therefore address this problem introduce entropybased method cluster word vector before construct paragraph vector
fact synonym tend gather in+a+word vector+space indicate that semantics word spatially spread certain distance this observation also suggest related literature therefore unify word vector almost same meaning trivial common word generate cluster word vector base on semantic diversity each concept thesaurus first extract 19%2C685 hypernym broad terms more than equal one hyponym narrower term from grammar%23NPR thesaurus grammar%23NPR thesaurus primarily consist keyword frequently index 36%2C000%2C000 article accumulate by grammar%23NPR since grammar%23ENT currently this thesaurus update every year include 276%2C179 terms english japanese notation category from bioscience computer+science civil+engineering base on system skos grammar%23NPR thesaurus also exist linked data form semantic relationship skos broad skos narrower skos related
broad narrower relationship essentially represent is- a subsumption relationship but sometimes denote partof relationship geography body organ terminology other academic discipline grammar%23NPR thesaurus be publicly accessible from on j-global website along visualization tool then calculate information entropy each concept grammar%23NPR thesaurus from grammar%23NPR data+set grammar%23NPH entropy information+theory be estimate event informativeness grammar%23NPR deeplearning4j.org grammar%23NPR jglobal.jst.go.jp%2Fen grammar%23NPR thesaurus-map.jst.go.jp%2Fjisho%2Ffullif%2Findex.html japanese grammar%23NPR grammar%23ENT grammar%23ENT united+states+of+america t. grammar%23NPH et+al.
figure construction paragraph vector base on cluster vector
concept thesaurus
this entropy measure semantic diversity concept adopt from grammar%23NPR et+al. create cluster accord degree entropy unify all word vector same cluster cluster vector paragraph vector base on cluster vector overall flow show fig.
c j p grammar%23NPR c grammar%23NPR j p grammar%23NPR c hereafter word be word grammar%23NPR data+set term be term thesaurus terms classify into hypernym hyponym their synonym concept define as combination hypernym more than equal one hyponym one level below hypernym indicate as red+box fig. give that thesaurus consist terms ti calculate entropy concept c by consider appearance frequency hypernym grammar%23NPR hyponym tn as event probability frequency synonym grammar%23NPR sim term ti summarize corresponding concept synonym grammar%23NPR include descriptor terms ti eq. p grammar%23NPR c be probability synonym grammar%23NPR give concept c term ti each concept thesaurus calculate entropy h c grammar%23NPR data+set as probability event become equal h c increase if only particular event occur h c reduce because+of low informativeness thus proposed entropy concept increase when hypernym hyponym hyponym construct concept separately appear certain frequency data+set therefore degree entropy indicate semantic diversity concept then assume that degree entropy spatial size concept in+a+word vector+space be proportional certain extent split word vector+space into cluster in+fact our preliminary experiment indicate that entropy concept have high correlation r maximum euclidean distance hyponym concept vector+space at+least while entropy be rather high specifically refine cluster by repeatedly subdivide until defined criterion satisfy our method set determination condition as show eq.
wk cl wi h c wi h c wj wk-wi wk-wj cl wj otherwise this condition represent that word vector grammar%23NPR grammar%23NPR subdivide into two cluster proportionally ratio high two concept entropy h c wi h c wj select from all entropy concepts cluster initial cluster be whole vector+space c wi c wj mean concepts c word wi wj belong respectively word wi wj be word lemmatized form be indentical terms synonym thesaurus however note that entropy other word correspondence not include thesaurus not calculate eq. cl w mean cluster vector word w classify
vector+space subdivide until entropy become low than top 1.5%25 entropy number element cluster be low than parameter also determine empirically through experiment
generate 1%2C260 cluster from 66%2C830 word vector consider centroid all vector cluster as cluster vector then construct paragraph vector use cluster vector rather than word vector as show eq.
be extension eq. after+all each cluster vector represent concept concept have high entropy all concepts include cluster
grammar%23NPR t log p cl wt cl cl wt c di semantics relationship this section describe three approach find semantics project relationship since paragraph vector concept vector be cluster vector represent certain concepts be same vector+space compare offset vector middle point vector between paragraph vector concept vector moreover extract common concepts low entropy between project description
offset vector assume that offset vector between paragraph vector represent any semantics their relationship because relationship between two word such as woman man queen king characterize by their vector offset grammar%23NPR thus try compose offset vector by combine concept vector however because there be no complete solution use dynamic programming algorithm algorithm be method solve complex problem by recursively break grammar%23NPR graph characterize recent scientific landscape use paragraph grammar%23ENT grammar%23ENT united+states+of+america figure offset vector between two project figure middle point vector between two project into subproblems as illustrate fig. obtain approximate solution be at most three concept vector by repeatedly select concept vector vector be most similar offset vector
middle point vector also assume that middle point vector between paragraph vector represent any commonality project therefore as show fig. try compose middle point vector by combine concept vector through dynamic programming in+addition select three concept vector vector be most similar middle point vector
common concepts finally extract common concepts c between two project title description since entropy represent semantic diversity concept as describe section concepts not concept vector low entropy represent particular topic project description thus select three concepts low entropy as significant common concepts specifically extract lists common concepts that sort ascending+order entropy from following three category project grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR british+telecom be broad concepts concepts concepts appear title description thesaurus case common concepts upper category be than three concepts extract from low category
experiment evaluation science graph grammar%23NPR show fig. publicly accessible on our each node represent project distance between project correspond cosine similarity paragraph vector grammar%23NPR grammar%23NPR togodb.jst.go.jp%2Fsample-apps%2Fmap grammar%23NPR grammar%23NPR github.com%2Fgephi%2Fgephi%2Fwiki%2Fopenord figure science graph grammar%23NPR project that have cosine similarity other project grammar%23NPR biotechnology grammar%23NPR medical biotechnology lif life+science soc social aspect inf information media ips information+processing information+systems ict information communication technology application grammar%23NPH robotics
produce edge weighted graph use locate project accord thier distance grammar%23NPR produce layout wherein similar node place as close as possible show local cluster related node global structure related cluster compute 328%2C000%2C000 cosine similarity all pair 25%2C607 project however keep only be above given threshold figure as edge since number edge exponentially increase threshold
graph keyword search be available project title description security+insurance+company code each node represent project size node correspond project cost when mouse point node project title cost security+insurance+company code appear when node click details including project description appear below graph time bar on left side represent start year project graph move along time bar
each node couple publication month year zoom specified time window detect emerging trend however edge their label be invisible by+default since graph fill line currently develop more flexible graph drawing interface
all data node edge store our linked data database j-global knowledge thus data be able query use grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH observational measure
ask domain expert specify context two object consider identical be not intuitive task as identity context differ from one expert task another
specify some constraint on context be more effective way benefit from expert knowledge
thousand explicit contextual identity link detect reasonable time despite high connectivity between graph instance
contextual identity link use generate rule rule help predict missing observational measure
relevancy certain context vary depend on conducted observation for+instance identity mixture composition require task study mixture acidity while identity mixture step require task study experiment environmental+impact
rule detect more specific context have good error+rate than one detect less specific context
acknowledgment this work support by fund by grammar%23NPR
conclusion propose this paper approach link knowledge+base base on notion global context represent sub- ontology detect each pair individual target class give by user most specific context this pair be identical more general context infer from detected most specific thanks+to order relation hierarchize all global context furthermore this approach take+into+account some expert constraint constraint be form list necessary properties identity link list unwanted properties list property occur together first experiment this approach realize on two scientific dataset contextual identity link use generate rule rule serve prediction missing experimental observation
prediction rule certainty vary depend on specificity context
next step like exploit contextual identity link other task in+particular like discover causality rule contextual identity link serve compare experiment select relevant variable variable explain cause experiment result variation
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
infer same a fact from linked data iterative import-by-query approach proceedings on artificial+intelligence grammar%23ENT grammar%23ENT united+states+of+america
grammar%23NPH christian grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH a
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
scientific lens support multiple view over linked chemistry data
semantic+web grammar%23NPR 13th grammar%23ENT proceedings part i. 98+113
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR semantics owl grammar%23NPR springer 405+419
grammar%23NPH grammar%23NPH grammar%23NPH named graph provenance trust proceedings 14th international conference on association+for+computing+machinery 613+622
grammar%23NPH not quite identity constraint linked data grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH l grammar%23NPH owl web ontology language reference grammar%23ENT grammar%23ENT
li ding grammar%23NPH grammar%23NPH grammar%23NPH
grammar%23NPR linked data empirical study semantic+web grammar%23NPR 145+160
grammar%23NPH data linking semantic+web inf. syst. grammar%23ENT
grammar%23NPH grammar%23NPH graph- farm+credit+administration practice reasoning 22nd international conference on conceptual structure grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT proceedings 107+121
grammar%23NPH identity indiscernibles philosophy winter grammar%23ENT ed. grammar%23NPH ed. metaphysics research lab stanford+university
grammar%23NPH grammar%23NPH quality trust utility scientific data on towards joint model proceedings 3rd grammar%23NPR association+for+computing+machinery new+york new+york united+states+of+america article+15 page
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH assess link data mapping use network measure semantic+web research application grammar%23ENT 87+102
grammar%23NPH grammar%23NPH relational concept analysis mining concept lattice from multirelational data grammar%23NPH math. grammar%23NPR grammar%23NPR grammar%23ENT 81+108
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH when owl sameas be not analysis identity linked data springer
grammar%23NPH grammar%23NPH grammar%23NPH when owl grammar%23NPR be not same redux towards theory identity context inference on semantic+web international interdisciplinary conference on modeling using context springer
grammar%23ENT self-training approach resolve object coreference on semantic+web
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH po grammar%23NPR process observation ontology food science application dairy gel metadata 10th international conference grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT proceedings
155+165
grammar%23NPH grammar%23NPH uri disambiguation context linked data linked data on grammar%23NPR proceeding grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR vol. ceur-ws.org
grammar%23NPH grammar%23NPH unsupervised learning link discovery configuration 9th extended semantic+web conference grammar%23NPR
119+133
grammar%23NPH grammar%23NPH grammar%23NPH logical detection invalid grammar%23NPR statement rdf data knowledge engineering knowledge+management 19th grammar%23NPR grammar%23ENT grammar%23ENT proceeding 373+384
grammar%23NPH grammar%23NPH grammar%23NPH owl ontology language semantics abstract+syntax section rdf compatible model-theoretic semantics technical report grammar%23NPR www.w3.org%2Ftr owl semantics grammar%23NPH grammar%23NPH grammar%23NPR detect contextual identity technical report grammar%23NPR github.com%2Fraadjoe grammar%23NPR blob master technical grammar%23NPH grammar%23NPH combine grammar%23NPR numerical method data reconciliation grammar%23ENT
ontology aware prediction from rule reconciliation based approach grammar%23NPR based syst. grammar%23ENT 117+130
grammar%23NPH grammar%23NPH grammar%23NPH discover maintain link on data proceedings 8th springer-verlag 650+665 using title versus as source automated semantic document annotation grammar%23NPH grammar%23NPR economics l.galke%40zbw.eu stu96542%40mail.uni kiel.de stu111405%40informatik.uni kiel.de deb%40informatik.uni kiel.de grammar%23NPH grammar%23NPR economics a.scherp%40zbw.eu abstract conduct first systematic comparison automated semantic annotation base on either full text only on title metadata document apart+from prominent text classification baseline grammar%23NPR also compare recent technique learning rank neural+network revisit traditional method logistic regression grammar%23NPH our four dataset performance classification use only title reach over 90%25 quality compare performance when use full text
computing methodology grammar%23NPR learn by classification applied computing document analysis keyword multi label classification document analysis semantic annotation association+for+computing+machinery reference format grammar%23NPH grammar%23NPH use title versus full text as source automated semantic document annotation proceeding grammar%23NPR grammar%23ENT knowledge united+states+of+america grammar%23ENT grammar%23ENT grammar%23NPR grammar%23ENT page
introduction contrast full text document metadata be directly available on linked open data cloud accessible rdf format process no legal barrier semantic annotation
semantic annotation by use only title challenge since title be short thus carry only little information compare full text process semantic annotation be multi label classification task not only one but set concept be permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148039 need appropriately describe semantics document
run extensive series experiment compare established method recent method from machine+learning multi label document classification goal be decide whether be possible reach comparable classification performance when use only title document all approach operate on underlying machine+learning level level make comparison prevalent end-to-end ontology tag system such as grammar%23NPR ontology grammar%23NPR grammar%23NPR difficult instead show that despite not use hierarchical properties thesaurus presented method outperform best performing method method make use hierarchy such as one our own prior work apart+from well-known multi label classification baseline k nearest+neighbor grammar%23NPR revisit traditional text classification method such as grammar%23NPH also include prominent learning rank approach as+well+as modern variant neural+network motivate by success deep learning field in+the+past algorithm lazy learner family such as grammar%23NPR use dominate multi label classification task on such dataset large+number target class
show that eager learner such as logistic regression feed forward neural+network outperform lazy learner lazy learner as+well+as learning rank need store traverse o grammar%23NPR example grammar%23NPR space predict label single new document at test time contrast most eager learner have benefit o grammar%23NPR time complexity predict label set unseen document document important when apply automated semantic annotation process on-the-fly enrichment metadata on linked open data cloud summarize contribution this work be best our knowledge first large-scale systematic comparison multi label classifier apply either full text only title document result result show that eager learner such as neural+network linear model outperform lazy learner even when high amount possible label consider offer evidence that use only title high dimensional multi label classification be reasonable choice semantic annotation resource where only metadata be available our technical report reveal more details on approach including hyper parameter as grammar%23NPR www.opensemanticsearch.org%2Fsolr-ontology-tagger grammar%23NPR github.com%2Fzelandiya%2Fmaui-standalonek-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america l. grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH well as overview field publish full source+code our text+processing pipeline on grammar%23NPR
experimental setup dataset conduct our experiment on four dataset english document two dataset obtain from scientific digital+library domain economics political+science along two news dataset from new+york+times
each document dataset be manually created goldstandard annotation provide by domain expert expert work as professional subject indexer corresponding organization
each dataset provide domain specific thesaurus thesaurus serve as control vocabulary gold+standard concepts use as target label our multi label document classification task thesaurus also offer set concept specific phrase i. e. skos grammar%23NPR skos grammar%23NPR in+case+of grammar%23NPR format that use concept extraction from document full text title economics dataset consist document provide by grammar%23NPR economics annotation take from version be controlled domain specific thesaurus economics business+studies maintain by grammar%23NPR
thesaurus contain concept concept specific phrase from concept are use corpus thus consider multi label classification task each document annotate by domain expert on average label south+dakota political+science dataset have document similar economics dataset make legal agreement political+science dataset international+relations provide document label take from thesaurus international+relations contain concept equivalent number concept specific phrase i. e. be no alternative phrase from concept are use corpus each document dataset have on average label south+dakota dataset contain article
choose article where both title full text document be available from this set document randomly select 000 article match scale scientific corpus
our experiment employ thesaurus re- engineer from dataset by st.+george%27s et+al. thesaurus contain concept total concept specific phrase from concept are use corpus each document be on average south+dakota label new+york+times annotate corpus dataset new+york+times contain article each article have two set annotation create by professional indexing service annotation add by author use semi-automatic system use annotation provide by indexing service because be reasonable expect that be more consistent high quality cf as dataset choose random subset 000 document contain both full text title number concept new+york+times dataset be from concept are use our random+sample
github.com%2Fquadflor%2Fquadflor grammar%23NPR zbw.eu%2Fstw%2Fversions%2F9.0%2Fabout.en.html grammar%23NPR www.fiv-iblk.de%2Feindex.htm grammar%23NPR www.fiv-iblk.de%2Finformation%2Finformation_thesaurus.htm figure configurable text+processing pipeline use our experiment best performing strategy emphasize
document be on average south+dakota label
political+science dataset each concept consist only single concept specific phrase
method compare different method input text as show figure see our detail one vectorization base on term frequency tf israel+defense+forces base on concept frequency compact+flash israel+defense+forces cf experiment re weighting method grammar%23NPR use term frequency grammar%23NPR use concept frequency concatenation both terms concept denote by ctf-idf grammar%23NPR respectively as classifier employ grammar%23NPR cosine distance
performance grammar%23NPR rely on assumption that document well represent by feature that similar document have similar label therefore classification performance be good indicator quality feature
method determine best performing method compare classifier from lazy learner family as+well+as eager learner leverage generalized linear model optimize by grammar%23NPR grammar%23NPR logistic regression perform multi label classification binary relevance i. e. train separate classifier each class adapt learning rank approach multi layer multi label consider use threshold as+well+as stack decision+tree cf also experiment stack decision+tree on top binary relevance logistic regression please+note keep all hyperparameters fix all experiment dataset
prior counting term concepts both input text concept specific phrase thesauri be subject preprocess steps include discard all character except+for sequence alphabetic+character length at+least two word connect hyphen be grammar%23NPR versus automated semantic annotation grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america i. e. hyphen remove detected word be low case lemmatized base on morphological processing wordnet
evaluation separate each dataset into 90%25 training document 10%25 test document perform 10-fold cross validation such that each document occur exactly once test set hence each test document compare predicted label label set gold+standard evaluate formula+one measure formula+one measure be harmonic+mean between precision i. e. true positive false+positive recall i. e. true positive false+negative when no label predict precision set zero f score average over test document choose this sample based formula+one measure over class global variant because be closest assumed application where each individual document need be as good as possible
report mean sample based f score over ten+fold cross validation please+note be possibility that all document specific label fall into one test set
no training data be available label not exclude from our evaluation metric
result result vectorization method grammar%23NPH show result text experiment term based method tf israel+defense+forces perform consistently good than purely compact+flash israel+defense+forces method on both title full text difference f score range from on economics grammar%23ENT score on when combine term vector concept vector performance be at+least as good as other text method many case yield good result be more noticeable on title than on full text
re weighting not improve result compare grammar%23NPR neither in+case+of title full text rather observe decrease performance by up experiment use nearest+neighbor classifier indicate that ctf-idf be good method henceforth use ctf-idf compare performance classifier
classifier result compare different classifier document table generalized linear model logistic regression be close each other difference be no more+than any dataset consider learning rank observe that technique yield consistently low score than themulti layer overall eager learner lr outperform both lazy learner grammar%23NPH grammar%23NPR among all classifier mlp dominate on all dataset apart+from new+york+times on title where grammar%23NPR achieve high score
stacked decision+tree module increase f score logistic regression on all dataset than document label but impact stacking method be inconsistent learning approach be noteworthy that be case where classifier perform better on title data than same classifier apply on full text data
be on dataset grammar%23NPR on economics dataset as general rule however full text generate high score than title compare different classifier title full text make observation that some table sample f score text method use grammar%23NPR as common classifier grammar%23NPH econ. tf israel+defense+forces grammar%23NPR compact+flash israel+defense+forces grammar%23NPR ctf-idf grammar%23NPR title tf israel+defense+forces title grammar%23NPR title compact+flash israel+defense+forces title grammar%23NPR title ctf-idf title grammar%23NPR classifier train on title outperform train on full text apart+from new+york+times corpus eager learner lr grammar%23NPR mlp on title be superior grammar%23NPR on full text finally compare f score best performing multi layer on title score obtain on full text on new+york+times dataset 58%25 f score retain when use only title on political+science economics dataset retained f score be 83%25 91%25 respectively on dataset mlp use solely title retain 95%25 f score obtain full text
discussion conclusion result show that multi label classification text document reasonably conduct use only title document
all dataset multi layer on title retain 82%25 f score obtain on full text give empirical justification value automated semantic document annotation use metadata from first experiment find that combine word concepts as feature be preferable over alone concepts hold valuable domain specific semantic information term frequency on+the+other+hand hold implicit information information be as+well important correct classification
learner be by+design capable learning terms concepts need associate respective class result show that also lazy learner benefit from this joint representation
second experiment show that eager learner such as logistic regression mlp consistently outperform lazy learner multi label classification this result extend recent advancement multi label towards document classification scenario many possible output label only few example class
result title full text best performing classifier still perform better on full text be not surprising since full text hold considerably more information including title however all dataset apart+from new+york+times dataset grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america l. grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH sample f score classificationmethods use good vectorization method grammar%23NPR baseline grammar%23NPR lr mlp grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR title grammar%23NPR title title grammar%23NPR title title lr title title mlp title grammar%23NPR title grammar%23NPR title grammar%23NPR title grammar%23NPR difference f score best performing mlp be small difficulties classify document new+york+times dataset explain by characteristic that title consist only word on+average there be lower+bound on title length perform classification task since short+title limit amount available information thus prohibit discrimination from other dataset state that average word title lead at+least 80%25 retain f score thus require further investigation understand specific influence title length on classification performance complexity multilabeling problem depend on number available document label independent whether full text title use
binary relevance classifier suffer from conservative label assignment high precision low recall when many negative example only few positive example present during training while result stacked decision+tree module be inconsistent mlp alleviate conservative assignment problem binary relevance when only few document label be available
our experiment over four large-scale real-world corpus cover broad range domain economics political+science news not limit complexity by rare label kept all independent+variable as+well+as hyperparameters fixed our prior work use thesaurus hierarchy model label dependency dependency improve classification obtain by grammar%23NPR despite not make use hierarchy anymore be able achieve even high absolute f score use eager learning technique supply term feature in+addition+to concept therefore drop constraint hierarchical organization among label due this minimal amount requirement invariant configuration text+processing pipeline expect our finding generalize wide range other corpus
validate practical impact experimental result conduct qualitative assessment experimental result expert workshop three subject indexing specialist at grammar%23NPR national+library economics expert state that title be sufficient classification scientific document further note that title contain less information than intellectual indexer have available when manually conduct classification task document also point+out that researcher carefully choose their title findability expert argue that reasonably good automatic indexing base on title be valuable since not raise legal problem compare processing full text as discuss introduction conclude that use document title automated semantic annotation be not only technically possible high+quality but also valuable from practical point+of+view
this research co-finance by european+union moving contract no thank from grammar%23NPR valuable discussion
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH use title versus full text as source automated semantic document annotation grammar%23NPR e print grammar%23ENT grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH news personalization use compact+flash israel+defense+forces semantic recommender mining semantics association+for+computing+machinery
grammar%23NPH grammar%23NPH comparison different strategy automated semantic document annotation association+for+computing+machinery
grammar%23NPH grammar%23NPH multi value classification very short text advance artificial+intelligence springer
grammar%23ENT recommend mesh term annotate biomedical article be grammar%23ENT
grammar%23NPH grammar%23NPH fan grammar%23NPR grammar%23ENT new benchmark collection text categorization research machine+learning research grammar%23ENT
grammar%23NPR grammar%23NPH grammar%23NPH large-scale multi label text classification revisit neural+network machine+learning database springer
grammar%23NPH grammar%23NPH empirical study lazy multilabel classification algorithm artificial+intelligence springer
princeton+university about grammar%23NPR wordnet.princeton.edu grammar%23ENT
ling grammar%23NPR review on multi- label learning algorithm institute+of+electrical+and+electronics+engineers trans. grammar%23NPR data eng. grammar%23ENT grammar%23ENT grammar%23ENT
ling grammar%23NPR grammar%23NPR lazy learning approach multi label learning pattern+recognition grammar%23ENT
grammar%23NPR solve large scale linear prediction problem use stochastic gradient descent algorithm grammar%23NPR grammar%23NPR cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america t. grammar%23NPH et+al.
within domain model sufficient detail so that complete domain model express
be related level granularity need by requirement derive from idea representational expressive adequacy knowledge+representation+language as+many extension grammar%23NPR introduce last decade extend expressive power language obvious example inadequacy be where use basic grammar%23NPR not allow represent any sort numerical temporal information but where requirement demand that action have specific temporal duration
note that completeness domain model depend on language adequacy domain model be accurate all feature present conform requirement sense that their interpretation be true but be case that some requirement not represent at+all hence completeness domain model prevent because+of inadequate language
grammar%23NPR automated planning scheduling literature validation domain model often solely base on test whether lead acceptable behaviour planning scheduling application system be if acceptable plan be output in+fact give definition completeness introduce section above mention informal definition operationality see as weak form completeness however be normally many encoding any given domain model pass this test but some encoding lead much+more efficiently generated solution than our applied work on urban traffic+control for+example show that use same planner improve efficiency encoding considerably enhance scope problem instance that planner solve
that complete domain model exist there always be way re represent domain model without compromise completeness domain model give different result when input planner for+example not satisfy some real+time constraint requirement application domain
this sense operationality be also relationship partly dependent on choice planning engine
generally be also possible that two distinct domainmodels be complete but lead more efficient implementation good quality plan hence process find acceptable plan application depend not only on strategy use by planner but also domain model in+particular if model be not accurate then planner generate flawed plan no plan at+all
speed plan generation affect dramatically such circumstances for+instance case+study show that fix refine domain model e.g. add additional relevant knowledge improve performance planner without modify planner their search mechanism
work like show that add relevant redundant constraint form control knowledge rule domain model also speed+up planning engine thus provide solution plan short cpu-time moreover be worth remind that cpu-time be not only resource constrain requirement hard+drive usage for+instance represent critical resource as+well
given planner requirements specification define grammar%23NPR as attribute domain model planning engine e as+follows
domain model be operational with+respect+to planning engine e if e produce solution s problem instance p acceptable resource bounds such that s be acceptable solution p according+to requirement
this context acceptable resource bounds define for+instance term runtime memory usage number cpu etc. of+course assume that such resource bounds allow planning engine e run properly from this perspective operationality property domain model also change over time as acceptable resource bounds reduce due modified application requirement planning engine substitute machine on planning engine execute change since planning engine performance significantly affect by hardware software configuration exploited machine also be case that exogenous events such as major release compiler new version operating+system impact on operationality given domain model regard considered planning engine note that definition not demand that planner output all acceptable solution be somewhat infeasible computationally hence have weak definition definition be more in+tune practice
state-of-the-art domain independent grammar%23NPR planner automatically investigate operationality as+well+as perform check help promote confidence consistency accuracy knowledge model exploit pre processing step such as reachability analysis assess whether specified goal achieve by exploit available action object describe initial state while this pre processing step be incomplete sake planning efficiency recent work focus on provide formal complete unsolvability proof form certificate considered planning problem furthermore grammar%23NPH et+al. introduce approach that give problem problem be unsolvable i.e. inconsistent suggest how modify model in+order+to make solvable finally be tool such as torchlight focus on investigate shape search space some well-known heuristics approach this sort technique already fruitfully exploit obtain overview consistency problem model describe grammar%23NPR implicitly investigate also domain model
discussion one key question be how well modelling language support pursuit form high+quality knowledge model particularly domain model language feature support faithful engineering model grammar%23NPR for+example show not be too good this case primarily because be few grammar%23NPR knowledge automated planning towards notion quality grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america process keepinter parameter p phase intersection precondition inter p%29 contain p%29 intertime i%29 interlimit p%29 effect increase intertime i%29 t figure example process grammar%23NPR model time pass between phase traffic+signal road intersection time between phase call inter green time construct such as state invariant help cut down on bug
weakness use grammar%23NPR knowledge engineering be that be typically much departamento+del+meta knowledge leave implicit govern be acceptable state problem instance application be not not capture by grammar%23NPR application this departamento+del+meta knowledge state as set invariant on planning state prove very useful check validity problem instance but state within another formalism
question be do definition key criteria easily generalise more expressive language
grammar%23NPR be language encode domain model model contain processes events continuous variable as+well+as action discrete+variable example simple grammar%23NPR process definition give figure model passing inter green time between two consecutive phase traffic+signal road intersection while general definition accuracy grammar%23NPR model be still appropriate procedure check grammar%23NPR give rely on there be finite grounding operator be not case hybrid language contain continuous variable also operational semantics grammar%23NPR be much+more complex than classical planning problem do co-occurrence action processes cascade event as found hybrid automata thus while properties define paper be still useful check quality such expressive model be more difficult
case multi agent planning multi agent plan generation additional machinery require be public privacy nature feature predicate action be several scheme on how deal such as use feature be either global all agent restrict one agent localise quality criteria each agent then look+at overall quality terms sum individual plan generation conceptualise as each agent create their own complete plan public part plan combine by central agent in+order+to achieve main goal this case contrast case grammar%23NPR there needs additional quality criteria deal communication and%2For privacy aspect additionally concepts such operationally be more complex since execution plan depend on more than one agent
conclusion this paper investigate role nature planning domain model discuss importance process create planning application
quality domain model need many reason not little help engineer construct model ii underpin tool environment environment help process create model iii assist efficiency planning iv assess action learning program v compare one domain model against another
introduce notion consistency accuracy completeness adequacy operationality with+respect+to formation planning domain model their associated problem instance
introduce much needed discussion on subject explore implication definition as aspect quality planning knowledge model our future work further explore definition how use compare contrast existing keps tool method
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR jc j grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al. grammar%23ENT grammar%23NPR mixed initiative planning schedule mars exploration rover mission
intelligent+systems grammar%23ENT
use temporal logic express search control knowledge planning artificial+intelligence grammar%23ENT 123+191
grammar%23NPH grammar%23NPH verification validation meet planning scheduling on software tool technology transfer grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al. grammar%23ENT towards structured approach building qualitative reasoning model simulation ecological informatics grammar%23ENT
grammar%23NPH grammar%23NPH
fifth international competition on knowledge engineering planning scheduling summary trend grammar%23ENT 104+106
grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPR acquire planning domain model use grammar%23NPR knowledge grammar%23ENT 195+213
grammar%23NPH la grammar%23NPH grammar%23NPH learning domain control knowledge grammar%23NPR beyond on planning learning
grammar%23NPH grammar%23NPH grammar%23NPR temporal logic based planner grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPR certificate classical planning proceedings on planning scheduling
grammar%23NPH grammar%23ENT model mixed planning grammar%23ENT 235+297
grammar%23NPH concise introduction model grammar%23NPR automated planning grammar%23NPH grammar%23NPR publisher
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR planning domain definition language dcs grammar%23NPR computational vision control
grammar%23NPH grammar%23NPH grammar%23NPH automated planning theory practice grammar%23NPH
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH come grammar%23NPR good excuse do when no plan can find proceedings on automated planning scheduling
analyze search grammar%23NPR run any search on connection between causal graph h grammar%23ENT 155+229
grammar%23NPH grammar%23NPH critical assessment benchmark comparison planning grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPH automatic plan validation continuous effect andmixed initiative planning use grammar%23NPR proceedings 16th on tool artificial+intelligence 294+301 grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america t. grammar%23NPH et+al.
grammar%23NPH grammar%23NPH control knowledge planning benefit tradeoff proceedings seventeenth national conference on artificial+intelligence
grammar%23NPH completeness formal specification language design process control+system proceeding third workshop on formal+methods software practice
grammar%23NPH grammar%23NPH grammar%23NPH planning domain plan validation verification analysis workshop on v v planning
t. grammar%23NPH knowledge engineering issue grammar%23NPH plan community keynote talk proceeding grammar%23NPR workshop on knowledge engineering tool technique grammar%23NPH planning
grammar%23NPR language purpose grammar%23NPR grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPR grammar%23NPR planning roadmap
grammar%23ENT
engineering compiling planning domain model promote validity efficiency artificial+intelligence grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPR automate planning within urban traffic management operation proceedings international conference on automated planning scheduling grammar%23NPR
draw grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR planning domain definition language dcs grammar%23NPR computational vision control
grammar%23NPH grammar%23NPH multi objective optimisation machine+tool error mapping using automated planning expert syst. appl. grammar%23ENT
grammar%23NPH using model checking validate ai planner domain model proceeding 23rd
grammar%23NPH grammar%23NPH grammar%23NPH roman grammar%23NPR inspect edit document simply efficiently grammar%23NPR studio
grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR tool verify grammar%23NPR planning domain grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH
unified+modeling+language specification version
grammar%23NPH grammar%23NPH grammar%23NPH representation matter planning competition proceeding grammar%23NPH
grammar%23NPH management
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPH explore strategy design model road+traffic accident management domain proceeding grammar%23NPR
s grammar%23NPR on compare planning domain model proceeding workshop
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23ENT international planning competition progress trend grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH on effective configuration planning domain model proceeding grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH efficient macroscopic urban traffic model reduce congestion grammar%23NPR planning approach on artificial+intelligence grammar%23NPR
grammar%23NPH grammar%23NPR vaquero grammar%23NPH j grammar%23NPH et+al. grammar%23ENT improve planning performance through post- design analysis proceeding grammar%23ENT workshop on scheduling keps
grammar%23NPH s vaquero grammar%23NPH grammar%23NPH j grammar%23NPH
towards integrated design system real planning application
grammar%23ENT 215+230
grammar%23NPH grammar%23NPH grammar%23NPH model amnesty+international planning task proceeding grammar%23NPR
grammar%23NPR grammar%23NPH grammar%23ENT learning complex action model quantifier logical+implication artificial+intelligence grammar%23ENT https%3A%2F%2Fdoi.org%2Fdoi grammar%23NPR automatic rdf key ranking houssameddine.farah%40lirmm.fr danai.symeonidou%40inra.fr todorov%40lirmm.fr abstract automatic approach key discovery on rdf dataset generate set discriminative property use configure system rely on link specification key often come large+number generate independently two dataset link lack assessment their usefulness linking task propose novel generic algorithm select key valid two dataset rank with+respect+to their individual likelihood generate identity link in+addition explore combined use several complementary key improve their individual performance evaluate our approach on diverse synthetic real-world benchmark data show robustness with+respect+to different linking tool domain
key ranking key selection link specification association+for+computing+machinery reference format grammar%23NPH
automatic rdf key ranking proceedings washington+dc district+of+columbia united+states+of+america grammar%23ENT grammar%23NPR page
introduction number diversity rdf dataset publish as link open data on be continuously growing during past couple year data project fullness suppose binding dataset dense network entity link premise premise remain challenge date vast majority resource particularly establish identity link dataset entity commonly give form owl grammar%23NPR statement be crucial realization this project promise enable interoperability facilitate access rich more diversified information at data scale be hard imagine perform this task manually therefore a+number+of instance entity matching link discovery tool develop make available community data practitioner researcher
pipeline tool implement begin by often manual selection set property compare as part configuration protocol comprise tuning other parameter such as similarity measure on permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright third-party component this work honor
all use contact owner author s
grammar%23ENT washington+dc district+of+columbia united+states+of+america grammar%23ENT copyright hold by owner author s
isbn 123+4567+24+567+08+06
literal threshold in+order+to facilitate discovery discriminant properties automatic key detection algorithm apply however algorithm commonly operate on single dataset thus not accounting potential commonality difference two more dataset link more importantly often return very large+number key reach on certain data several hundred no intuition provide about their individual usefulness task at+hand
above propose generic approach implement system grammar%23NPR approach stand between key discovery process attempt bridge two give set key generate by key discovery system on two rdf dataset rank key so as favor most likely produce identity link between dataset rank correspond individual likelihood each key produce link when use link specification file system such as silk lime that process both dataset process simultaneously ensure applicability key on two dataset hence their usefulness linking task our method represent instance by use their concise bounded description information find on different depths graph allow efficiently handle object properties have uri value value bypass major shortcoming key discovery tool take+into+account various data heterogeneity at key selection step by explore partial similarity string literal grammar%23NPR be generic tool independent on used key discovery system on data
separately propose method allow combination selected subset key that apply together increase number discovered correct link this end introduce apply notion key complementarity
evaluate our approach on diversified data come from different instance match track as+well+as on data about classical+music ensure reproducibility use experimental data refer make available executable version our system provide download from https%3A%2F%2Fgithub.com%2Fhfarah%2Fkeyranker-2017
like draw reader attention fact that this paper attempt resolve same problem as previous work two co-author note that approach present here be substantially different from obtained result considerably improve in+order+to avoid any ambiguity explicitly outline difference between two framework confront experimentally section
sequel review related work comparison our approach then introduce in+detail grammar%23NPR evaluate algorithm discuss obtained result before conclude
www.w3.org%2Fsubmission%2Fcbd grammar%23NPR oaei.ontologymatching.orgconference washington+dc district+of+columbia united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH provide correct owl grammar%23NPR link rdf graph be challenging task due volume multiple form heterogeneity data many approach propose past year recently survey consider our work relate certain group sub- problem linking task describe below
most commonly used linking tool require previous knowledge provide by either user another tool in+order+to proceed linking task this knowledge express form link rule describe condition two instance compare link linking rule compose various parameter such as type instance compare certain exception like rely on ontology match technique identify equivalent class set property compare similarity measure s use compare value property along their threshold aggregation function allow combine measure
parameter very challenge even expert
approach that consider related complimentary introduce assist this process
set selection intuitively instance have common value highly discriminant set property usually refer as key be likely represent same real-world object while many approach automatic key discovery from rdf data exist their use linking be not always straight forward
tool produce large+number key valid on single dataset with+the+exception+of no assessment give their likelihood discover link system not consider heterogeneity properties use describe instance dataset compromise usefulness key linking task
grammar%23NPR notable exception be not aware other approach allow produce list key valid on two dataset rank with+respect+to their usefulness task this work author propose selection ranking algorithm use support based measure take+into+account distribution generated key this measure favor key key be valid maximum number instance assume that frequent key have more chance produce link however obtained result show that top ranked key not guarantee good performance thus fail support premises approach additionally key identification exploit only value literal uri resource directly connect resource link on+the+one+hand this information be insufficient because precise description resource use information information be at great depth on+the+other+hand use uri key identification process result invalid key likely case dataset use different vocabulary
link specification several link specification algorithm mostly base on machine+learning technique introduce automatically tune parameter such as similarity measure threshold cite grammar%23NPR extension silk raven genetic algorithm based approach describe contrarily key discovery method approach mainly focus on automatic selection combination tuning similarity measure apply on value comparable property identification property compare do by matching algorithm no key computation imply this process efficiency algorithm improve if system know on properties on what type value similarity measure apply that line+of+thought propose genetic algorithm based approach discover link specification including discriminative properties as part thorough processing chain this approach proceed supervised manner necessitate set already correct link produce linking rule
our method suggest unlock potential keybased technique by provide user list ranked key valid on set dataset well suit particular instance match task
also address discovery key set dataset propose no strategy sort key key not allow effectively decide on use one key as oppose another in+addition output use their system whereas grammar%23NPR be generic apply tool like silk lime
compare automatic link specification algorithm our approach see as complementary focus on identification limited set property use effectively link dataset while leave choice similarity measure their combination tune user auto link specification method give above with+regard+to direct comparison appear be difficult our case produce rank discriminating properties fixed data preprocess protocol properties exploit way decide by user while their case full linking recipe produce evaluate be linking quality while evaluate this work be quality each key with+respect+to linking task regardless+of tool use
internals linking system silk lime if several combination property explore link retain depend on aggregation selection function previously set configuration+file key thus not rank but their respective value similarity score aggregate
pay particular attention disambiguate current approach since attempt solve same problem in+addition+to develop by cross group co-author
therefore dedicate separate section methodological comparison two show that base on different paradigm two approach then confront experimentally demonstrate achieved improvement
preliminary grammar%23NPR resource+description+framework be data+model use describe statement about resource form triple subject property object denote s p v for+example grammar%23NPR grammar%23NPR grammar%23NPR state that grammar%23NPR be writer grammar%23NPR this work consider rdf data conform owl grammar%23NPR ontology grammar%23NPR ontology represent as tuple form south+carolina socialist+party south+africa where south+carolina be set class ontology socialist+party set property south+africa set axiom assume that rdf dataset compose instance class c sc engineering knowledge automated planning towards notion grammar%23NPR united+kingdom lee%40hud.ac.uk massachusetts+institute+of+technology tvaquero%40mit.edu grammar%23NPH united+kingdom m.vallati%40hud.ac.uk abstract automated planning be prominent artificial+intelligence challenge as+well+as be common capability requirement intelligent autonomous agent critical aspect call domainindependent planning be application knowledge add planner create complete planning application make explicit domain model model be formal representation persistent domain knowledge ii associated problem instance contain detail particular problem solve component use by automated planning engine reasoning in+order+to synthesize solution plan
knowledge use planning engine be currently ad-hoc process where skill knowledge+engineer significantly influence quality resulting planning application on top notion quality knowledge capture within domain model miss be therefore hard provide useful guideline knowledge+engineer
paper raise some issue relate engineering application knowledge automated planning focus on domain model use idea domain model as formal specification domain consider mean measure quality such specification do this propose definition attribute domain model encoding language need by automated planning community in+order+to improve tool support engineering planning knowledge advance toward shared inclusive definition quality domain model
concept grammar%23NPR artificial+intelligence knowledge+representation reasoning planning scheduling corresponding author
make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than author s honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s publication right licensed association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148012 keyword automate planning knowledge engineering domain model association+for+computing+machinery reference format grammar%23NPH engineering knowledge automated planning towards notion quality
proceedings grammar%23NPR grammar%23ENT association+for+computing+machinery new+york new+york united+states+of+america https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148012 introduction automated planning be research discipline address problem generate totally partially order sequence action transform environment from some initial state desired goal state automated planning successfully apply decade several area including space+exploration machine+tool calibration urban traffic+control mention
intensive development domain independent planner contribute advancement planning technology as planning engine exploit as embed component within larger framework since accept domain problem instance well defined interface language return plan use same syntax interchange without any change rest system on+the+other+hand efficiency plan generation remain most prominent challenge artificial+intelligence domain independent planning engine deal complexity issue inherent plan generation exacerbate by separation planner logic from domain knowledge
engineering planning scheduling keps define grammar%23ENT planet roadmap specifically domainindependent planner as collection process involve acquisition validation verification maintenance planning domain model ii selection optimisation appropriate planning machinery iii integration ii form automated planning schedule p%26s application
see as special case knowledge engineering where need methodology acquire domain model manage formally capture knowledge long accept
be also related area capture conceptual knowledge develop domain model qualitative reasoning general modelling simulation area however peculiarity automated planning scheduling application distinguish keps from general+knowledge base simulation system
grammar%23NPR be concerned acquisition representation knowledge about action resource processes events effect have on state secondly this knowledge be be grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america t. grammar%23NPH et+al.
create system system synthesise plan rather than perform more common function knowledge system such as classification diagnosis decision+making thirdly knowledge often acquire two part specification persistent knowledge literature this part call domain domain model specification particular scenario represent planning problem instance as illustrative example planning recently apply area urban traffic+control
resulting industrial led project use real scenario data domain model use contain representation continuous process event action scenario problem instance contain hundred assertion concerning road link junction traffic+flow traffic+signal configuration
on keps lead creation several tool technique support design domain knowledge structure use planner real-world problem tool present specialised workshop such as knowledge engineering planning grammar%23NPR workshop verification validation workshop as+well+as competition such as international competition on knowledge engineering planning scheduling grammar%23NPR competition motivate development powerful keps system advance domain model technique language analysis approach however grammar%23ENT edition grammar%23NPR also highlight two main issue firstly most team not use any keps tool except text+editor thus rely only on their expertise
more worrying be fact that some team be not aware existence keps tool secondly number participant grammar%23NPR be still not very large especially when compare late edition international planning competition suggest that planning community underestimate importance knowledge engineering despite enormous impact on applicability domain independent planning real-world scenario
algorithm tool representation language support organise design such domain model be important e.g. as show by grammar%23NPR related area knowledge-based+system but need be related commonly agreed domain model property metrics object design within process domain model creation design this paper focus on attempt define set fundamental domain model property research topic topic not seem progress since much earlier work introduction discussion properties be pivotal importance planning artificial+intelligence community allow improve keps tool support knowledge engineering planning domain model advance towards shared inclusive definition quality domain model
remainder this paper organise as+follows section contextualise present this paper then section relevant background information on automated planning introduce section concept domain model define section property metrics introduce describe implication property be icaps14.icaps-conference.org%2Fworkshops_tutorials%2Fkeps.html grammar%23NPR icaps11.icaps-conference.org%2Fworkshops%2Fvvps.html grammar%23NPR icaps-conference.org%2Findex.php%2Fmain%2Fcompetitions further discuss section section conclusion give
motivation accelerated development ai planning facilitate take+up planning tool as component autonomous system take+up mean that engineering issue surround automated planning be growing importance stakeholder user in+particular autonomous system need be assured dependability such system be need demonstrate traditional engineering quality within product such as validity reliability autonomous system be paramount importance many application be safety critical safety related in+particular area robotics autonomous vehicle thus be able define measure quality planning related knowledge be necessary process engineer final product
particular issue emerging aspect agent capability be their ability be able autonomously capture engineer refine knowledge this case engineer engineer create agent require method metrics evaluation criteria discuss quality knowledge capture by agent major part this knowledge enable rational planning reasoning be form domain model in+order+to compare different learning algorithm need be able discuss ultimately measure quality resulting model
motivation be increasing complexity domain model require need capture such knowledge structured well-founded way multi agent planning for+example plan be both generated execute distributed fashion by collection agent as+well+as conventional dynamical aspect knowledge about action public privacy feature reside individual agent need capture again planning hybrid system specification continuous variable continuous process exogenous events need capture faithfully as necessary precondition planning
automated planning automate planning specifically classical planning deal find partially totally order sequence action transform static deterministic fully observable environment from some initial state desired goal state classical representation atom be predicate united+states+of+america define as set ground predicate planning operator o name o pre o eff o eff o be specified such that name o grammar%23NPR xk be unique operator name grammar%23NPR xk be variable symbol argument appear operator pre o be set predicate represent operator precondition eff o eff o be set predicate represent operator negative positive effect action grind instance planning operator action pre eff eff be applicable state s if only if pre s. application s if+possible result state s eff eff
state this knowledge make explicit two component domain model problem instance when use dominant family planning knowledge+representation+language grammar%23NPR planning domain model problem instance grammar%23NPR knowledge automated planning towards notion quality grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america provide planner as two different file same domain model use all problem application restricted world+view classical planning domain model be specified via set predicate planning operator problem instance be specified via initial state set goal atom need reach solution plan be sequence action such that consecutive application action plan start initial state result state satisfy goal
classical planning model extend in+order+to handle wide range constraint increase expressiveness for+instance be case temporal planning where action have duration duration consider uncertainty planning planning study case environment be not fully observable effect be deterministic further extension name grammar%23NPR contain construct define hybrid domain including process event on this matter interested reader refer
domain model automated planning domain modelling be phrase use perhaps variety meaning computer+science applied+mathematics domain model often describe as abstract conceptual description some application use as aid software development process
form as part requirement analysis in+order+to specify object actor role attribute etc independent software implementation domain model often represent imprecisely use diagram such as uml human consumption be benefit analyst developer explore requirement subsequently create software application area model
meaning domain model represent knowledge within planning application be much+more specific be still abstract conceptual description some application area but encode different purpose analysis reasoning manipulation by planning engine in+order+to solve planning problem
assume that requirement specification planning component some wide project be available requirement be mind domain expert describe informally diagram textual document describe at+least part formal+language e.g. as use grammar%23NPR requirement specification naturally contain description kind planning problem that planner need solve kind plan need provide as output for+example be essential that resource consumption take into consideration so plan need generate achieve goal while minimise resource consumption before domain independent planner choose use domain information need conceptualise formalise during this process assumption feature feature be essential represent domain model derive from overall requirement within this context define planning domain model as+follows definition planning domain model be formal specification application domain part requirement specification represent entity invariant over every problem instance action load-truck parameter obj obj truck truck loc location precondition at truck loc at obj loc effect not at obj loc obj truck figure example possible encoding grammar%23NPR operator grammar%23NPR
as object class function property relation operator
be line terminology from general+knowledge engineering specific work on domain theory physical system call domain file most common planning domain encoding language grammar%23NPR in+particular expect language domain model write have well-defined syntax operational semantics mean that independent planner domain be defined process execute plan plan correspond sequence action application domain in+other+words be sound interpretation dynamics any well-formed domain model such interpretation embed into validator tool such as grammar%23NPH
problem instance give knowledge model definition grammar%23NPR be domainmodel corresponding problem instance example grammar%23NPR encoding part domain model be load-truck operator show figure such operator describe how state world change effect section when truck load at location loc lists precondition precondition meet in+order+to correctly apply operator
model quality central part domain model ai planning be representation set action that planner reason about element that support specification action form potentially complex knowledge+base correctness be essential factor overall quality planning function indeed grammar%23NPR et+al. state that domain model present big validation verification challenge planning scheduling community question include how domain model formulate debug generally judge fit purpose how determine quality domain model quality language write criteria choose one domain model encode rather than another within that language for+example show that choice representation within same language domain as+well+as order element list within domain have significant grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america t. grammar%23NPH et+al.
efficiency planning process domain model production have general bear on quality as+well+as inform general community planning application community answer question form underpinning any tool set assist this development process
model often formulate into planner input language directly from requirement specification use only basic editor be so-called hand-crafted domain model such as grammar%23NPR studio online editor grammar%23NPH editor some case requirement encode firstly into more applicationoriented language such as grammar%23NPR ais ddl grammar%23NPR interface then map into target planner input language international competition on knowledge engineering automated planning schedule for+example be dedicated system expect requirement capture application oriented notation then domain model produce automatically
this case quality domain model be dependent on initial encoding correctness translation process
variation on domain model formulate by automated acquisition tool such as lamp system grammar%23NPR system type system acquire domain model from example plan little no pre- engineer domain knowledge lamp system form grammar%23NPH domain model from example plan script associated initial goal state aim at help knowledge+engineer create new domain model as author maintain that learning model need be hand-crafted remove bug another system grammar%23NPR exploit assumption object centred domain enable learn from plan script only as lamp grammar%23NPR output model grammar%23NPH format but input only training plan script not require representation initial goal state any description predicate object class state etc. context engineering process training plan therefore consider part requirement specification be example kind plan that system require generate this class learning system concept quality domain model be key be able evaluate learning process
quality domain model be naturally part verification validation grammar%23NPR process overall planning system importance grammar%23NPR domain model planning long recognise both domain independent work more specifically space application way assess quality domain model class into two type dynamic static similar way investigating program code quality in+practice debug validate domain model be invariably use dynamic testing test ability execute planner domain model particular problem instance this dynamic view grammar%23NPR domain model take work grammar%23NPR et+al. author focus on model checking method often use test formal specification specification exhaustively check all reachable state test whether path find goal model checking be not feasible large grammar%23NPR editor.planning.domains grammar%23NPR grammar%23ENT kti.mff.cuni.cz bartak grammar%23NPR figure overview domain model property how relate domain requirement planning engine potentially generate plan current knowledge model exploited language
model however as state space explore be astronomical state be reachable depend on starting position domain model check process in+other+words process work fixed initial state whereas ai planning need investigate domain model over range initial state
consider static as+well+as dynamic properties syntactically correct domain model through introduce notion quality style software quality domain model software metrics define three attribute domainmodels accuracy consistency completeness domain model encode language adequacy domain model planner pairing operationality attribute metrics relate intend use investigate issue engineering domain model embed knowledge engineering tool hence contribute overall validation verification planning system
show domain model property way relate aspect element involve knowledge engineering process planning application i.e. domain requirement planning engine resulting potentially generate solution plan current knowledge model exploited model language
consistency assume knowledge model contain set logical+expression take model-theoretic approach semantics domain model problem instance use this form property definition as show figure interpretation planning knowledge model be mapping component feature object relation etc. some application domain
initial state then become simple assertion ground operator then become more complex assertion about when be possible execute action effect action have grammar%23NPR knowledge automated planning towards notion quality grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america application domain clearly interest knowledge model have interpretation give by requirement specification make assertion model true
knowledge model be consistent if there exist at+least one interpretation interpretation make assertion true
check whether be any interpretation interpretation make planning domain knowledge true therefore consider simply as property knowledge model consistency check be therefore application independent spot obvious but common error e.g. if domain knowledge admit state state contain both fact fact negation then no interpretation make state true hence no interpretation satisfy again if operator figure contained as additional precondition term not at obj loc then no interpretation satisfy hence operator never ground into usuable action
widen this definition accuracy by consider truth assertion when interpretation give by requirement specification application at+hand
knowledge model accurately represent part requirement specification if interpretation give by map component feature object relation etc. that specification make all assertion knowledge model true
consistency be thus special case accuracy accuracy be attribute knowledge model related application domain feature consider as part requirement specification
accuracy be essentially informal process if requirement describe informally if requirements already encode some formal+language then knowledge model be accurate if requirements provide model knowledge sense that knowledge model be abstract algebra requirement concrete algebra
use property grammar%23NPR encoding grammar%23NPR knowledge model encode as domain file problem file dynamics dynamics allow planner generate solution plan when execute from initial state reach goal state describe domain file check accuracy operator this file can create all possible reachable grounding operator use object problem file map logical+expression precondition ground schema set p relation property requirement map logical+expression effect set e relation property application check that if p be true application then action domain model execute if execute make e true
similar process use assess accuracy problem instance contain problem file be matter check that initial state goal map problem embed requirement specification
domain be inaccurate then be possible create metrics measure inaccuracy for+example grammar%23NPR domain learning system such as lamp be need evaluate quality domain learn as part learning tool evaluation use comparison hand-crafted domain author judge accuracy domain by count number missing predicate learned domain relate our definition hand-crafted domain take role part requirement this case requirement be state precisely formal+language though be matter future research discover sound way measure difference when compare two domain model
rule+of+thumb if all considered problem instance show be inconsistent similar way argue that domain be inconsistent as+well on+the+other+hand if consistency domain merely assess by analyse consistency problem fact that no inconsistency identify such set considered problem be not sufficient claim that domain be consistent every possible problem from domain
completeness software+engineering completeness formal specification recognise as difficult but important topic topic best tackle specific type application e.g. grammar%23NPR deal process control+system use check+list automated planning introduce following definition definition give specific problem instance domain their interpretation map requirement specification then domain be complete if following condition hold any solution plan s problem instance p form from ground operator domain s be acceptable solution p requirement ii converse be also true be any acceptable solution s problem p there exist solution plan s derivable from domain problem p such s s
requirement contain set problem .pn require solution fixed hence each grammar%23NPR need be complete this sense as example fail include operator schema be necessary provide solution cause incompleteness use definition be possible find domain model model be complete but not accurate be so because give complete be possible add construction e.g. extra operator construction interpret false requirement specification but never interfere solution required problem solve
adequacy adequacy be relationship between requirements language encode give fact that requirement not change usually easily modify define adequacy on basis chosen language
language be adequatewith respect requirement specification if have expressive power represent grammar%23ENT washington+dc district+of+columbia united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPR b grammar%23NPH et+al. figure result on grammar%23NPR dataset use silk grammar%23NPR
grammar%23NPR b grammar%23NPH et+al. figure result on grammar%23NPR the+halo+trust dataset use silk grammar%23NPR
using lime b using silk figure result on grammar%23NPR the+halo+trust use lime silk grammar%23NPR p threshold k threshold differ depend on k threshold reference grammar%23NPH grammar%23NPH grammar%23NPH
automatic key selection data linking grammar%23NPR springer
grammar%23NPH grammar%23NPH grammar%23NPH data interlink through robust linkkey extraction grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH key grammar%23NPR grammar%23NPR 144+153
grammar%23NPH grammar%23NPR open source+data cleaning deduplication record linkage system graphical+user+interface grammar%23NPR
grammar%23NPH learning expressive linkage rule use genetic programming vldb endowment grammar%23ENT grammar%23ENT grammar%23ENT
grammar%23NPH grammar%23NPH efficient multidimensional blocking link discovery without lose recall grammar%23NPR
grammar%23NPH grammar%23NPH silk rdf link while publishing consume link data grammar%23NPR p d vol.
grammar%23NPH grammar%23NPH grammar%23NPR logic based scalable ontology matching grammar%23NPR springer 273+288
grammar%23NPH grammar%23NPH grammar%23NPH survey current link discovery framework semantic+web grammar%23ENT 419+436
grammar%23NPH grammar%23NPH
raven active learning link specification om ceur-ws
grammar%23NPH eagle efficient active learning link specification use genetic programming grammar%23NPR springer 149+163
lime time efficient approach large-scale link discovery on data grammar%23NPR
grammar%23NPH grammar%23NPH unsupervised learning link discovery configuration grammar%23NPR springer 119+133
grammar%23NPH grammar%23NPH grammar%23NPH
by grammar%23NPR architecture grammar%23NPR 265+274
grammar%23NPH grammar%23NPH automatic key discovery approach data linking j. grammar%23ENT
grammar%23NPH grammar%23NPH rocker refinement operator key discovery grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH
scalable almost key discovery rdf data grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR mining conditional key on knowledge+base grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH a
grammar%23NPH a. r grammar%23NPR key discovery numerical data application oenological practice grammar%23NPR 222+236 content recommendation through semantic annotation user review linked data i.vagliano%40zbw.eu grammar%23NPH diego.monti%40polito.it grammar%23NPH a.scherp%40zbw.eu grammar%23NPH maurizio.morisio%40polito.it abstract nowadays most recommender system exploit user provide rating infer their preference however growing popularity social e-commerce website encourage user also share comment opinion through textual review this paper introduce new recommendation approach approach exploit semantic annotation user review extract useful trivial information about item recommend also rely on knowledge freely available data notably grammar%23NPR discover other resource connect entity evaluate our approach three domain use both grammar%23NPR result show that our solution provide good ranking than another recommendation method base on data while improve novelty with+respect+to traditional technique base on rating
information system content ranking social recommendation data extraction integration personalization user review semantic annotation linked data data semantic+web grammar%23NPR introduction because+of increased amount machine-readable knowledge freely available on be high interest investigate how such information use improve recommender system linked grammar%23NPR be set good practice publishing interlinking data on be base data interconnected global knowledge graph currently most grammar%23NPR linkeddata.org permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148035 recommender system exploit rating infer user preference although growing popularity social e-commerce website encourage user write review review enable recommender system represent multi facet nature user opinion build fine-grained preference model model not obtain from overall rating
address issue mining review show how information combine linked data exploit recommendation task on one side linked data provide rich representation item recommend since include interesting feature on other side review reveal additional connection among item for+instance various review interstellar mention grammar%23NPH although be not direct link between two resource
propose new recommendation approach approach semantically annotate review extract useful information from entity knowledge freely available data then combine discover additional resource generate recommendation our method exploit any dataset available data provide recommendation although rely on grammar%23NPR our implementation perform offline study movie book music domain evaluate different property recommender system i. e. prediction accuracy term rating ranking diversity novelty result show that our method achieve high diversity provide good accuracy than method base on linked data increase novelty recommendation with+respect+to traditional technique
contribution this paper be threefold firstly exploit state-of-the-art semantic annotation technique extract from user review useful trivial information about item recommend entity be resource data thus discover additional knowledge through their link secondly rely on discovered entity provide recommendation take+into+account their occurrence review their relationship data thirdly validate our approach by evaluate effectiveness through offline study conduct movie book music domain
technical report extensively describe offline study provide additional information on our approach
wiki.dbpedia.org grammar%23NPR www.wikidata.orgk-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH approach architecture grammar%23NPR consist two main module semantic annotation discovery recommendation former be responsible feed recommender system semantically entity linked data through knowledge+base while latter provide recommendation user every time new review submit system execute semantic annotation discovery step possibly add new entity while recommendation process start when user provide initial item recommendation module work online while semantic annotation discovery do offline initially some review annotate resulting entity use discover additional entity through linked data
deal discovered entity item recommend consider item particular type entity since grammar%23NPR recommend item item be discovered entity although item not appear as entity system e. g. movie review but be never discover however not mean that entity correspond such film not exist considered knowledge+base semantic annotation discovery explain section while recommendation present section
semantic annotation discovery semantic annotation be process annotate textual multimedia content semantic tag add information about their meaning written+text do by associate uri recognized entity consider two popular semantic annotator annotator rely on aida be both capable entity accord surrounding context be useful because user frequently write acronym abbreviation select aida because more accurate accord independent comparison
module semantic annotation discovery analyze text review store identified entity relational+database uri each entity associate uri item occurrence that entity all review that item in+effect same entity appear again review another item grammar%23NPH be capable identify disambiguate entity mention review consider by+default one available grammar%23NPR
aida resource map equivalent available exploit similar structure uri contrast mapping between grammar%23NPR rely on owl grammar%23NPR predicate available if same entity correspond more than other knowledge+base ignore in+order+to avoid probable inconsistency same hold if be no owl grammar%23NPR property although well link grammar%23NPR grammar%23NPR
annotation allow grammar%23NPR exploit linked data retrieve additional entity be possible because entity be also resource data thus discoverer find resource resource relate entity in+order+to enable our system recommend more item review be source trivial relation for+example movie grammar%23NPR www.yago-knowledge.org recommendation scenario user mention movie movie remind uri reviewed one because+of color setting atmosphere feature be hardly available as linked data at+the+same+time linked data enrich contextualize information come from user
entity discoverer retrieve from knowledge+base other relevant entity through grammar%23NPR query
rely on some properties properties configure depend on domain on dataset consider discovery not bound particular knowledge+base domain on+the+contrary this approach be fairly general since rely only on rdf grammar%23NPR
more specifically discoverer read entity store during semantic annotation phase discoverer be then able obtain all resource resource have given entity as object selected property
discoverer store discovered entity relational+database efficiency reason grammar%23NPH discover entity associate uri entity through discover optionally grammar%23NPR measure between this measure be inversely proportional number link between two resource more link result low distance discover entity found through more than single entity grammar%23NPR exploit ranking phase phase describe section
recommendation recommendation process consist twomain step generation candidate recommendation their ranking give initial item grammar%23NPR retrieve all entity entity relate initial item then rank
system select entity entity mention review initial item afterwards obtain entity entity mention initial item i. e. entity review generate entity entity correspond initial item
if initial item be interstellar review grammar%23ENT space mention interstellar then grammar%23ENT space consider as candidate recommendation then grammar%23NPR optionally retrieve discovered entity include entity discover through initial item for+instance if initial item be grammar%23NPR previously discover because movie direct by grammar%23NPH select same hold if grammar%23NPR discover from i. e. grammar%23NPH be review latter similarly entity discover through other entity be review initial item be relevant for+example if grammar%23NPR be initial item grammar%23NPH be review grammar%23ENT space discover through grammar%23NPH then grammar%23ENT space be candidate recommendation be possible configure generator include candidate recommendation discovered entity not be also possible specify minimum occurrence require entity include candidate recommendation set express as percentage with+respect+to maximum occurrence entity review item consider content recommendation through semantic annotation user review grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america ranking grammar%23NPR rank candidate recommendation define three different ranking function present take+into+account only occurrence occur entity available occur be equal number review initial item iin where entity be plus number review where iin be if any however entity be discovered latter occurrence entity through discover use
coefficient be if be entity otherwise configure custom value default be weight contribution discovered entity ranking obtain value between grammar%23NPR normalize maximum occurrence entity j belong candidate recommendation set cr
occur iin max j cr occur j iin second ranking function also consider grammar%23NPR measure between discover entity entity through discover avoid assign same value all entity discover through same entity as grammar%23NPR do as grammar%23NPR entity be discovered
coefficient be if be entity otherwise coefficient be discovered entity otherwise grammar%23NPR return number between be equal grammar%23NPR entity while discovered entity be average grammar%23NPR grammar%23NPR io where io be entity through discover
grammar%23NPR grammar%23NPR io third ranking function consider grammar%23NPR measure between entity initial item iin coefficient set custom value allow ranker weight differently contribution occurrence review give by grammar%23NPR through grammar%23NPR measure
grammar%23NPR grammar%23NPR iin grammar%23NPR measure between discovered entity entity through discover need be precomputed at discovery time see section enable grammar%23NPR exploit grammar%23NPR
measure between entity cr initial item need compute while ranking ranking time be increased
evaluation procedure evaluate performance grammar%23NPR two offline experiment conduct movie book music domain purpose first experiment be understand impact ranking function discovery occurrence threshold coefficient grammar%23NPR furthermore perform first experiment two time first rely on then on grammar%23NPR assess effect exploited knowledge+base on quality recommended item this experiment result describe technical report aim second experiment be compare our proposal traditional recommendation technique technique rely on rating state-of-the-art recommender system base on
conduct both experiment obtain from grammar%23NPR grammar%23NPR user review all item include grammar%23NPR grammar%23NPR grammar%23NPR dataset user rating item such rating dataset map corresponding entity available rely on work grammar%23NPH et+al. 5-fold crossvalidation execute exploit lists grammar%23NPR recommendation each user compute measure precision recall grammar%23NPR diversity
implementation rely on compute measure accord all unrated item protocol create top- n recommendation list each user by predict score every item not rate by that particular user whether that item appear user test set not rate item consider be irrelevant user explain low value measure as quality recommendation tend underestimate however grammar%23NPH suggest rely on this protocol rather than test item include only test item top- n list as user satisfaction top- n recommendation depend on ranking all item
evaluation result compare our technique most grammar%23NPR algorithm as implement grammar%23NPR based recommender set neighborhood size while use factor bpr as do by grammar%23NPH et+al. configure grammar%23NPR exploit grammar%23NPR as ranking method follow graph same properties select our algorithm
grammar%23NPH grammar%23NPH list result obtain movie book music domain respectively good value highlight bold grammar%23NPR report both configuration good trade-off among various measure good score achieve each measure optimization extensively describe technical report all experimental trail grammar%23NPR provide good diversity good accuracy both rate prediction rank than grammar%23NPR while improve novelty with+respect+to traditional technique bpr account high precision recall grammar%23NPR general diversity algorithm be rather low movie while music book be above apart
discussion grammar%23NPR show good diversity all domain notably sparse dataset book achieve precision recall grammar%23NPR comparable much high diversity although be content based method however collaborative filtering technique be know suffer less overspecilization problem grammar%23NPR grouplens.org%2Fdatasets%2Fmovielens%2F1m grammar%23NPR www.macle.nl%2Ftud%2Flt grammar%23NPR grammar%23NPR dataset lastfm grammar%23NPR www.librec.net grammar%23NPR value highlight same measure if difference among be not statistically significant case east+by+north diversity when be best also highlight second+best because precision recall grammar%23NPR be close zero mean that recommendation provide be completely unrelated their novelty diversity be not cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH comparison use dataset algorithm precis recall grammar%23NPR east+by+north diver
good score grammar%23NPR bpr grammar%23NPR grammar%23NPR grammar%23NPH comparison use grammar%23NPR dataset algorithm precis recall grammar%23NPR east+by+north diver
good score grammar%23NPR bpr grammar%23NPR grammar%23NPR grammar%23NPH comparison use grammar%23NPR dataset algorithm precis recall grammar%23NPR east+by+north diver
good score grammar%23NPR bpr grammar%23NPR grammar%23NPR provide good rating prediction rank than content based as grammar%23NPR this reason although collaborative filtering be very popular decide include baseline only one technique among many i. e. bpr bpr be new most promise nevertheless show low diversity than our algorithm not also account good rating prediction ranking
approach also provide high novelty than traditional technique good rating prediction rank than grammar%23NPR
movie domain grammar%23NPR account good novelty while music book second+best result close grammar%23NPR additionally when optimize this measure grammar%23NPR have similar book high music rate prediction rank than grammar%23NPR on+the+contrary when former optimize rate prediction rank prefer latter increase novelty recommendation but also limit loss rate prediction rank additionally grammar%23NPR evaluate consider recommendation generate all previous movie user like since generation approach be rather na%C3%AFve take+into+account only initial item combine machine+learning technique significantly improve performance but further experiment require prove
conclusion grammar%23NPR propose grammar%23NPR novel approach base on semantic annotation user review evaluate movie book music domain grammar%23NPR show good diversity improve rate prediction ranking compare another method base on while increase novelty recommendation with+respect+to traditional technique
review available book music domain seem contain small amount useful information result offline study suggest that our algorithm provide more diverse recommendation reach interesting compromise between accuracy novelty suggested item
this work support by european+union horizon grammar%23ENT programme grant agreement
grammar%23NPH grammar%23NPH study heterogeneity recommendation social music service grammar%23NPR association+for+computing+machinery
grammar%23NPH recommender system base on user review state+of+the+art user grammar%23NPH grammar%23ENT 99+154 https%3A%2F%2Fdoi.org%2F10.1007%2Fs11257-015-9155-5 grammar%23NPR grammar%23NPH grammar%23NPH improve efficiency accuracy multilingual entity extraction i-semantics
https%3A%2F%2Fdoi.org%2F10.1145%2F2506182.2506198 grammar%23NPH grammar%23NPH systematic literature review based recommender system grammar%23NPR computation practice experience grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1002%2Fcpe.3449 grammar%23NPH comparison knowledge extraction tool semantic+web semantic+web semantics big data grammar%23NPR grammar%23ENT springer grammar%23NPH 351+366 https%3A%2F%2Fdoi.org%2F10.1007%2F978-3-642-38288-8_24 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
robust disambiguation named entity text grammar%23NPR grammar%23ENT 782+792 dl.acm.org%2Fcitation.cfm%3Fid%3D2145432.2145521 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH semantics aware graph based recommender system linked open data grammar%23NPR association+for+computing+machinery 229+237 https%3A%2F%2Fdoi.org%2F10.1145%2F2930238
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
grammar%23NPR semantic path grammar%23NPR ranking association+for+computing+machinery trans. grammar%23NPR syst. technol. grammar%23ENT grammar%23ENT http doi.org%2F10.1145%2F2899005 grammar%23NPH dbrec music
semantic+web grammar%23NPR grammar%23ENT vol. springer grammar%23NPH 209+224
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
bpr from implicit feedback proceedings twenty-fifth conference on uncertainty artificial+intelligence 452+461 dl.acm.org%2Fcitation.cfm%3Fid%3D1795114.1795167 grammar%23NPH grammar%23NPH unlock semantics multimedia presentation multimedia metadata ontology association+for+computing+machinery 831+840 https%3A%2F%2Fdoi.org%2F10.1145%2F1772690.1772775 grammar%23NPH evaluation recommendation rating prediction ranking proceedings 7th on recommender system grammar%23NPR association+for+computing+machinery 213+220 https%3A%2F%2Fdoi.org%2F10.1145%2F2507157.2507160 grammar%23NPH grammar%23NPH grammar%23NPH content recommendation through semantic annotation user review extended technical report grammar%23ENT https%3A%2F%2Farxiv.org%2Fabs%2F1709.09973 mi grammar%23NPR grammar%23NPH avoid monotony improve diversity recommendation grammar%23NPR grammar%23NPR grammar%23ENT on recommender system grammar%23NPR association+for+computing+machinery 123+130 https%3A%2F%2Fdoi.org%2F10.1145%2F1454008.1454030k-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
plasmatica com presa emezzo e due ore
show discovery growth grammar%23NPR corpus all language use synthetic user define utilize grammar%23NPR
one seed only every language specifically use drug irbesartan behavior glimpse be homogeneous every language similar concept growth at each iteration average grammar%23NPR correlation amongst result all language be above
multilingual drug extraction synthetic user build synthetic user by crawl relevant all target+language make+sure cover same drug all language particularly use two biggest cross domain legion+of+doom dataset grammar%23NPR select all entity type dbo grammar%23NPR from all entity type wikidata grammar%23NPR from grammar%23NPR selected entity retrieve corresponding label english german spanish italian consider our gold+standard dictionary then select 20%25 this gold+standard as seed from language measure performance recreate remaining 80%25 by use grammar%23NPR perform 5-fold cross validation without repetition randomly select 20%25 seed at each iteration make+sure that seed represent same drug all language test if choice initial seed impact result figure show that algorithm have same behavior independently selection seed respectively use grammar%23NPR as gs four language figure show average 5-fold experiment each language single plot discovery growth be comparable all language correlation always above
drug extraction human-in-the-loop grammar%23NPR quantify benefit use grammar%23NPR enrich i.e.
existing linked dataset how+much can add by run grammar%23NPR over relevant corpus extract new term use as seed random 20%25 available same+as do previous experiment run grammar%23NPR on english document grammar%23NPR involve medical doctor as human-in-the-loop only use english as be native+language subject expert synthetic user test grammar%23NPR against already available therefore drug drug be not already there not get count as correct human adjudicate as correct not only evaluate true performance grammar%23NPR but also quantify portion new terms that be able add figure grammar%23NPR show discovery growth iteration only take grammar%23ENT obtain dictionary times bigger than initial seed use dictionary produce this experiment as gold+standard closely approximate recall both glimpse grammar%23NPR
grammar%23NPR show comparison two method glimpse start manually provide seed grammar%23NPR seed iteration grammar%23NPR cover same instance instance take more+than iteration glimpse
use abbreviation dbo dbpedia.org%2Fontology andwikidata www.wikidata.org%2Fentity
check produced terms against all item already available by use our method expand achieve extended coverage 8.16%25 on 0.36%25 on grammar%23NPR although figure grammar%23NPR seem small grammar%23NPR already contain different lexicalisation drug yet our subject expert identify some relevant drug drug class corpus specifically be not yet
building color dictionary from twitter data use same methodology as before collect seed from select all entity type dbo colour from all entity typewikidata selected entity retrieve corresponding label english german spanish italian run experiment native+speaker
experiment run simultaneously all participant same room so that if have doubt concern on accept reject item discuss other participant user instruct stop iteration interest notice that despite seed grammar%23NPR all available color on grammar%23NPR be still able find additional color e.g. azulgrana spanish as+well+as capture occurrence shortened lexicalisation certain color such as limn in+place+of color l%C3%ADmon total find new color english spanish german italian respectively then employ human created dictionary as gold+standard perform further synthetic test start one seed color red all language as use manually created gold+standard be equivalent re run test user all language number iteration require get same size lexicon be at+least with+respect+to start seed
comparison state+of+the+art this section compare our approach grammar%23NPH grammar%23NPH because+of strict api limit be not able compare grammar%23NPH grammar%23NPH on grammar%23NPR dataset but only on twitter dataset more precisely compare number identified lexicon terms use grammar%23NPR synthetic user glimpse human-in-the-loop result show grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPR grammar%23NPR twitter dataset respectively both case all language grammar%23NPR human-in-the-loop be able identify high number lexicon terms furthermore when look+at language other than english grammar%23NPR outperform on both dataset argue that be important result as information extraction method especially when target user-generated data traditionally focus on english while adaptation new language still remain open issue be worth noting that standard grammar%23NPH tool fail arbitrary type e.g. color grammar%23NPH try link word external knowledge identify concept such as grammar%23NPR grammar%23NPR etc. etc. particular task identify instance color not fit
conclusion grammar%23NPR world be inherently multicultural multilingual place where different country region develop their own grammar%23NPR lingual concept extraction grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPR r b grammar%23NPR grammar%23NPR r figure comparison discovery growth grammar%23NPR different language on grammar%23NPR dataset use seed from figure grammar%23NPR grammar%23NPR figure grammar%23NPR grammar%23NPR correlation r amongst result from different language report
discovery growth grammar%23NPR b recall glimpse versus grammar%23NPR
experiment subject+matter expert physician figure grammar%23NPR show discovery growth grammar%23NPR seed figure grammar%23NPR show comparison recall between glimpse completely manually seed grammar%23NPR seed
social concept fashion language etc. at astounding rate develop semantic asset dicionaries taxonomy etc. language allow detection interconnection concept between culture open door even more rapid discovery even within single language cultural technological terms develop evolve staggeringly quickly as+usual be very new terms terms be most interest inclusion semantic asset linked data resource this paper address challenge by propose solution discover new instance specific ontology concept be independent language ii domain iii text style our algorithm be iterative purely statistical hence not require any feature+extraction feature+extraction be difficult expensive different language text organically incorporate human feedback improve accuracy control concept drift at every iteration cycle additionally be able run many experiment require quantify this due usage oracle determine growth+rate add instance automatically show extremely similar discovery growth over extract drug name on four language over parallel corpus medical text minimal variation from initial seed show similar efficacy second entity type color over parallel microblogging corpus show that exploit bootstrapping phase be able maintain same comparable performance all language speed+up hurdle initial iteration lastly due tight integration human-in-the-loop very high+quality instance grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.

248+822+352 e lexicon size on grammar%23NPR dataset
en e b lexicon size on dataset
comparison produced lexicon size drug extraction task on grammar%23NPR dataset table grammar%23NPR n dataset table grammar%23NPR when use grammar%23NPR either synthetic user orwith true grammar%23NPR grammar%23NPR spotlight
form develop by our technique include rapidly directly back into linked data
work include use multiple semantic resource improve performance e.g. identify employ multiple drug concepts in+parallel additionally obvious next step be cross lingual alignment discovered instance be critical aspect current effort towards multilingual semantic+web this alignment benefit from rich set context develop by glimpse be important area future interest
although already develop test user+interface facilitate human feedback plan on improve
grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH
grammar%23NPR grammar%23ENT named entity recognition link italian tweet neel-it task grammar%23NPR workshop proceeding vol. grammar%23NPR
grammar%23NPH md grammar%23NPH grammar%23NPH grammar%23NPH text mining vigilance using machine+learning drug name recognition drugdrug interaction extraction classification grammar%23ENT 122+132 https%3A%2F%2Fdoi.org%2F10.1016%2Fj.jbi.2015.09.015 grammar%23NPH grammar%23NPH use reduce data sparseness pattern grammar%23NPR information extraction grammar%23NPR springer
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH method accelerate human grammar%23ENT 237+245 https%3A%2F%2Fdoi.org%2F10.1137%2F1.9781611974973.27 grammar%23NPR epubs.siam.org%2Fdoi%2Fpdf%2F10.1137%2F1.9781611974973.27 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
spot drug unsupervised pattern match method extract drug name from very large clinical corpus grammar%23NPR grammar%23ENT http doi.org%2F10.1109%2Fhisb.2012.16 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH analysis named entity recognition link tweet information+processing management grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH program-tasks data evaluation grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH extract enterprise vocabulary using linked open data semantic+web grammar%23NPR grammar%23ENT grammar%23ENT 779+794
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH semantic+web machine reading fred semantic+web grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH
wrapper induction using linked data grammar%23NPR grammar%23NPR
https%3A%2F%2Fdoi.org%2F10.1145%2F2479832.2479845 grammar%23NPH grammar%23NPH grammar%23NPH induce from unlabeled corpus proceedings grammar%23ENT conference on empirical method natural+language+processing 595+605 https%3A%2F%2Faclweb.org%2Fanthology%2Fd16-1057 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
modern japanese color lexicon grammar%23NPR grammar%23ENT http doi.org%2F10.1167%2F17.3.1 grammar%23NPH grammar%23NPH grammar%23NPH adverse drug event detection tweet semi- supervised convolutional neural+network proceedings 26th on international republic canton 705+714 https%3A%2F%2Fdoi.org%2F10.1145%2F3038912.3052671 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR uren grammar%23NPR grammar%23NPR large-scale multilingual knowledge+base grammar%23NPR from semantic+web journal grammar%23ENT
grammar%23NPR effect semantic+feature onmachine learn based drug name recognition system word embeddings versus manually construct dictionary information grammar%23ENT 848+865 https%3A%2F%2Fdoi.org%2F10.3390%2Finfo6040848 grammar%23NPH grammar%23NPH grammar%23NPH
spotlight shed light on document i-semantics

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH from linked data+source grammar%23NPR springer
grammar%23NPH grammar%23NPH grammar%23NPH entity linking meet word+sense disambiguation unified approach grammar%23NPR grammar%23ENT 231+244
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH open knowledge extraction challenge semantic+web evaluation challenge springer
grammar%23NPH grammar%23NPH grammar%23NPH
lightweight multilingual entity extraction linking grammar%23NPR
365+374 https%3A%2F%2Fdoi.org%2F10.1145%2F3018661.3018724 grammar%23NPH grammar%23NPH grammar%23NPH
terminology extraction analysis linguistic statistical approach grammar%23ENT grammar%23ENT 255+279 https%3A%2F%2Fdoi.org%2F10
grammar%23NPH grammar%23NPH grammar%23NPH use learning grammar%23NPR grammar%23ENT grammar%23ENT
grammar%23NPH roman kern grammar%23NPH information+retrieval based approach grammar%23NPR grammar%23NPR grammar%23NPR springer 433+439
grammar%23NPH grammar%23NPH learning dictionary information extraction by multi-level bootstrapping grammar%23NPR grammar%23NPR 474+479 http id grammar%23NPH mine linked data rapidminer grammar%23ENT 142+151
grammar%23NPH semantic+web data+mining knowledge discovery comprehensive survey grammar%23ENT
automatic bilingual lexicon acquisition use random indexing parallel corpus grammar%23NPH grammar%23ENT 327+341 https%3A%2F%2Fdoi.org%2F10.1017%2Fs1351324905003876 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR task extraction from biomedical text grammar%23NPR grammar%23ENT grammar%23NPR grammar%23ENT 341+350 www.aclweb.org%2Fanthology grammar%23NPR automatic ontology population machine+learning technique from semi- structured document
grammar%23NPR institute+of+electrical+and+electronics+engineers 534+539
j grammar%23NPR news grammar%23NPR collection multilingual parallel corpus tool interface grammar%23NPR grammar%23ENT books.google.com%2Fbooks%3Fhl%3Den grammar%23NPH introduction language independent named entity recognition grammar%23NPR 142+147 https%3A%2F%2Fdoi.org%2F10.3115%2F1119176.1119195 grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH german grammar%23NPR et+al. grammar%23ENT newsreader using knowledge resource crosslingual reading machine generate more knowledge from massive stream news knowledge-based+system grammar%23ENT
grammar%23NPH grammar%23NPR free collaborative knowledgebase grammar%23NPR association+for+computing+machinery grammar%23ENT knowledge semantically typed relational pattern kuldeep.singh%40iais.fraunhofer.de grammar%23NPH mulang%40iai.uni bonn.de lytra%40cs.uni bonn.de yaser.jaradeh%40uni-bonn.de ahmad.sakor%40uni-bonn.de grammar%23NPH vidal%40cs.uni bonn.de langec%40cs.uni bonn.de soeren.auer%40tib.eu abstract transform natural+language question into formal query be integral task question+answering grammar%23NPR system grammar%23NPR system build on knowledge graph like grammar%23NPR require step natural+language+processing link words specifically including named entity relation their corresponding entity knowledge graph achieve this task several approach rely on background knowledge+base contain semantically type relation e.g. grammar%23NPH extra disambiguation step two major factor affect performance relation link approach whenever background knowledge+base access limited availability such semantic knowledge source b lack systematic approach on how maximize benefit collected knowledge tackle this problem devise grammar%23NPR semantic base index able capture knowledge encode on background knowledge+base like grammar%23NPH grammar%23NPR represent background knowledge+base as bi partite dynamic index over relation pattern include knowledge+base moreover develop relation link component able exploit grammar%23NPR feature benefit grammar%23NPR empirically study on existing grammar%23NPR benchmark observe result suggest that grammar%23NPR be able enhance accuracy relation link+up by three times
computing methodology natural+language+processing knowledge+representation reasoning relation linking knowledge graph knowledge capture permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright third-party component this work honor
all use contact owner author s
grammar%23ENT grammar%23ENT 6th grammar%23ENT united+states+of+america grammar%23ENT copyright hold by owner author s
isbn 123+4567+24+567+08+06 grammar%23ENT https%3A%2F%2Fdoi.org%2F10.475%2F123_4 introduction last decade see explosion data publicly available on form knowledge graph kg e.g. grammar%23NPR freebase although knowledge graph provide access structural representation unstructured knowledge be still challenge capture exploit this knowledge natural+language+processing nlp for+example question+answering grammar%23NPR system build on top knowledge graph be nlp system empower ability translate natural+language question equivalent formal query against knowledge graph task formal query generation further view+as pipeline consist several sub- task namely relation extraction link query generation research show that formal query formulation from natural+language question more specifically link relations kg properties often require extra knowledge source source contain semantic description extension underlying knowledge graph knowledge+base kb capture knowledge from large corpus taxonomy e.g. grammar%23NPR grammar%23NPH boa pattern library allow+for enhance accuracy process map natural+language relations concept specific knowledge graph hence extract knowledge from such background knowledge+base also improve effectiveness relation link task increase overall performance grammar%23NPR system
idea provide semantically type pattern against properties knowledge graph be special feature for+example patty be large knowledge+base consist semantically type relational pattern their associated properties open domain knowledge graph such as grammar%23NPR therefore grammar%23NPH provide rich source relational pattern use during relation linking grammar%23NPR et+al. report that grammar%23NPH allow flexible mapping natural+language relations their kg properties however this flexibility imply that one relation match several pattern for+example natural+language relational pattern play appear times grammar%23NPH associate relations hence efficient method need capture grammar%23NPR wordnet.princeton.edu%2Fdbpedia relation grammar%23NPH pattern birthplace be+born adj hometown s homecountry grammar%23NPR be+born be+born grow+up die det town spouse be+born also married det age num married parent be+born s son adj s daughter con predecessor be+born s son adj con father relation be+born be+born det son pro pattern pattern pattern pattern pattern pattern excerpt patty knowledge+base where grammar%23NPH be+born candidate grammar%23NPR relation birthplace grammar%23NPR spouse parent predecessor relation match be+born b candidate grammar%23NPR relation select x where dbr grammar%23NPH dbo birthplace x select x where dbr grammar%23NPH dbo grammar%23NPR x select x where dbr grammar%23NPH dbo spouse x select x where dbr grammar%23NPH dbo parent x select x where dbr grammar%23NPH dbo predecessor x em pt y re su lts w ro ng re su lts dbr dbr grammar%23NPR dbr grammar%23NPR select x where dbr grammar%23NPH dbo relation x c potential grammar%23NPR query figure motivating example patty knowledge+base use identify grammar%23NPR predicate associate pattern from given question grammar%23NPR query build from grammar%23NPR predicate excerpt patty knowledge+base b grammar%23NPR predicate grammar%23NPH associate pattern be+born question c potential grammar%23NPR query answer input question only grammar%23NPR predicate dbo birthplace allow+for collect correct answer
from large corpus like grammar%23NPH exploit their feature grammar%23NPR system this work devise approach capture knowledge from collection semantically typed relational pattern like grammar%23NPH further present relation link method able exploit feature first grammar%23NPR semantic index based representation knowledge+base propose grammar%23NPR provide search mechanism accurately link relational pattern semantic type benefit grammar%23NPR empirically evaluate on existing grammar%23NPR benchmark result suggest that grammar%23NPR enhance performance relation link method by up three times
remainder article be structured as+follows section motivate our work example section elaborate on specific problem capture knowledge from semantically typed knowledge+base follow by detailed illustration approach proposed solution section present experimental study evaluation our approach related work present section finally conclusion future plan outline section
motivating example motivate our work by analyze problem extract knowledge from large knowledge+base corpus during relation predicate link task grammar%23NPR system patty be one such large knowledge+base semantically type relational pattern contain 127%2C811 pair relational phrase grammar%23NPR predicate involve grammar%23NPR relation total many grammar%23NPR system such as grammar%23NPR grammar%23NPR use grammar%23NPH relational phrase match word pattern input question find corresponding grammar%23NPR relation as part understand natural+language question
consider following question where grammar%23NPH be+born part understanding this natural+language question include extraction named entity identification predicate s successful completion task allow grammar%23NPR system construct formal query e.g. grammar%23NPR query in+order+to retrieve answer from knowledge graph like grammar%23NPR first task name entity recognition disambiguation tool such as grammar%23NPR spotlight use identify grammar%23NPH entity disambiguate grammar%23NPR mention dbpedia.org%2Fresource%2Falbert_einstein second task patty knowledge+base use link phrase pattern question such as be+born associated grammar%23NPR predicate i.e. relation our exemplary question pattern be+born map six different grammar%23NPR predicate grammar%23NPH corpus namely dbo birthplace dbo grammar%23NPR dbo spouse dbo parent dbo relation dbo predecessor
relation link several textual pattern e.g. dbo birthplace be related more than 6%2C000 pattern grammar%23NPH corpus often share among different relations as illustrate figure grammar%23ENT for+example grammar%23NPH corpus grammar%23NPR lingual concept extraction linked data human-in-the-loop grammar%23NPH research united+states+of+america aalba%40us.ibm.com new+york united+states+of+america anni%40us.ibm.com grammar%23NPH research united+states+of+america annalisa.gentile%40ibm.com grammar%23NPH research united+states+of+america dgruhl%40us.ibm.com united+states+of+america petar.ristoski%40ibm.com grammar%23NPH research united+states+of+america welchs%40us.ibm.com abstract ontology be dynamic artifact evolve structure content keep up-to-date be very expensive critical operation any application rely on semantic+web technology this paper focus on evolve content ontology by extract relevant instance ontological concept from text
propose novel technique be completely language independent ii combine statistical method human-in-theloop iii exploit linked data as bootstrapping source our experiment on publicly available medical corpus on dataset show that proposed solution achieve comparable performance regardless+of language domain style text give that method rely on human-in-the-loop our result safely feed+back into linked data resource
computing methodology information extraction association+for+computing+machinery reference format grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH multi lingual concept extraction linked data human-in-the-loop proceeding grammar%23NPR grammar%23ENT knowledge grammar%23NPR grammar%23ENT association+for+computing+machinery new+york new+york united+states+of+america https%3A%2F%2Fdoi
introduction by their very nature real+world ontology be dynamic artifact ontology evolution pose major challenge all application rely on semantic+web technology ontology evolve their structure datamodel their content instance keep up-to-date be quite expensive this paper focus on computer human partnership more rapidly evolve content ontology through extraction new relevant concept from text
atomic+operation behind this population step be discovery all instance belong each concept plethora solution propose extract relevant terminology dictionary permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
isbn grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148021 from both unstructured text semi- structured content need constantly update ontology dictionary terminology well know as motivating example online+shop integrate new product description provide by vendor on daily basis feature vocabulary use describe product continuously evolve different vendor provide product description varied writing+style standard despite difference fully integrate new product e.g. be able provide meaningful comparison shopping grid merchant correctly identify assign equivalence instance
another example consider medical surveillance medical+report need scan clinical issue e.g. adverse drug reaction cause by prescription+drug new+drug constantly approve available on+the+market therefore use obsolete drug dictionary identify miss all new products be coincidentally most important surveil another source valuable information terms vigilance be user-generated content for+example focus online+community such as grammar%23NPR grammar%23NPR general like where drug symptom reaction express many varied way by different user evolution dictionary not confine products other naturally growing set even concepts assume as simple stable for+example color name constantly evolve way color name change different language be quite dissimilar give+in cultural difference how express different country for+instance new color name mizu recently propose addition list japanese basic color terms on more practical level capture right instance concept also be highly task dependent as our user learn during experiment discover space gray matte black jet+black be all relevant color mobile+phone while white+chocolate amber rose be color wall+paint products
task ontology population from text extensively address previous research suggest that many task fully automated approach be not effective mainly due negative effect semantic drift moreover majority information extraction technique be language dependent i.e. rely on natural+language+processing nlp operation tool tool be language specific such as parse part+of+speech tagging etc.
www.askapatient.com%2Fk-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
propose grammar%23NPR novel solution solution build upon our previous work revolve around three main aspect be statistical method extract concept instance base on context pattern ii rely on human feedback on item automatically tune score threshold extraction pattern iii use when available even small quantity bootstrap process
contribution this work be threefold first show that performance original glimpse algorithm be language independent do so design novel testing strategy strategy systematically simulate human-in-the-loop exploit parallel corpus demonstrate that glimpse have similar performance all language second show that grammar%23NPR glimpse exploit bootstrapping phase maintain same comparable performance all language ii be robust with+respect+to choice seed iii reduce number required iteration by at+least half while subject+matter expert still provide initial seed s use suggest further candidate user speed+up hurdle initial iteration show that grammar%23NPR significantly speed+up concept discovery process
demonstrate that grammar%23NPR be robust with+respect+to different text style specifically prove efficacy extract color name from dataset multiple language show that extract additional color terms not already available on prove that despite richness be always new data extract from unstructured text
advantage method be that be fully independent with+respect+to language domain style corpus human-inthe-loop have power drive extraction concept towards their semantic interpretation thus ensure domain membership concept promptly discard pertinent member avoid propagation erroneous extraction pattern while serve as powerful bootstrapping tool
prove robustness method synthetic experiment on parallel corpus also test efficacy real user experiment prove that subject+matter expert produce quality dictionary different semantic class with+respect+to available state+of+the+art propose method strong partnership between human machine consider human integral part learning process power steer semantic drift at any point process
following explore related work section present our solution concept extraction section test extensive experiment section finally conclude lesson learn future work section
state+of+the+art be vast amount literature devote ontology population from text a+number+of established initiative foster research on topic such as knowledge+base population task at grammar%23NPR grammar%23NPR knowledge+base acceleration grammar%23NPR open knowledge extraction challenge name grammar%23NPR linkeddata.org grammar%23NPR www.nist.gov%2Ftac%2F2015%2Fkbp grammar%23NPR trec-kba.org initiative system compare on basis individual belong few selected ontology class span from common+person place organization more specific class such as facility weapon vehicle role drug among evaluation focus be usually on specific sub- task involve process such as entity recognition linking typing several solution propose literature span from general+purpose comprehensive approach more domain specific fred be established example comprehensive solution problem convert text into ontology transform text internal ontology representation then attempt align available be general+purpose machine reader mostly base on core nlp tool tool potentially process text from any domain work rely on machine+learning technique tailor algorithm certain specific domain e.g. drug method be general expensive require corpus and%2For language specific feature+extraction comprehensive overview found work rely on statistical method iteratively grow number ontology entry start seed knowledge but require grammar%23NPR pos tagging as minimum therefore bound language moreover without iterative+method easily generate semantic drift
majority available method operate assess english+language although specific initiative aim at encourage study other language argue that truly language independent method this task be not yet widespread grammar%23NPR et+al. address task building multilingual lexica but their method require aligned corpus each language lexicon be construct achieve good result english german although fail on word appear low+frequency grammar%23NPR et+al. use exploit linguistic feature such as part+of+speech tagging other morphological feature test their method english spanish chinese varying result depend on language grammar%23NPH et+al. also propose irish+national+liberation+army approach drug name extraction base on linguistic feature drawback method be that although proposed feature extract many language rely on nlp tool not guarantee out-of-the-box portability different language different domain furthermore be not clear if method apply un grammatical text
major challenge concept extraction involve deal chaotic text give importance user generate content content prove be extremely valuable source information many domain vigilance be grammar%23NPR this end grammar%23NPH et+al. propose semi- supervised model model use random stream as unlabeled training data prove successful recognition adverse drug reaction another hurdle be fact that dictionary create be highly dependent on task at+hand especially when deal positive negative word word be highly domaindependent while completely automatic technique be grammar%23NPR be recent benchmarking initiative on problem diego.asu.edu grammar%23NPR sharedtaskeval.html.multi-lingual concept extraction grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america highly appeal need be fine-tuned every new task propose approach where tuning be integral part process i.e. humanworks partnershipwith statistical method drive semantic task effectively efficaciously
last contribution our work be usage inform extraction process vastly explore many information extraction task specifically concept extraction et+al. exploit type information find their statistical system promise result when apply english grammatically correct corpus grammar%23NPR et+al. exploit frommultiple source populate target ontology their system retrieve appropriate instance insert into target ontology step map model property applicability model other language not explore paper
method require that all instance be already present as whereas our proposed method use bootstrap process collect more data from unstructured text similar bootstrapping technique use extract gazetteer from semi- structured content difference that structural web+page information exploit process
this work tackle three specific aspect concept extraction multi-linguality usage inform extraction organic integration show that our algorithm be truly language independent perform similar accuracy all language as+well+as be robust on grammatical text different semantic class best our knowledge our work seem be first approach expose such language independence
concept extraction glimpse approach glimpse be statistical algorithm dictionary extraction base on spot fast underlying matching engine input be large text corpus content be relevant domain concept extract for+example choose corpus medical document find drug general corpus e.g. content from find color besides corpus glimpse need one ormore example seed concept instance extract start from evaluate context set word surround item seed occur identify good context
score retrospectively terms how many good result generate all context keep score over given clip level candidate appear most good context provide first more detail on scoring function find good context use identify further term phrase corpus present human as new concept candidate one accept by subject expert be add dictionary ii use as additional seed next iteration algorithm also learn from rejected item their context down vote as not good steps find new good context additional candidate terms repeat until some threshold i.e. saturation achieve and%2For no new term find
model be quite conducive many scenario however be expensive evaluate operation algorithm many different language semantic context type text as+many gold+standard corpus need create towards this end develop automatic evaluation method method show discovery growth dictionary item as function number iteration be good approximation real+world problem find concept surface form real scenario where no gold+standard be available but correctness extraction assure by human-in-the-loop ratio new correct term add at each iteration be useful indication performance large enough corpus e.g. be not not be complete gold+standard as language always evolve best+bet be rapidly grow dictionary capture new terms as emerge
this work introduce methodology automatically test that glimpse be independent from language style corpus as+well+as from desired concept extraction type reuse concept synthetic user introduce evaluate performance synthetic user know all answer i.e. all item belong target concept but not share priori clearly concept instance instance be not corpus not discover define synthetic user utilize established dictionary domain determine subset have mention corpus oracle algorithm then work as before however be not human human accept reject propose candidate but synthetic user user only accept term oracle
multilingual experiment further describe section require that each item multilanguage oracle have fairly similar frequency target corpus
consequence not include some otherwise correct terms therefore some good context reject despite be semantically correct however since corpus be parallel this penalty be language independent our experiment show that concept discovery growth use glimpse be nearly identical all language
present grammar%23NPR use knowledge from bootstrap algorithm seed grammar%23NPR initial seed term same type as target concept from
number initial relevant seed term significantly improve effectiveness efficiency algorithm i.e. allow extract high number term corpus fewer human iteration be worth noting retrieve relevant seed from perform several way start handful user-defined seed search similar item let user explore ontology model etc. as show be many tool approach approach use link string terms given dataset such as tool pattern base label based approach
as show be rich multilingual semantic resource resource exploit multilingual setting grammar%23NPR however deep discussion on this topic be outside scope this paper purpose our experiment identify set relevant type query their instance extract their label user define language s use as seed cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
experiment run series experiment aim at test different contribution this work first test language independent capability glimpse use parallel corpus medical domain task create dictionary drug then test performance grammar%23NPR on same parallel corpus use seed discovery instead manual seed evaluate how fast converge desired dictionary considered grammar%23NPR finally perform true experiment quantify give concept how many new instance discover i.e. how grammar%23NPR use assist task concept dictionary expansion maintenance additionally test that grammar%23NPR be also robust independently style write this purpose use corpus tweet several language use grammar%23NPR construct dictionary color name choose color as concept as+it+is simple enough domain recruit native+speaker different language experiment all experiment evaluate performance discovery growth i.e. at each iteration quantify how many new instance add dictionary relative initial number seed
accuracy calculate presence gold+standard discovery growth be useful indication performance real scenario where no gold+standard be available but correctness extraction assure by human-in-the-loop
following give details about all dataset use section describe our experimental setting section
dataset grammar%23NPR dataset grammar%23NPR document be parallel corpus comprise pdf document from relate medicinal products their translation into official language european+union
document be sentence alignedwithin project strength grammar%23NPR corpus be that be nearly parallel corpus many language reason say nearly be that not all document be there all language however desiderata be that statistical properties terms our algorithm suppose find be very similar note some semantic class some language use two word e.g. blood+pressure english versus single word e.g. grammar%23NPR german so frequency word blood grammar%23NPR be quite different both language even when translate word+for+word select english spanish italian german portion dataset use task construct dictionary drug various language this parallel dataset select aim create clean testing environment effectively obtain gold+standard but be worth specifying that technique not bound existence such resource
dataset grammar%23NPR be most popular microblogging platform on provide huge collection grammar%23NPR target+language choose base on availability native+speaker team team participate experiment analyze result
opus.lingfil.uu.se%2Femea.php grammar%23NPR twitter.com brief user generate text update tweet give personal nature content limited size each message max character text style usually not follow strict grammatical+rule be often cryptic chaotic although majority tweet be english tweet many different language be also available build collection tweet our target+language task extract dictionary from choose color as simple concept extract collect tweet post period between first grammar%23ENT write english german spanish italian grammar%23NPR contain at+least mention color respective language use both grammar%23NPR as gold+standard lists color select tweet be create manageable size collection re-create somehow focus corpus give that some language english in+particular have tweet than make+sure that size dataset different language be balanced process tweet one day at time downsize all language chunk number tweet smallest collection randomly select tweet big language final dataset contain tweet language
gold+standard synthetic user section introduce concept synthetic user i.e. oracle act as human-in-the-loop allow exhaustive exploration impact change system without require very large human user study build several synthetic user drug extraction scenario experiment grammar%23NPR standard drug dataset ii drug collection obtain from specifically use grammar%23NPR while grammar%23NPR only build english oracle obtain oracle all target+language experiment give semantic simplicity domain perform true evaluation all language
discovery growth glimpse english en italian spanish e grammar%23NPR de start one seed drug irbesartan use grammar%23NPR as gold+standard synthetic user average correlation amongst all language r
language identify base on field
www.nlm.nih.gov%2Fresearch%2Fumls%2Frxnorm%2Fmulti-lingual concept extraction grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPR english r b grammar%23NPR german r c grammar%23NPR spanish r d grammar%23NPR italian r figure discovery growth grammar%23NPR 5-fold cross validation on grammar%23NPR dataset use as seed each plot grammar%23NPR grammar%23NPR german grammar%23NPR spanish grammar%23NPR italian show discovery growth randomly generated fold report grammar%23NPR correlation r amongst
grammar%23NPR english r b grammar%23NPR german r c grammar%23NPR spanish r d grammar%23NPR italian r figure discovery growth grammar%23NPR 5-fold cross validation on grammar%23NPR dataset use grammar%23NPR as seed each plot grammar%23NPR grammar%23NPR german grammar%23NPR spanish grammar%23NPR italian show discovery growth randomly generated fold report grammar%23NPR correlation r amongst
experimental setting multilingual drug extraction synthetic user use grammar%23NPR corpus run glimpse synthetic user as synthetic user this experiment build gold+standard dataset drug use grammar%23NPR resource providing name clinical drug link name drug vocabulary commonly use pharmacy management drug interaction software as grammar%23NPR be dataset english from full list drug select only appear grammar%23NPR all four selected language amount terms be same have same distribution all language but of+course their occurrence pattern be language dependent
following example use two parallel sentence from grammar%23NPR corpus english italian illustrate occurrence term pravastatin two different language
target term be context be highly language dependent plasma elimination half-life oral pravastatin be grammar%23ENT acknowledgment this project receive fund from european+union horizon grammar%23ENT r programme grammar%23NPH action grammar%23NPR ga no ga no
www.okbqa.org grammar%23ENT
christian grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH nucleus open data grammar%23NPR grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH freebase shared database structured general human knowledge grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR ecosystem get new insight by compose question+answering pipeline grammar%23NPR
sourish grammar%23NPR grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR framework natural+language+query formalization grammar%23NPR grammar%23NPR
grammar%23NPH extract multilingual natural+language pattern rdf predicate grammar%23NPR
grammar%23NPH grammar%23NPH
question+answering system over linked data use relational pattern
grammar%23NPR grammar%23NPR
grammar%23NPH grammar%23NPH casia%40 grammar%23NPR based question+answering system over linked data clef working note
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH survey on challenge question+answering semantic+web semantic+web grammar%23ENT
dynamic bit grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH
spotlight shed light on document i-semantics
grammar%23NPH grammar%23NPH match natural+language relation knowledge graph properties question+answering
semantics grammar%23ENT appear
grammar%23NPR grammar%23NPH grammar%23NPH taxonomy relational pattern semantic type grammar%23NPR grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH glove global vector word representation grammar%23NPR
grammar%23NPH grammar%23NPH
message driven vocabulary promote interoperability question+answering system grammar%23NPR
grammar%23NPH a. c grammar%23NPH grammar%23NPH grammar%23NPH graph grammar%23NPR disambiguation named entity using linked data grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH hybrid question+answering over linked data grammar%23NPR
xser%40 grammar%23NPR answer natural+language question via phrasal semantic parsing grammar%23ENT
patient behavior from natural+language united+kingdom united+kingdom grammar%23NPH university united+kingdom grammar%23NPR grammar%23NPR training+and+enterprise+council united+kingdom grammar%23NPH united+kingdom united+kingdom united+kingdom abstract visualization procedural knowledge from textual document use 3d+animation be way improve understanding interest apply this approach document relate patient education bariatric surgery domain challenging textual document describe behavior recommendation recommendation contain few procedural step leave much commonsense knowledge unspecified this work look+at how automatically capture knowledge from range differently recommendation use implicit knowledge about compliance violation such that recommendation visualize use 3d+animation our solution be end-to-end system automate this process via analysis input recommendation uncover their conditional structure use commonsense knowledge deontic+logic generate compliance violation rule mapping this knowledge update default knowledge+base knowledge+base use generate appropriate sequence visualization this paper overview this approach demonstrate potential
concept computing methodology grammar%23NPR default reasoning belief revision planning scheduling keyword nlp default reasoning deontic modality ai planning serious games interactive narrative virtual+reality format grammar%23NPH grammar%23NPH grammar%23NPH visualization patient behavior from natural+language recommendation proceedings on knowledge capture grammar%23ENT grammar%23ENT grammar%23NPR grammar%23ENT page
permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT introduction rationale understanding instruction from complex document aim at general audience be widespread problem assist through use 3d+animation patient education document be typical example as research suggest that dissemination information patient limit by understanding difficulties be growing evidence benefit various visual representation facilitate patient education including interactive system serious games 3d+animation
this paper introduce novel approach base on automatic generation 3d+animation correspond patient education document use ai planning based narrative technique both representation generation one novelty be that start baseline representation application domain be possible single+out specific sentence e.g. from patient education leaflet automatically analyze so that corresponding knowledge emphasize generated 3d+animation
domain application be patient education bariatric surgery life changing intervention severe obesity involve permanent modification digestive+tract where patient need permanently modify their eating+habit post- surgery by following recommendation such as not drink while eat figure patient education be thus essential aspect support by various leaflet educational document in+particular motivate explore mechanism by natural+language text recommendation clarify through visualization compliant as+well+as violate behavior stem from need visualize relevant information when patient present negative instruction not drink while eat as part recommendation visualize behavior have advantage present implicit information e.g. on amount timing drink more importantly show adverse consequence violation because not comply recommended behavior bariatric surgery result severe discomfort be essential that patient understand recommendation
paper overview fully implemented system system generate virtual narrative narrative illustrate range different eating+habit bariatric surgery system integrate multiple ai technique including natural+language+processing nlp commonsense reasoning ai planning based narrative technology into visualization engine paper give overview grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT et+al.
system overview analysis input grammar%23NPR recommendation extract desired behavior b use common+sense knowledge deontic+logic generate compliance violation rule c rule use generate new action extend default planning model red highlight d update planning model use visualize input recommendation either compliant scenario
component system then illustrate system behavior via end-to-end example also report on evaluation user recognition recommendation from system generated animation
previous related work there be significant interest use natural+language grammar%23NPR input generate 3d scene animation range from automatic scene creation visualization complex procedural instruction text-to-scene visualization system aim primarily at use grammar%23NPR image graphics generation at render spatial information convey text such as road+accident description other system aim at generate 3d+animation from grammar%23NPR drive virtual agent behavior from text grammar%23NPR system be dedicated visualization procedural instruction use virtual agent aim ultimately use instruction control behavior virtual human grammar%23NPR generate animation from film script including stage element follow work on virtual agent animation from grammar%23NPR
approach introduce this paper inspire by early+work grammar%23NPR et+al. but differ by take+advantage significant progress field plan based narrative generation animation main objective be clarify specific instruction namely recommendation aim at patient by modify narrative generation so that emphasize recommendation content if necessary by show happen when not comply at implementation level use so formalism use action representation as+well+as inference commonsense reasoning also use predicate grind grammar%23NPR semantics facilitate integration nlp despite limited coverage
system overview figure give overview different component our system
input be grammar%23NPR sentence from patient education document output be visualization illustrate this recommendation context typical meal be two mode operation system illustrate behavior behavior be compliant with+respect+to input recommendation ii illustrate behavior behavior violate
generation behavior our approach use ai plan formalism be because this eating+habit domain be actioncentric enable patient behavior naturally represent as planning action additionally ai planning use generate multiple different eating behavior scenario ready visualization
prototype system feature default planning domain model contain key action such as food selection put food mouth chew swallow along basic commonsense knowledge take+for+granted application domain such as basic physics food property substance size physical transformation how accumulate plan based scenario generation our system initial state create that involve selection food be appropriate recommendation e.g. soft small portion since narrative generation require complete plan produce bespoke goal use drive plan generation namely that patient complete their meal this default domain model planning problem enable generation range eating+habit scenario
key+stage system as show figure be as+follows stage input grammar%23NPR recommendation analyze produce deontic structure target action recommendation embed e.g. eat drink recommendation not drink while eat action represent as grammar%23NPR patient behavior from natural+language grammar%23ENT grammar%23ENT grammar%23ENT use feature structures processing base on cascade first layer dedicate uncover deontic structure from deontic verb imperative statement
b system generate violation this recommendation by apply set logical rule deontic structure result modification recommendation structure this stage also use default commonsense knowledge about basic physics eat when negate specific action i.e. not chew properly require default knowledge on chew speed size food bite
c output knowledge capture use update default plan domain first mapping step select most relevant planning operator each action template take+into+account specific feature add during default reasoning stage
whenever action negate either as part recommendation as part violation recommendation this negation apply relevant planning operator use rule rule modify default parameter
d finally system generate eating scenario correspond input recommendation use extended planning domain i.e. set operator include default as+well+as modify previous stage reflect recommendation
scenario generated plan then visualize use animation grammar%23NPR game engine attach planning operator emphasize selected recommendation compliance violation
worked example system illustration our system this section work through end-to-end example use following recommendation do so calm environment stage build representation text recommendation
deontic analysis recognise down eat as deontic imperative form different scope sentence along conjunction also recognize action find deontic scope remainder sentence contain properties relate action conjunction second part this stage be action recognition a+number+of fstn run against sentence detect action slot value special instance e.g.
input sentence be segmented on conjunction generate following segment do so calm environment this process be necessary action analysis issue pose this example be that be properties related eat both segment sentence but pattern pattern cause eating template instantiate only occur first segment template instantiate from trigger fstn match first segment initial action recognition template go through a+number+of processing stage this example process execute on both segment as show below segment basic template successful trigger fstn cause eat template instantiate be only concrete template instantiate at this stage
recognition this stage identify property seat attach eat template
do so calm environment basic template no template recognize this segment
recognition no action recognize this segment but since action template successfully instantiate first segment slot analysis carry+over second segment capture additional properties occur conjunction here property calm environment recognize
final post processing step this stage merge all information compute into final action template slot value normalize e.g. property sit+down transform seated
b first step extend generated template commonsense knowledge however not need this recommendation as commonsense knowledge infer by default reasoning engine be already present second step be generation compliant violation rule single compliant rule extract from action template do eat position seat environment calm violation automatically generate by implement a+number+of rule express use dynamic deontic+logic similar rule list possible shape recommendation corresponding set violation for+example shape rule some action e.g. swallow forbid be f have single corresponding violation do use set definition all possible violation generate this example be do not eat do eat not position seat environment calm do eat position seat not environment calm do eat not position seat not environment calm stage c mapping establish between action template action default planning model base on semantic relevance default action template relevance measure use approach bow word normalize use porter stemmer bow each operator contain name precondition predicate operator rank linearly scale reduce bias small bow top ranked operator select for+example planning model condition on eat seat calm environment encode operator call begin meal although template name eat added context bow lead mapping select begin meal operator
run-time one more rule select compliance violation mode new planning model generate each case this example violation mode illustrate use violation rule mark above then plan this example compliance violation generate use newly extended domain model
d output plan planning operator map script grammar%23NPR modify select animation object present environment for+example noisy environment property map onto physical device scene property such as volume level e.g. tv thus quiet environment be empty room tv off while noisy environment have multiple agent high tv volume figure show example visualization part show part plan compliant behavior seat quiet room whilst eat contrast part b show busy cap grammar%23ENT grammar%23ENT grammar%23ENT et+al.
compliance sit quiet b violation busy room figure screenshot take from visualization recommendation grammar%23NPR do so calm environment be compliant b violation room be busy
end-to-end example demonstrate ability system automate visualization input recommendation
evaluation user study conduct first usability study focus on correct recognition recommendation from visualization both compliance violation
adult participant watch video system+generation visualization postoperative patient recommendation compliance violation video generate by our system following recommendation grammar%23NPR not drink while eat grammar%23NPR do so calm environment see figure for+example visualization grammar%23NPR grammar%23NPR respectively each recommendation participant watch two short video show behavior patient in+relation+to recommendation compliant violate then answer grammar%23NPR by choose sentence out provided sentence best match whether statement correspond recommendation follow contradict video for+instance violation recommendation grammar%23NPR following choice provide chew food thoroughly ii not drink while eat iii grammar%23NPR raw fruit vegetable iv drink water between meal study finding show that grammar%23NPR majority participant 94.7%25 correctly identify recommendation comply while only 57.9%25 correctly identify recommendation violate however recommendation grammar%23NPR result be high 94.7%25 both compliance violation recommendation explain relatively low recognition violation recommendation grammar%23NPR as consequence competing element grammar%23NPR hence conduct second study
study adult participant watch visualization recommendation grammar%23NPR ask give free text response identify violation recommendation finding show figure show that majority participant 78.6%25 correctly identify violation recommendation grammar%23NPR from generated video
conclusion introduce novel approach visualization instruction recommend behavior customize figure response study participant participant ask identify violation recommendation from system generated video
compliance violation specific aspect recommendation use medical domain domain embed domain knowledge commonsense reasoning where individual recommendation have impact on trivial behavior one element this customization be presentation violate behavior detail often leave implicit textual document facilitate by fact that default knowledge embed planning domain activate infer visualize adverse consequence violation user experiment confirm correct recognition occurrence compliance violation from just 3d+animation even+though not mean display without associated text second experiment user be able infer key element recommendation successively present compliant behavior generate by system recommendation consider
grammar%23NPH grammar%23NPH grammar%23NPH author perspective model classify deontic modality event flair conference
grammar%23NPH grammar%23NPH put language based interactive manipulation object computer+graphics application institute+of+electrical+and+electronics+engineers grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH role 3d+animation periodontal patient education randomized controlled trial
grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPR automatic conversion system grammar%23NPR 28th on computer+graphics interactive technique association+for+computing+machinery 487+496
grammar%23NPH grammar%23ENT plan constraint preference grammar%23NPR language fifth international planning competition
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR automatic visualisation screenplay grammar%23ENT advance ai springer 265+272
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH christian g grammar%23NPH beryl grammar%23NPR grammar%23NPH jahanara+foundation grammar%23NPR grammar%23NPH et+al. grammar%23ENT multimedia support improve preoperative patient education randomized controlled trial use example radical prostatectomy
grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
automatic conversion traffic accident domain grammar%23NPR
19th on ai vol.
grammar%23NPH virtual human animation natural+language visualisation artificial grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH apply planning interactive storytelling narrative control use state constraint association+for+computing+machinery grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH dynamic deontic+logic complex contract grammar%23ENT
grammar%23NPH grammar%23NPH action representation virtual human agent embody conversational agent grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH serious gaming behavior change state play institute+of+electrical+and+electronics+engineers pervasive computing grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH instruction intention expectation artificial+intelligence grammar%23ENT 253+269 grammar%23NPR automatic rdf key ranking data linking grammar%23NPR grammar%23ENT washington+dc district+of+columbia united+states+of+america figure key ranking support example
grammar%23NPR axiom grammar%23NPR ce grammar%23NPR opem grammar%23NPR dpen state name instance class expression ce uniquely identify by object property expression opei data property expression dpej if two named instance class expression coincide on value florida+keys property then individual be
department+of+social+security directorate+of+territorial+surveillance denote source target rdf dataset respectively describe by ontology o instance class c grammar%23NPR denote p set their common property begin by provide novel definition definition be necessary application our approach use figure as illustration
key instantiation let k grammar%23NPR pn p be key on dataset d instantiation k given subject s d define as set triple k s d s p v p k s p v d where each p p appear at most one triple k s d v be set object d
value property instantiation call instantiation value while subject triple call instantiation subject since properties our context be multivalued key instantiation allow presence property only once given subject different key instantiation exist that subject follow disambiguate by low index set all instantiation k on d denote by k d
let consider dataset department+of+social+security give figure
key k name practice have two instantiation instance grammar%23NPR department+of+social+security k grammar%23NPR department+of+social+security grammar%23NPR name grammar%23NPR practice football k grammar%23NPR department+of+social+security grammar%23NPR name grammar%23NPR practice handball
key instantiation similarity two instantiation k k given key say be similar if be partial perfect string similarity between instantiation value corresponding property assess by string similarity measure some kind denote k k
consider two string perfectly similar when equal while partial similar when have degree similarity explore partial similarity alongside perfect allow consider possible data heterogeneity
www.w3.org%2Ftr%2Fowl2-new-features definition concordance concordance be function assign key instantiation from source dataset set instantiation that same key from target dataset be similar grammar%23NPR x k department+of+social+security y k directorate+of+territorial+surveillance y x
consider key k name practice key instantiation k grammar%23NPR department+of+social+security grammar%23NPR name grammar%23NPR practice handball k r directorate+of+territorial+surveillance r name r practice handball two instantiation consider similar since be perfect string similarity between their instantiation value each key property thus grammar%23NPR k grammar%23NPR department+of+social+security k r directorate+of+territorial+surveillance
concordance function allow assign key ranking support support represent number source instance have at+least one corresponding resource target dataset i.e. similar instantiation that key key key tend share value more instance source target dataset consider as more important linking task
key ranking support ranking support key k grammar%23NPR pn p define as r k distinct s department+of+social+security grammar%23NPR k s department+of+social+security
ranking support be number instance source dataset share value all florida+keys property at+least one instance target dataset dot low index k s department+of+social+security stand possibly multiple instantiation k single resource s
our running example ranking support key k be two since there exist two distinct instance source dataset department+of+social+security grammar%23NPR grammar%23NPR appear gray figure that share value instance target dataset directorate+of+territorial+surveillance
proceed describe our key ranking approach along method combine complementary key improve linking quality begin by introduce definition notation
key identification ranking assumption suppose have two dataset department+of+social+security directorate+of+territorial+surveillance that rewrite use only set their common property use common property intend guarantee discovery grammar%23ENT washington+dc district+of+columbia united+states+of+america grammar%23NPH grammar%23NPH processing pipeline key ranking grammar%23NPR core workflow give in+the+middle box
key on both dataset if dataset use different ontology ontology alignment step take+place use dedicated tool
be optional external our system component grammar%23NPR not directly impact absence property alignment but discovered key be little use link
approach implement processing pipeline depict figure go through component this pipeline
cleaning in+order+to harmonize data by reduce certain trivial form terminological structural heterogeneity between dataset data clean step take+place here different strategy such as downcasing eliminate special+character alphabetical reordering token literal apply depend on nature data
transformation existing key discovery algorithm not make distinction between object properties data+type property treat uri same way as literal i.e. as string
different uri refer same real+world object thus see uri as simple string lead undesired result
render data compatible this limitation without loss information propose rewriting scheme base on resource as show figure give particular node rdf graph cbd be subgraph consist statement statement together constitute focused body knowledge about resource denote by that particular node in+addition+to cbd resource link explore cbd their neighboring resource retrieve literal locate at predetermined exploration depth rdf graph denote by grammar%23NPR according+to grammar%23NPR value data then rewrite by replace chain property between resource literal by artificial properties
key identification once data transform apply external grammar%23NPR automatic key identification algorithm on two dataset link use grammar%23NPR conform grammar%23NPR semantics this choice be line world+wide+web+consortium standard allow respect same semantics as data link tool use obtained key then merge in+order+to generate valid key on both dataset see detail
set valid key both dataset obtain proceed key ranking step rely on concordance criterion introduce section given key first extract only data relate instantiation our dataset value each property compose this key index transform into vector representation projection space vector grammar%23NPR www.w3.org%2Fsubmission%2Fcbd form by different textual entity recover over set value each property next look+for similarity between vector representation come+down classical information+retrieval problem use cosine similarity between query document vector
instantiation value property name figure will translate vector representation vector+space e have b brown cooper grammar%23NPH grammar%23NPH grammar%23NPH as base
evaluate similarity instantiation given key use two level restriction at first every property participate key compare all pair instance two dataset retain only similarity every property exceed given threshold denote as p threshold all pair instance not fulfil this criterion eliminate additionally two instance consider similar aggregation similarity score all property key exceed threshold denote as k threshold ranking support florida+keys be number different instance belong source dataset appear on final set pair instance illustrate this idea provide example
grammar%23NPR grammar%23NPR grammar%23NPR directorate+of+territorial+surveillance r r k grammar%23NPR grammar%23NPR p p threshold k threshold denote s s score pair instance s department+of+social+security s directorate+of+territorial+surveillance have score as string similarity score
threshold calculation instance similarity grammar%23NPR grammar%23NPR r grammar%23NPR r grammar%23NPR r grammar%23NPR r pair similar instance grammar%23NPR grammar%23NPR r grammar%23NPR r grammar%23NPR r pair instance retain grammar%23NPR r grammar%23NPR r grammar%23NPR r instance similarity grammar%23NPR grammar%23NPR r grammar%23NPR r grammar%23NPR r grammar%23NPR r figure data rewriting using grammar%23NPR automatic rdf key ranking data linking grammar%23NPR grammar%23ENT washington+dc district+of+columbia united+states+of+america pair similar instance grammar%23NPR grammar%23NPR r grammar%23NPR r grammar%23NPR r pair instance retain grammar%23NPR r grammar%23NPR r k threshold calculation final pair instance grammar%23NPR r k r k combining complementary merge set identity link produce by several top ranked key yield large set identity link contain more link than set produce by key individually however different top ranked key lead similar set link when combine due set inclusion base on that observation introduce notion key complementarity instead combine key high rank combine complementary key cover maximum number link be optional step apply enlarge output+link set fig. step
definition key complementarity let consider each key set instance source dataset be subject instantiation have empty concordance say that two key be complementary if their respective set instance comply this criterion be subset
first key be one cover largest set instance always belong combined set key also instance set be core global set cover maximum number source instance recursively iterate over all instance set remaining key rank+order each instance set suppress intersection global set retain only complementary instance if instance set be empty then relevant key discard since coverage already include global set at end add combined key best ranked key key offer good complementarity global set this process continue as+long+as global set not cover all source instance as+long+as be key key propose complementarity global set
evaluation result current section aim demonstrate both effectiveness our key ranking algorithm supremacy over in+addition show impact similarity threshold variation as+well+as improvement data link result achieve by comb appropriately select complementary key
data use following dataset
person benchmark instance match track synthetic data contain two dataset compose instance
grammar%23NPH sandbox benchmark grammar%23NPR synthetic benchmark approximately instance
two real-world benchmark base on data from describe musical+work from catalog two french institution grammar%23NPR pp version data be part grammar%23NPR grammar%23ENT challenge this dataset contain two subbenchmarks
islab.di.unimi.it%2Fcontent%2Fim_oaei%2F2010 grammar%23NPR islab.di.unimi.it%2Fcontent%2Fim_oaei%2F2016 grammar%23NPR www.doremus.org grammar%23NPR github.com%2Fdoremus-anr%2Fdoremus-playground consist pair dataset contain each musical+work this benchmark design by expert challenge linking tool capacity avoid generation false+positive match correctly instance presence highly similar but still distinct candidate
ds ht contain two dataset link musical+work each be highly heterogeneous their description this dataset design by expert determine whether tool be able cope heterogeneity
each benchmark both dataset conform same ontology
setting proceed as+follows start by key identification ranking themerged key on given benchmark
key then use individually one+after+another data link as part configuration link specification file start from key high rank use linking tool silk lime ensure independence result on used system since result obtain two system be analogical mainly report obtain by use silk efficiency key then evaluate accord quality resulting instance matching let recall that be not linking tool objective be sort key propose by automatic key discovery tool accord their usefulness produce link by use dedicated tool therefore be not silk lime key discovery tool that evaluate here but quality ranking give by hence like observe monotonic descending curve relate key on x-axis their corresponding f measure yaxis produce when key use link high rank florida+keys great corresponding f measure our experimental result along data use make available https%3A%2F%2Fgithub.com%2Fhfarah%2Fkeyrank-2017
scalability all experiment run on core grammar%23NPR grammar%23ENT as be intermediate tool apply between key generation data link evaluate scalability be tangled scalability two steps silk grammar%23NPR not scale well on very large dataset numerous key even if ranked list key provide be likely that linking part not terminate reasonable time
example on grammar%23NPH contain triple generated key key ranking alone take ms while execution silk ms complexity take alone inherit polynomial complexity information+retrieval technique use
versus et+al. before confront two approach experimentally let provide detailed comparison their methodology cf table
both approach take key discover each dataset separately before apply common merge operation obtain key valid on set dataset introduce series pretreatment data rewrite scheme provide novel more sophisticated ranking strategy ensure improvement quality ranking while not make distinction between uri literal key selection step construct rich description resource link by exploit literal values found at different depths rdf graph use grammar%23ENT washington+dc district+of+columbia united+states+of+america grammar%23NPH grammar%23NPH aspect et+al. uri literal distinction yes no depth description information variable n fixed data preprocess rewrite yes no key ranking support key value key frequency complementarity method yes versus et+al.
measure grammar%23NPR grammar%23NPR about name grammar%23NPR about name grammar%23NPR about comment grammar%23NPR about comment grammar%23NPR grammar%23NPR grammar%23NPR comment grammar%23NPR grammar%23NPR grammar%23NPR comment grammar%23NPR grammar%23NPR grammar%23NPR caption grammar%23NPR grammar%23NPR grammar%23NPR caption grammar%23NPR table grammar%23NPR key compare
cbd this way avoid identify as distinct different uri refer same real+world object share value same property take+into+account inadequacy key frequency as ranking criterion introduce new notion key support base on value value similarity allow consider potential data heterogeneity both approach propose strategy combine key from ranked list et+al. rely on top- k ranked key analyze impact choice k contrast exploit complementarity key terms instance set cover select key consecutive rank improve result
keep platform favorable comparison remainder subsection linking tool use only strict equality compare literal values no similarity measure take into consideration this step finally ensure that set key rank be both approach apply all pretreatment on data prior ranking
on synthetic benchmark grammar%23NPR first experiment carry+out on person benchmark grammar%23NPR grammar%23ENT this setting exploration depth set merged key discover exploit link two dataset use tool silk
see figure grammar%23NPR ranking strategy our approach succeed predict linking quality discovered key
four top ranked key use our approach be grammar%23NPR postcode postcode street note note key compose artificial properties
result show significance rewrite data retrieve useful information describe resource from different depths rdf graph
grammar%23NPR show that not manage classify key correctly lead rocking curve f measure note also that key rank grammar%23NPR by rank 9th observe that frequency florida+keys data use as quality measure be not reliable linking criterion since key low support produce more link
second experiment conduct on two dataset contain instance class blogpost from grammar%23NPH sandbox benchmark this evaluation demonstrate impact variation graph exploration depth figure report result obtain use respectively
result consolidate conclusion first experiment show that our approach outperform observe minor flaw on f measure curve due false+positive cause by instance be not identical but share value key property their monotonic aspect however remain globally preserve
exploration depth variation result show impact this parameter on number quality discovered key on one hand increase exploration depth almost quadruple number discovered key from
important number discovered key perfectly illustrate value automatic key ranking case large volume data on+the+other+hand increase exploration depth make possible recover more discriminating information consequently discovered new important key table present each exploration depth four good key use as+well+as their respective f measure produce by silk overall state that key set be disjoint that linking performance improve considerably by compare key same rank
on real-world data figure show our result on data confirm both quality performance supremacy as compare note that top key produce by latter be unable generate any identity link evaluation on the+halo+trust show figure lead similar conclusion note on data two most frequent key be also most efficient explain why be able rank correctly
key ranking recall that our approach explore similarity between key instantiation take+into+account data heterogeneity not restrict perfect match although try reduce some form trivial heterogeneity other more complex form persist at this stage focus on deal terminological heterogeneity by apply string similarity measure
this experiment study ranking behavior as function relaxation constraint similarity between instantiation key this purpose adopt constant similarity threshold compare instantiation value property p threshold test decrease global similarity threshold k threshold set use both linking tool silk lime four different k grammar%23NPR use compare string value as show figure verify hypothesis hypothesis reduce similarity threshold alter key ranking improve linking quality for+example key 7th rank k threshold raise 4th rank k threshold
combining complementary key close this section by report on result our complementary key selection approach apply all benchmark dataset use our experiment set use only grammar%23NPR automatic rdf key ranking data linking grammar%23NPR grammar%23ENT washington+dc district+of+columbia united+states+of+america b et+al. figure result on person benchmark use silk b et+al. figure result on grammar%23NPH sandbox use silk
b et+al. figure result on grammar%23NPH sandbox use silk
discovered key combined key f measure combined key f measure all key f measure grammar%23NPR person grammar%23NPR grammar%23NPH grammar%23NPR grammar%23NPR the+halo+trust grammar%23NPR daewoo+k7 table complementary key selection result use silk p threshold k threshold instantiation match silk table summarize result obtain on each benchmark compare performance combined set complimentary key all key top ranked key alone note that combined complementary key achieve maximum performance link compare first use all key second top key grammar%23NPR note that complementary key be not necessarily successive rank rank perfectly illustrate premises our complementarity method
conclusion grammar%23NPR key identification approach propose facilitate data link task be case not directly applicable this task
precisely approach not generate key valid pair dataset link do provide ranking discovered key with+respect+to data linking task in+turn hinder ability both linking tool user select most appropriate set property linking system configuration attempt close this gap propose florida+keys selection ranking tool assist link specification extend propose combination complementary key improve linking quality series diversified experiment demonstrate high performance supremacy over only known competitor
plan further improve exploration key instantiation similarity in+order+to support different type data heterogeneity ranking birthplace dbo spouse dbo award be+born marry die at play r p portion bi partite property pattern graph where grammar%23NPH be+born question be+born be grammar%23NPH noun noun born be noun question pattern dbo birthplace dbo dbo spouse dbo relation relation q pattern q grammar%23NPR pattern q g b question question pattern relation select x where dbr grammar%23NPH dbo relation x select x where dbr grammar%23NPH dbo birthplace x select x where dbr grammar%23NPH dbo x em pt y re w ro ng re su lts dbr dbr grammar%23NPR dbr grammar%23NPR c potential grammar%23NPR query using identified relation figure instance problem bi partite graph semantically type relational pattern illustrate portion bi partite graph grammar%23NPH b question pattern corresponding relations grammar%23NPR properties from grammar%23NPH c potential grammar%23NPR query from selected grammar%23NPR properties
be+born appear times correspond six grammar%23NPR relation like dbo birthplace correct answer this case dbo religion dbo parent dbo predecessor dbo spouse dbo hence if simple keyword base matching generic similarity technique use match phrase pattern question multiple grammar%23NPR relation given pattern retrieve from knowledge+base pattern be+born for+instance lead six candidate grammar%23NPR relation grammar%23NPH as depict figure grammar%23NPR grammar%23NPH et+al. first two author current paper grammar%23NPR et+al. describe this problem grammar%23NPH report noisy behavior grammar%23NPH pattern while build grammar%23NPR system relation linking tool many incomplete ambiguous pattern grammar%23NPH such as s son adj lt ref gt also cause noisy behavior grammar%23NPH however our question only dbo birthplace provide correct relation relation allow grammar%23NPR developer construct grammar%23NPR query retrieve correct answer case identify predicate dbo dbo spouse dbo parent grammar%23NPR system system utilize patty retrieve wrong answer while match predicate dbo predecessor dbo relation lead empty answer set see figure grammar%23NPR therefore relational pattern knowledge+base need exploit efficient way in+order+to increase precision recall grammar%23NPR system
problem statement solution this section present problem capture knowledge semantically type relational pattern further propose index based approach approach allow+for efficiently extract properties from knowledge+base solve relation link task question+answering pipeline
bi partite graph semantically typed grammar%23NPR pattern collection semantically type relational pattern correspond bi partite graph pattern properties knowledge+base
collection g semantically type relational pattern define as triple g r p e where p r be two disjoint+sets represent semantic relational pattern properties knowledge+base e.g. rdf property from grammar%23NPR grammar%23NPR respectively
e be set pair r p grammar%23NPR represent semantic type r relational pattern p i.e. r be property semantically related p
represent as bi partite grammar%23NPR r p e where relational pattern p aremined from large corpus properties r correspond grammar%23NPR predicate associate semantically relate pattern figure grammar%23NPR illustrate portion bi partite graph grammar%23NPH
pattern q g r p pattern q p r e figure grammar%23NPR present relational pattern grammar%23NPR be grammar%23NPH born as+well+as their associated semantic type grammar%23NPR semantic type associate pattern use question+answering pipeline building grammar%23NPR query evaluation provide answer question q. for+example figure grammar%23NPR show three grammar%23NPR query query build from grammar%23NPR predicate dbo birthplace dbo dbo relation give set grammar%23NPR semantic type rdf property grammar%23NPR pattern q g f grammar%23NPR d q g denote set grammar%23NPR query over knowledge+base d that use predicate grammar%23NPR provide correct answer question q f grammar%23NPR d q g define as+follows f grammar%23NPR d q g r r grammar%23NPR grammar%23NPR grammar%23NPR pattern q g r grammar%23NPR q d r be grammar%23NPR query compose triple pattern predicate be r grammar%23NPR q d represent set set only include grammar%23NPR query query need run over d produce complete answer question q
our running example resource dbr grammar%23NPR dbr grammar%23NPR dbr correspond complete answer q grammar%23NPR one grammar%23NPR query produce result i.e. grammar%23NPR q d only compose this query
although f grammar%23NPR d q g figure grammar%23NPR include this query grammar%23NPR born marry die at play birthplace spouse parent spouse parent successor child grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR team league college vector representation knowledge graph r p each pattern grammar%23NPH act as index bucket relation bucket pattern grammar%23NPH associated vector vector representation pattern knowledge graph figure example grammar%23NPR on grammar%23NPH portion semantically bi partite knowledge+base grammar%23NPR grammar%23NPH
two query this set produce either incorrect empty result
problem statement give question q grammar%23NPR semantically typed relational pattern problem link relational pattern q semantic type from knowledge+base d correspond select subset grammar%23NPR grammar%23NPR pattern q g from maximal number grammar%23NPR query that produce correct answer q generate define problem link relational pattern question as following optimization problem argmax grammar%23NPR pattern q g f grammar%23NPR d q g max grammar%23NPR grammar%23NPR q d since set grammar%23NPR q d only include one query our running example optimal+solution this optimization problem correspond set grammar%23NPR only compose grammar%23NPR property dbo birthplace this property be part only triple pattern grammar%23NPR query produce complete answer question q
proposed solution match correct relation from knowledge+base given input question q follow two-step process first step semantically bi partite knowledge graph grammar%23NPR build second step grammar%23NPR utilize pipeline relation linking
semantically bi-partite knowledge+base grammar%23NPR
first step apply glove model grammar%23NPH build vector representation bi partite graph g r p e i.e. each node r p replace by vector representation
convert into g r p e where r p be vector representation semantically typed relational pattern their associated grammar%23NPR relation respectively furthermore dynamic hash on semantically typed relational pattern build each entry hash+table correspond bucket compose predicate e.g. grammar%23NPR associate pattern key bucket figure illustrate portion build on top patty
pipeline relation linking use find associated relation set grammar%23NPR be part set grammar%23NPR pattern q g see section four step process follow figure illustrate step this pipeline
potential relevant relations this first step pipeline convert pattern q into vector representation pattern q then calculate cosine similarity between pattern q semantically typed relational pattern p such that sim pattern q p threshold t grammar%23NPR t be minimum admissible limit cosine similarity value result into set potential relevant relation vector grammar%23NPR pattern q g our example input this step be vector question pattern e.g. where where be be+born be noun noun born output be list vector associate potential relevant relations dbo parent dbo spouse dbo relation dbo grammar%23NPR dbo birthplace dbo predecessor
perform output grammar%23NPR p from vector question pattern be+born be noun noun born grammar%23NPR sim p q threshold grammar%23NPR pattern q g parent spouse grammar%23NPR predecessor birthplace relation find potential relevant relations input step perform output grammar%23NPR pattern q g parent spouse grammar%23NPR predecessor birthplace relation grammar%23NPR penalty w threshold grammar%23NPR grammar%23NPR pattern q g parent spouse birthplace grammar%23NPR predecessor relation b ranking potential relevant relations input step perform output question predicate pr be+born remove unnecessary word eg. be apply synonym create vector synonym grammar%23NPR pr born birth deliver bear input step perform output grammar%23NPR pr born birth deliver bear from grammar%23NPR c grammar%23NPR pattern q g parent spouse birthplace grammar%23NPR predecessor relation from step b grammar%23NPR sim grammar%23NPR grammar%23NPR grammar%23NPR r birthplace parent spouse grammar%23NPR predecessor relation birthplace change position grammar%23NPR potential relations generate c extend set relevant natural+language relations input question d re rank relevant relations figure relation linking pipeline four-step pipeline exploit indices knowledge
potential relevant relations number occurrence particular pattern grammar%23NPH be not uniform as illustrate section therefore be likely that while calculate cosine similarity some relations rank high than due high number associated matched pattern solve this issue apply penalty function each relation r grammar%23NPH first count number relational pattern associate then this value normalize by total number pattern grammar%23NPH penalty grammar%23NPR define as+follows w count pr count pall count pr n count pall pr pr n be number pattern relation pall be total number relational pattern grammar%23NPH this step change ranking relations step therefore grammar%23NPR pattern q g now turn into ranked relation grammar%23NPR pattern q g be output this pipeline step our example ranked list relevant relation update from list dbo parent dbo spouse dbo grammar%23NPR dbo predecessor dbo birthplace dbo relation dbo parent dbo spouse dbo birthplace dbo grammar%23NPR dbo predecessor dbo relation i.e. grammar%23NPR predicate dbo birthplace rank high position
set relevant natural+language relation input question many times irrelevant pattern appear question match high number while calculate cosine similarity previous step for+example word where appear 1%2C498 times grammar%23NPH negatively impact on overall result therefore overcome this problem extract nl relation from input question grammar%23NPR be very likely that grammar%23NPR predicate associate have similar name nl predicate for+example nl relation be+born associate dbo birthplace relation president associate dbo president relation wife associate dbo spouse ranked list grammar%23NPR properties and+so+on therefore extract predicate pr from question q furthermore expand this list synonym from grammar%23NPR
then create vector representation relation grammar%23NPR pr use glove model our running example relation be+born expand list born birth bear deliver convert further into vector representation
rank relevant relation last step pipeline take output second third step correspond vector representation ranked potential relation grammar%23NPR pattern q g extended predicate pattern grammar%23NPR pr again calculate cosine similarity between re- rank list obtained relation grammar%23NPR pattern q g
our example extended question predicate list from third step be born birth bear deliver ranked list potential relation from second step pipeline be dbo parent dbo spouse dbo birthplace dbo predecessor dbo relation dbo grammar%23NPR this step relation dbo birthplace have high similarity birth change position ranked list relation therefore our final re- ranked list relation associate pattern be+born be following dbo birthplace dbo parent dbo spouse dbo grammar%23NPR dbo predecessor dbo relation grammar%23NPR predicate dbo birthplace be grammar%23NPR
experimental study empirically study efficiency effectiveness extract properties from knowledge+base solve relation link task first experiment assess precision recall f-score our approach use grammar%23NPR benchmark address following research question grammar%23NPR what impact use on relation linking task grammar%23NPR what grammar%23NPR use on relation link execution+time grammar%23NPR impact on size collection semantically type relational pattern experimental configuration be as+follows relation linking benchmark grammar%23NPR al create benchmark entity link task base on grammar%23NPR question+answering over benchmark use evaluate complete grammar%23NPR system devise similar approach relation link benchmark use grammar%23NPR train grammar%23NPR contain question
execution+time elapsed+time between submission question engine delivery relevant grammar%23NPR relation timeout set grammar%23ENT ii calculate as average execution+time baseline average execution+time iii memory+size total size grammar%23NPH knowledge+base size it corresponding
calculate as memory+size grammar%23NPH memory+size v global precision number correct relation retrieve at first rank list relation out total number vi global recall number question answer at any position our case till 5th position occurrence relation list out total number vii f-score harmonic+mean global precision global viii precision k cumulative precision at position k. ix recall k correct relation question recommend top k position out total number x f-score k harmonic+mean precision recall at position k
pipeline relation linking implement grammar%23ENT experiment execute on laptop quad-core grammar%23ENT grammar%23NPR processor grammar%23ENT ram run grammar%23NPH word vector conversion be use glove furthermore extract nl predicate from input question third step pipeline section use source+code evaluation result download from https%3A%2F%2Fgithub.com%2Fwdaqua%2Frematch
experiment performance evaluation using relation linking benchmark evaluation relation linking task using evaluate impact on relation link task first calculate performance patty use similarity measurement between question pattern patty relational pattern use cosine similarity call baseline approach baseline approach patty directly use without indices however our approach use i.e. patty indices along pipeline describe section out question grammar%23NPR use patty pattern answer question
remaining question not have any associated relational pattern grammar%23NPR question grammar%23NPH be therefore out scope evaluation table illustrate result use our approach global precision increase from 17%25 51%25 compare grammar%23NPR github.com%2Fag-sc%2Fqald%2Fblob%2Fmaster%2F7%2Fdata%2Fqald-7-train-multilingual
www.textrazor.com global precision global recall f-score baseline our approach
figure performance baseline compare terms global precision global recall global f-score high value be good increase precision recall f-score at cost evaluation time andmemory consumption precision recall f-score improve by up three times
baseline mean significant improvement nearly three+fold be true global recall f score
further observe impact our approach on capture knowledge from knowledge+base by calculate precision recall value till first five occurrence obtained list relation divide question two three properties into different group as show table for+example question professional surfer be+born contain two grammar%23NPR properties namely dbo occupation dbo birthplace
table have two three row depend on number relation question use our approach relation link precision recall at first+position be high enough prove that our implementation easily use as relation link tool modular question+answering framework such as grammar%23NPR grammar%23NPR significantly improve overall performance grammar%23NPR system general for+example grammar%23NPR system system use patty show average precision
relation link tool replace by our approach improve overall performance system furthermore exclude performance comparison state-of-the-art relation link tool present because this work not use background knowledge+base patty rely on modeling nl relationswith their underlying part+of+speech part+of+speech be then grammar%23NPR dependency parsing contrast our approach focus on enhance efficient knowledge capture from knowledge+base relation linking further extend other similar knowledge+base like patty however combine both approach result good performance relation link task since relational pattern grammar%23NPH limit
experiment trade-off between illustrate trade-off between different dimensions performance metrics based approach compare grammar%23NPR grammar%23NPR question grammar%23NPR be also part later version grammar%23NPR performance cumulative frequency at rank position precision recall f-score also report at grammar%23NPR grammar%23NPR accuracy based relation link method enhance whenever grammar%23NPR result consider
frequency at rank position precision k recall k f-score k num property total rank rank rank rank rank property 116+55+71+78+84 47.4%25 57.6%25 75%25 63.2%25 property 47.6%25 53.1%25 61.9%25 57.2%25 23.8%25 44.9%25 71.4%25 52.6%25 property 16.6%25 16.6%25 16.6%25 16.6%25 16.6%25 30.5%25 66.6%25 41.8%25 16.6%25 22.2%25 33.3%25 26.64%25 table performance comparison baseline grammar%23NPR predicate consider enhance accuracy proposed relation linking method
precision recall f-score baseline 17%25 37%25 23%25 51%25 73%25 60%25 baseline choose global precision global recall f score memory size execution+time as five different dimensions in- memory+size grammar%23NPH knowledge+base increase from mb mb as convert patty two column corpus relational pattern associate grammar%23NPR relation into index bipartite knowledge+base from patty use glove model also average execution+time question be increased from grammar%23ENT grammar%23ENT large portion total execution+time our implementation question include call nearly 20%25 extract nl predicate from question figure illustrate trade-off between five dimensions although based approach be more expensive by order+of+magnitude terms memory consumption execution+time show drastic improvement with+regard+to precision recall f-score
grammar%23NPR advancement semantic+web technology researcher develop more+than grammar%23NPR system since grammar%23ENT interpret user natural+language input question fetch answer from online knowledge+base such as grammar%23NPR freebase however have monolithic implementation find limitation reusability their component address this problem considerable work do research community build grammar%23NPR system use generic framework grammar%23NPR grammar%23NPR be effort build grammar%23NPR system collaborative effort by create component different task question+answering process rather than build whole grammar%23NPR system from+scratch
tool such as grammar%23NPR thus find applicability grammar%23NPR framework perform ned relation linking task however grammar%23NPR system developer implement relation link task use different approach system system have monolithic implementation have dedicated component name map subject predicate object question corresponding grammar%23NPR resource properties have similar component component take dependency tree generate from input question as input map all node this tree concepts grammar%23NPR ontology same task grammar%23NPR system like grammar%23NPR use patty however from research publication no source+code available be unclear how system use patty grammar%23NPR system grammar%23NPR use boa pattern library patty collectively relation linking task largely rely on boa pattern library if relational pattern be not then search pattern grammar%23NPH corpus
not fully rely on grammar%23NPH due noisy grammar%23NPR also report by grammar%23NPR et+al.
conclusion this paper present novel approach semantic base index index be able capture knowledge encode background knowledge+base such as patty relation linking task many grammar%23NPR system e.g. state-of-the-art relation link tool such as not completely rely on grammar%23NPH owe inherent noise poor baseline performance memory overhead be approach be generalized application on similar knowledge+base alleviate limitation
indices allow not only speed+up search but also reduce irrelevant relations appear selection while efficiently effectively matching natural+language pattern semantic relational pattern knowledge+base demonstrate case where semantically type knowledge+base now fully utilize comparable degree already successful graph type
can therefore be integrated other successful technique semantic disambiguation such as grammar%23NPR similarity measure besides inclusion synonym extend precision relation linking tool be three possible way extend this work relation can match use other already existing approach weighted metrics provide discriminate among approach
ii explore evaluate performance integrated relation match system system incorporate multiple type background knowledge+base
iii provision more semantic knowledge+base use overall success grammar%23NPR system
discuss main developer personally grammar%23NPR hide link enhance quality rwanda+defense+force grammar%23NPH grammar%23NPH grammar%23NPH nmihindu isantana rgarcia asun abstract become core component most artificial+intelligence application as method publishing kg allow application traverse within even out graph thanks+to global dereferenceable identifier denote entity form iris however as show this work analyze several popular dataset namely grammar%23NPR cache data many entity represent use literal string where iris use diminish advantage use remedy propose approach identify such string replace their corresponding entity iris proposed approach base on identify relation between entity base on both ontological axiom as+well+as data profile information convert string entity iris base on type entity link by each relation our approach show 98%25 recall 76%25 precision identify such string 97%25 precision convert their corresponding grammar%23NPR considered kg further analyze how connectivity kg be increased when new relevant link add entity as result our method
experiment on subset spanish grammar%23NPR data show that add 25%25 more link kg improve overall connectivity by 17%25
information system data cleaning grammar%23NPR knowledge+representation reasoning keyword knowledge enrichment data quality association+for+computing+machinery reference format grammar%23NPH grammar%23NPH grammar%23NPH repair hide link enhance quality rwanda+defense+force grammar%23NPR grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america
new+york new+york united+states+of+america https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148020 permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148020 introduction be core component most modern artificial+intelligence cognitive application kg provide representation real+world entity their relationship use graph based model popular such as grammar%23NPR grammar%23NPR grammar%23NPR use rdf model knowledge+representation usually publish their data as key advantage using be that data consumer traverse through typed link graph discover related entity represent by global dereferenceable identifier i.e. iris provide additional value allow access resource inside outside graph for+example by connect other external kg this characteristic be very core as state first fourth principle use uri as name thing grammar%23NPR link other uri so that discover more thing
when literal object use object become leaf node graph without outgoing link further vertices be many situation be legitimate use literal object for+example when represent numerical+value date clear proper semantics associate nevertheless notice that in+practice knowledge+base such as grammar%23NPR data embed web+page literal often use represent entity when entity identifier i.e. grammar%23NPR use this issue exemplify figure where rwanda+defense+force graph have literal apple related two entity b same literal correctly disambiguate link two corresponding entity graph
problem use string represent entity be that hinder connectivity graph even+though literal be not truly isolated node graph due their incoming link much less connect compare define entity entity have both incoming outgoing link also be not possible provide additional information about such literal because unlike grammar%23NPR resource properties not associate ideally issue never happen specially when domain model i.e. vocabulary ontology be available define how knowledge represent ensure that proper entity link between define during kg construction phase however as demonstrate later this paper find large+number situation where be not case goal this work be identify such string literal replace their proper entity iris make data more consistent easy query increase connectivity graph
www.w3.org%2Fdesignissues%2Flinkeddata.htmlk-cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH et+al.
rwanda+defense+force graph literal b iris
connectivity use literal where iris be introduce ambiguity terms graph similar happen natural+language coming+back example depict figure apple+pie ingredient apple grammar%23NPH employer apple term apple refer two different real+world entity semantic+web mitigate this problem by name entity distinct identifier for+instance grammar%23NPR entity assign dbr apple dbr grammar%23NPR respectively
instead use literal if iris use such ambiguity eliminate
materialize as string several reason be due limitation graph generation process for+instance grammar%23NPR extraction framework when key grammar%23NPR not have mapping infobox value be string system not have enough information decide whether that string refer entity kg not however such information infer later+on use profile on all value given property as discuss later this paper further publishing structured data from relational+database csv as be considerably much easy publisher directly represent column value as string rather than disambiguate properly link nevertheless such string reduce quality published data this paper propose approach detect string literal denote entity transform into iris graph
transformation not always be possible for+example when detected entity not yet exist graph make not possible find corresponding identifier even+though new entity add kg remedy be out scope this paper our approach focus on link already existing entity base on detection properties link entity on kg hereafter use entity relation refer such link entity relation link entity other entity irrespective+of whether entity represent as grammar%23NPR as string literal entity e.g. los+angeles apple
not consider link literal such as integer date string not denote entity e.g. abstract comment as entity relation detect entity relation use data profile technique identify pattern pattern give clue that relation be between two entity not between entity literal in+addition+to information provide by corresponding ontology vocabulary
wiki help grammar%23NPR main contribution work present this paper be as+follows analyze entity denote by string two large dataset grammar%23NPR json-ld data grammar%23NPR b propose approach identify such string 76%25 precision 98%25 recall c propose approach transform string corresponding entity identifier 97%25 precision 49%25 recall d improve subset spanish grammar%23NPR dataset version grammar%23ENT by increase connectivity by 17%25
rest paper organize as+follows section define problem study this paper how differ from other related problem field section discuss related work section describe approach propose solve problem section illustrate some result section evaluate proposed approach section draw some conclusion future work
problem definition research question question address define as+follows how identify string literal literal denote entity kg how transform string literal into iris correspond entity denote how measure improvement quality because+of transformation first hypothesis this work be that string literal literal denote entity identify by detect relation that link entity i.e. entity relation some entity relationship define explicitly ontology for+example owl ontology property explicitly define as either object property datatype properties all object property be entity relation by+definition for+example dbo capital+property listing define explicit entity relation between country city this property not have string literal as object triple since be not consistent schema therefore correct rdf property not make this explicit distinction rdf property have both entity literal as object triple thus be not straightforward detect entity relation rdf property
dc creator property list explicitly define range be agent be entity relationship but be not possible identify that just use schema contrast if range be either rdf literal instance rdf grammar%23NPR assume that such property be not entity relation
property definition example pref ix r d f h t t p grammar%23NPR org rd f syntax ns
pref ix r d f s h t t p grammar%23NPR org rd f schema
pref ix owl h t t p grammar%23NPR org owl
pref ix dbo h t t p dbped org on logy
pref ix dbp h t t p dbped org p rope r t y
pref ix dc h t t p pu r l. org dc term
dbo c p t l p r ope r t y d e f n t o n dbo c p t l owl ob+river j e c t p r op e r t y r d f s domain dbo country r d f s range dbo c ty
dc c r e t o r p rope r t y d e f n t o n dc c r e t o r r d f p rope r t y r d f s range dc agent
dbp b r t h p l c e p rope r t y d e f n t o n dbp b r t h p l c e r d f p rope r t y
datahub.io%2Fdataset%2Fopenlink-lod-cache grammar%23NPR webdatacommons.org%2Fstructureddata%2F2016-10%2Fstats%2Fstats.html grammar%23NPR hidden link linked data grammar%23NPR grammar%23ENT united+states+of+america rdf property property do not constraint range potentially have both rdf resource rdf literal object for+example dbp birthplace dataset even+though intuitively guess that birthplace be entity relationship schema not provide such information thus propose approach detect whether property be relation between two entity even when schema information be not available using data profiling
entity relation identify either use explicit ontological axiom use data profiling goal our approach be use proper entity identifier iris such relation instead literal introduce new link link improve connectivity graph make data easy query
this line+of+thought second hypothesis this work be that connectivity knowledge graph improve by transform literal node entity relation into their corresponding entity iris be worth note that not all rdf graph be mathematical graph rdf graph edge also act as node e.g. property definition as this work when calculate connectivity work at data level grammar%23NPR not at model grammar%23NPR thus not face this issue
convert literal into iris introduce new link thus increase connectivity graph enhance quality query result allow traverse more entity
critical example if rdf graph have two connected graph grammar%23NPR by add conversion link two component component become single component all node lead much+more result query improve how navigable graph be
related problem some problem similar one target this paper exist some solution propose therefore this section clarify how our goal approach differ from
entity disambiguation named entity disambiguation generally refer identification entity mention refer as surface form natural+language text link corresponding entity knowledge graph such as grammar%23NPR be challenging task due two main concern ambiguity synonym ambiguity be when single surface form refer multiple entity for+example refer programming+language indonesian island synonym be when be entity refer from multiple surface form for+example inventor refer same entity objective this paper be quite related named entity disambiguation however our case literal be already knowledge graph lot structural information be already available nevertheless textual information such as surrounding word be not available as consequence our approach aim focus more on structural feature compensate lack textual information use entity connect
table annotation web table annotation refer semantically annotate relational html table extract from
use component as define by graph+theory describe subgraph all node connect but not connect other subgraphs supergraph
consist three sub- task matching each table map class ontology b matching each class column relation map property ontology c matching each row map instance kg task set row label column column uniquely identify row map corresponding entity knowledge graph this task be related because also have more structured input set literal convert into entity already group together into similar thus our approach use some technique inspire by table annotation algorithm such as class matching iterative annotation
state before different area produce result result be related scope this work following subsection describe most relevant how relate our approach
entity consolidation entity consolidation technique aim find consensus over entity be providing mean identify rdf individual individual be equivalent since represent same real-world resource
instance consolidate usually by+means+of owl grammar%23NPR link quality dataset increase specially consider integration different dataset reconciliation service widely propose this goal specially context dataset make connected hub rather than isolated data silo usually select candidate iris decide whether represent same entity reconciliation service perform grammar%23NPR query over target dataset retrieve iris their related properties descriptive label system use measure how close two entity be rely on string similarity technique apply label literal associate
grammar%23NPR based system other tool implement reconciliation api such as one support by silk silk server index semantic resource such as grammar%23NPR
recent contribution area explore data structure find equivalency apply statistical method predicate value identify iris consolidate this approach allow state be key property consolidate iris argue that some property some value property be more relevant than when discriminating candidate author argue that this approach be even more relevant tackle opposite problem problem be state whether two candidate entity be different later study author expose more comprehensive method same problem at scale subset rule use consolidate entity previous method apply disambiguate not consider
table match as already discussed table on expose vast amount data range from numerical+value string make+sense this data be challenging process involve analysis identify their appropriate type also refer as semantic label assign semantic type table allow consume navigate data ink-cap grammar%23ENT united+states+of+america grammar%23NPR et+al.
more efficiently be specially relevant task such as search where model collect evidence semantic type propose such model build around fact that most case table contain one key column column see as label on entity correspond given row rest column be property given entity this approach crawl label entity relation use calculate how likely candidate class relation be apply entity on table result expose show that system improve search engine+performance terms query precision recall when use resultant labeled table
use nlp technique also explore solve this problem use lexical analysis content table select appropriate semantic label this approach exploit latent structure data by apply graph technique label dependency between neighbour token enhance accuracy process labeling field use tokenization graph technique offer high accuracy several domain outperform other string based technique e.g. regular expression
recently gold+standard grammar%23NPR evaluate performance table match system propose relate table record from well-known guarantee quality correspondence process keep mainly manual involve human expert generate validate total almost schemalevel entity level correspondence represent first gold+standard at+large scale author also propose evaluate system base on several manuallycreated conversion rule regular expression clean label data+record
grammar%23NPR gold+standard use evaluation purpose on work where domain independent approach semantic labelling introduce system use several similarity metrics as feature train model model then use infer semantic label table record since system purely base on similarity metrics not related any specific domain but rather composition type data once train work seamlessly regardless+of domain author evaluate performance system by train ontology attribute their values 1%2C000 each from three domain then apply grammar%23NPR result show slightly good performance than
rdf connectivity connectivity dataset partially define by amount link among entity apart+from connectivity also define by type link dataset contain link be direct relate two entity on triple indirect contain several intermediate entity linking path grammar%23NPR grammar%23NPR analyse path base on five different metrics assess their quality
metrics use analyze impact new link usually generate by reconciliation tool structure quality dataset
be system focus on detect how two given entity connect use grammar%23NPR query explore one grammar%23NPR webdatacommons.org%2Fwebtables several dataset take as input pair entity system obtain set most relevant path limit by distance threshold contain node relation relation link base on path recap be also able discover other pair entity relate by similar follow same line but focus only on aim go beyond recap scope provide additional connectivity feature rely on article relate entity obtain category annotate use calculate relevance entity on connectivity path provide good connectivity description even+though both system compare this work conclude latter offer good information quality about connectivity entity notice that recap be generic solution not limited thus evaluation result be not applicable other dataset as note by author other system on state+of+the+art outperform by recap including grammar%23NPR grammar%23NPR
tool provide connectivity profile entity expose how relate this work aim analyze structure dataset by study how component underlying graph connect each other this study target data discoverability dataset aim avoid unconnected piece information repair add implicit link measure how strongly connect different component dataset be
approach this section outline our approach detect string literal literal denote entity entity thus potentially convert iris
approach base on intuition that such string detect by first identify aforementioned entity relation link link relate entity
identification entity relation follow two-fold approach on+the+one+hand follow ontology based approach by check whether definition rdf property be available when rich ontology be available entity relation directly identify by+definition owl object property be entity relation rdf property either rdf literal instance rdf grammar%23NPR as range be not entity relation
when not enough information be available follow data driven approach explore characteristic given property kg data for+example dataset be more+than 50%2C000 properties properties only define as rdf property without any further schema information because their lack semantics identify entity relation by analyze classproperty combination rather than properties independently e.g. dbo plant-dbp family versus only dbp family use class property combination allow analyse properties at more granular level
algorithm identify entity relation class property combination given rdf graph outline algorithm first generate all class property combination rdf graph grammar%23NPR profile generate class-property profile table illustrate information extract during profile list show example then analyze each class property pair identify if represent entity relationship not use their definition as+well+as generated hide link linked data grammar%23NPR grammar%23ENT united+states+of+america algorithm candidate detection algorithm data rdf graph g result set entity relation 1+cp 2+cp grammar%23NPR f grammar%23NPR cs grammar%23NPR g collect class property pair profile each class cs do properties grammar%23NPR g class each property properties do 8+cp add class property grammar%23NPR f ile grammar%23NPR f ile g class property grammar%23NPR f class property grammar%23NPR f ile end end identify candidate class property pair each class property do grammar%23NPR f ile grammar%23NPR f et class property if cpprofile.objectdatatypes.contains rdf grammar%23NPR xsd string then continue else if grammar%23NPR property then if owl grammar%23NPR then 19+cp add class property continue else if owl grammar%23NPR then continue if grammar%23NPR grammar%23NPR then 24+cp add class property end profile use identify entity relation only analyze one one have at+least one string literal grammar%23NPR because our goal be convert string into iris first try decide base on property definition if available grammar%23NPR if not pass classifier train on set manually labeled data classifier take element as feature perform binary classification identify entity relation provide evaluation this classifier section
string literal grammar%23NPR conversion once entity relation identify next step be convert their literal object entity iris goal this step be increase quality kg by improve connectivity alsomaking graph consistent property semantics i.e. no violation range constraint this step input be object value given class property pair value contain both iris string literal see listing
sample dbo athlete dbp birthplace value dbr grammar%23NPR dbp b r t h p l c e united+states+of+america en
mi r dbp b r t h p l c e v l l d o l d spa en
s dbp b r t h p l c e en
m chae l grammar%23NPH rdan dbp b r t h p l c e dbr
grammar%23NPR f o o t b l l e r dbp b r t h p l c e dbr
use standard disambiguation tool provide language specific model for+example spanish data use spanish model as our use case be not typical use case disambiguation text annotation propose several approach making input suitable disambiguation
also aim make good use entity already disambiguate mutually disambiguate remaining string
definition property description grammar%23NPR class class property profile
property class property profile
grammar%23NPR number instance given class have at+least one triple given property
number object this classproperty combination also equal number triple contain this classproperty combination
number distinct object this class property combination
number grammar%23NPR object
number literal object
distribution type grammar%23NPR object
distribution datatypes literal object
snippet from class property profile pref ix loupe h t t p ont- l l n k e d d t e s d e f co r e
pref ix dbo h t t p dbped org on logy
pref ix dbp h t t p dbped org p rope r t y
pref ix r d f h t t p grammar%23NPR org grammar%23ENT rd f syntax ns cpp grammar%23NPR loupe c l s s p r o p e r t y p r o f l e l oupe bou grammar%23NPR l s s dbo person loupe bou grammar%23NPR rope r t y dbp b r t h p l c e l oupe n s t grammar%23NPR t xsd long loupe r o b j e c t c oun t xsd long loupe l t e r l o b j e c t c o u n t xsd long loupe r t o l t e r l r t o xsd doub le l oupe grammar%23NPR ta type loupe d t t y p e p r t t o n l oupe d t t yp e r d f l n g s t r n g l oupe ob j e c grammar%23NPR t 2+707924 xsd long l oupe grammar%23NPR ta type loupe d t t y p e p r t t o n l oupe d t t yp e xsd d t e l oupe ob j e c grammar%23NPR t xsd long l oupe grammar%23NPR jec grammar%23NPR loupe ob+river j e c grammar%23NPR t l oupe ob j e c grammar%23NPR dbo country l oupe ob j e c grammar%23NPR t xsd long l oupe grammar%23NPR jec grammar%23NPR loupe ob+river j e c grammar%23NPR t l oupe ob j e c grammar%23NPR dbo c ty l oupe ob j e c grammar%23NPR t xsd long r e s t s omi t t ed f o r b r e v t y context disambiguation tool need textual content have sufficient context disambiguate string i.e. surface form entity link corresponding entity
generate textual context each string by utilize entity entity directly connect string knowledge graph first create simple+sentence by combine entity label as subject property label as predicate string as object then complement by append description entity generate complete context as show listing first line be generated sentence be description extract from kg use grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPR et+al.
from properties such as dbo abstract rdf comment dc description our experiment show that generated context be effective disambiguate entity string
grammar%23NPH grammar%23NPH employer
s american bu s n e s s e x e t v e n d u s t r l eng e e r e l op e r. grammar%23NPH s c u r r e n t seven ch e f ex e t v e o f f c e r o f c. p r e v o u s l y s e r v n g as company ch e f opera t ing o f f c e r t s
apple+pie p e n g r e d e n t
app l e p e s f r u t p ie n p r n c p l f l l n g n g r e d e n t s app l e
t s on occa s on s e r v ed whipped+cream c e cream on top l o n g s d e cheddar se p s t r y s g e n e r l l y use top-and-bottom
identification contrast text disambiguation from unstructured source our use case rely on fact aim disambiguate entity same similar type type already know be because all string take from single entity relation for+example employer relation expect all object be type company organization
aim predict range entity relation use guide disambiguation choose correct entity when disambiguation result multiple entity on+the+one+hand when be large+number existing iris analyze frequency type iris derive heuristics about entity type that given set string denote base on perform disambiguation take+into+account most frequent k type observe existing iris on+the+other+hand when grammar%23NPR percentage be low not reliably do top k frequency+analysis this case follow iterative approach perform disambiguation first iteration resolve all string that disambiguate high confidence then perform type analysis entity predict range relation then second round disambiguation use that information select correct entity when disambiguation confidence be low when disambiguation tool give multiple entity this technique inspire by annotation algorithm
calculation string replacement as final step calculate confidence value base on confidence return by disambiguation tool most frequent k type identify during type identification phase when confidence be high than threshold replace string corresponding entity grammar%23NPR knowledge graph when confidence be low keep string literal as be graph
result grammar%23NPR on technique discuss previous section develop system analyze structure kg decide string denote entity replace by corresponding iris
identification candidate property identify entity relation string object first extract all explicit entity relation property according+to their definition from catalogue property not have any literal object then analyze two large dataset grammar%23NPR b grammar%23NPR grammar%23NPR embed data identify triple where such property have string object result show table table show each property number literal total number object corresponding literal grammar%23NPR percentage
top entity relation string cache property lit. obj. lit. grammar%23NPR dct language 24%2C942%2C739 2%2C147%2C483%2C647 rdf grammar%23NPR 83%2C156 1%2C205%2C624%2C455 rdf grammar%23NPR 2%2C147%2C483%2C647 dct license 2%2C147%2C483%2C647 dct publisher 4%2C312 49%2C312 top entity relation string property lit. obj. lit. grammar%23NPR schema creator 15%2C782%2C874 16%2C573%2C426 schema brand 2%2C420%2C908 4%2C694%2C411 schema author 1%2C500%2C898 26%2C193%2C682 schema grammar%23NPR 1%2C231%2C840 1%2C251%2C870 schema publisher 560%2C577 31%2C317%2C151 identify entity relation relation not have rich property definition i.e. rdf property without any constraint apply algorithm introduce section spanish grammar%23NPR dataset identify 25%2C560 class property combination combination be entity relation contain string literal object be 23%2C880%2C424 string as object properties by analyze corresponding object type convert iris link existing entity each class property relation most common type entity that denote by string show table
top type entity as string grammar%23NPR type string skos concept 7%2C691%2C836 dbo person 6%2C186%2C125 dbo place 4%2C904%2C153 dbo organisation 3%2C267%2C411 dbo work 2%2C319%2C624 literal grammar%23NPR conversion test literal grammar%23NPR conversion phase select subset entity relation from spanish grammar%23NPR data identify previous step this subset include all class property relation class dbo athlete dbo grammar%23NPR dbo grammar%23NPR
show result this conversion process
low recall be mainly due large+number real-world entity not have corresponding page grammar%23NPR hide link linked data grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america table literal grammar%23NPR conversion class prop string convert grammar%23NPR dbo grammar%23NPR 2%2C147%2C483%2C647 41.01%25 dbo grammar%23NPR 2%2C147%2C483%2C647 34.63%25 dbo grammar%23NPR 220%2C169%2C647 98%2C488 58.05%25 such case be not possible find grammar%23NPR for+example string denote mma fighter but because fighter be not string not disambiguate look+into generate new identifier such entity as future work
evaluation evaluate our work base on three dimensions first evaluate our entity relation detection algorithm goal this evaluation be validate if relation detect by algorithm be correct i.e. precision also algorithm detect all entity relation i.e. recall perform evaluation by manually annotate all class property combination three selected class dbo athlete dbo grammar%23NPR dbo grammar%23NPR annotation process conduct by three reviewer reviewer agree on whether each property consider entity relation not table show evaluation result show that algorithm achieve high recall reasonable precision intentionally optimize algorithm recall because false+positive get filter second step
candidate identification performance relation dbo athlete 183+178 77.06%25 97.27%25 dbo grammar%23NPR 157+156 76.47%25 99.36%25 dbo grammar%23NPR 116+115 73.71%25 99.14%25 total 456+449+142 75.97%25 98.47%25 second evaluate string grammar%23NPR conversion process use candidate class property combination similar previous evaluation perform by select sample string from three class select before manually annotate whether linking do correctly not table show evaluation result this step have high precision low recall low recall be due fact that some entity not have corresponding grammar%23NPR kg high precision this step compensate high recall medium precision previous step
select subset spanish grammar%23NPR contain data about instance dbo athlete dbo grammar%23NPR dbo grammar%23NPR dbo grammar%23NPR i.e. original graph then run our algorithm detect entity string convert entity iris generate new graph i.e. repair graph then calculate number graph component check increase connectivity rdf graph table show as result increase number edge connectivity increase significantly accord several metrics
string disambiguation performance class ofstrings recall correct dbo athlete 98.04%25 50%25 dbo grammar%23NPR 44%25 dbo grammar%23NPR 94.83%25 56%25 total 97.38%25 49.67%25 table improvement connectivity graph edge com-ponents isolate largest component size total original 2%2C147%2C483%2C647 repaired 1%2C035%2C912 99%2C137 2%2C147%2C483%2C647 207%2C602 -20%2C486 -18%2C824 24%2C677 25.06%25 17.12%25 16.76%25 14.68%25 when be reduction number component graph graph increase connectivity repaired graph reduce number component by 17.12%25 due additional link another measure connectivity be number component have only one node node be isolated node node not connect any other node graph when connectivity increase number isolated node decrease repaired graph get reduction 16.76%25 number isolated node
number percentage node largest component provide good indicator connectivity when this number increase more node graph be largest subgraph as result connect other node
our experiment increase largest component by 14.68%25 figure show size component order by size number node component original graph as discuss before be able move large+number node 24%2C677 from long tail into main largest component be previously isolated component 18%2C824
be worth note that several issue identify during evaluation as part manual annotation make conversion process not always straightforward some properties be ambiguous for+instance property inicio meaning start athlete instance use relate either club where athlete begin career b year athlete begin such case be hard detect whether be entity relation not because act as depend on sense use other case detect that some string object actually denote entity lists rather than single entity for+example equipos team have string value such as los+angeles+lakers chicago+bulls toronto+raptors new+orleans such value need preprocess extract each individual entity also notice that often url encode as string whereas represent as iris handle case propose perform quick regex check detect if string be url convert grammar%23NPR cap grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPR et+al.
component index s iz e c po figure ranking component size original graph
more than node label number node
consider that be not always possible convert string grammar%23NPR even when correctly detect that given string denote entity corresponding resource not exist kg thus not have identifier link for+instance case grammar%23NPR even if resource exist real+world not have entry grammar%23NPR if be not page be out scope this paper
conclusion grammar%23NPR from our analysis on three popular kg conclude that problem entity wrongly represent as string literal exist all three indicate that be common problem kg
introduce approach address this problem by identify string link corresponding entity grammar%23NPR improve overall data quality e.g. consistency connectivity usability kg this approach rely on schema information when available on data pattern such as statistics about grammar%23NPR literal object type distribution object entity allow our system convert string denote instance into entity iris thus add new link between individual improve connectivity kg implement our approach demonstrate practical feasibility evaluate performance our method terms precision recall against set manually property entity also measure how new link affect connectivity kg by compare component metrics graph evaluation result show that improve connectivity by 17%25 from result conclude that proposed approach detect string denote entity high recall convert corresponding iris high precision b this process increase connectivity graph
remaining challenge be increase recall string disambiguation when corresponding entity not exist kg currently explore how new identifier automatically add kg remedy also plan expand our evaluation other dataset this study apply our approach on spanish grammar%23NPR increase number link by 25%25 plan perform similar study on english grammar%23NPR near future further also plan study impact connectivity improvement on grammar%23NPR query result
this work fund by spanish government grammar%23NPR grant grammar%23NPR project grammar%23NPR grammar%23NPR grammar%23NPR project
grammar%23NPH grammar%23NPH grammar%23NPH silk link discovery framework data 18th international conference article page
gong cheng grammar%23ENT grammar%23NPR association between entity via top- k ontological pattern facet 422+437
grammar%23NPH grammar%23NPH grammar%23NPH exploit structure within data accurate label use conditional random field proceedings on
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH assess linked data mapping using network measure semantic+web research application 9th proceedings

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR revealing relationship rdf knowledge+base
semantic multimedia 4th on grammar%23NPR proceedings 182+187
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH lope grammar%23NPH p grammar%23NPR profiler tool profile connectivity entity pair grammar%23NPR grammar%23NPR 5th grammar%23NPR
on intelligent exploration semantic data grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
entity be more equal than statistical method consolidate linked data proceeding workshop on new form reasoning semantic+web scalable dynamic grammar%23NPR
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH scalable distributed method entity matching consolidation disambiguation over linked data corpus science service agent on grammar%23ENT 76+110
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR grammar%23NPH grammar%23NPR grammar%23NPH et+al. grammar%23ENT grammar%23NPR large-scale multilingual knowledge+base grammar%23NPR from semantic+web grammar%23ENT 167+195
grammar%23NPH grammar%23NPH re-use cool uri entity reconciliation against hub 4th international workshop on linked data on article page
grammar%23NPH grammar%23NPH grammar%23NPH
spotlight shedding light on document proceedings 7th on semantics association+for+computing+machinery new+york new+york united+states+of+america
grammar%23NPH grammar%23NPH grammar%23NPH semantic grammar%23NPR domain independent approach semantic+web grammar%23NPR grammar%23ENT 15th international semantic+web conference proceeding part i. 446+462
grammar%23NPH explaining knowledge graph semantic+web grammar%23NPR grammar%23ENT 14th international semantic+web conference proceeding part i. 622+639
grammar%23NPH grammar%23NPH matching html table grammar%23NPR proceedings 5th on mining semantics grammar%23NPR association+for+computing+machinery new+york new+york united+states+of+america article page
grammar%23NPH grammar%23NPH matching html table grammar%23NPR proceedings 5th on mining semantics grammar%23NPR article page
grammar%23NPH grammar%23NPH sindice.com weave open linked data proceedings 6th grammar%23NPR 2nd on grammar%23NPR grammar%23NPR

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23ENT recover semantics table on grammar%23NPR vldb grammar%23NPR grammar%23ENT 528+538 be my grammar%23NPR same+as your grammar%23NPR lenticular lens department+of+computer+science o.a.k.idrissou%40vu.nl between entity different dataset be crucial element semantic+web architecture since link allow integrate dataset without agree on uniform vocabulary
widely acknowledge that owl grammar%23NPR construct be too blunt tool this purpose entail full equality between two resource independent context but whether not two resource consider equal depend not only on their intrinsic properties but also on purpose task resource use present system construct contextspecific equality link first step our system generate set probable link between two given dataset potential link decorate rich metadata describe how why when by generate second step user then select link link suit current task context construct context specific such lens combine use operator such as union intersection difference composition illustrate validate our approach realistic application application support researcher social+science
computing methodology knowledge+representation reasoning keyword owl grammar%23NPR linkset lens data+integration introduction link between corresponding entity different dataset ontology be crucial element semantic+web architecture after+all link be responsible integration dataset ontology publish by multiple independent party without requirement agree priori on uniform vocabulary allow semantic+web scale
make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148029 quality such correspondence link depend on how well requirements correspondence fit formal semantics predicate use express link dataset information+retrieval purpose pose different constraint than use case where statistical+analysis automated decision+making play role
similarly false+positive skos related relation less far reach consequence than misguided grammar%23NPR as+long+as data use in+isolation link well understand this problem be manageable however when data link share on context link deem be true lose be especially problematic transitive predicate formally state equality between resource for+instance show that grammar%23NPR often misuse as follow chain grammar%23NPR triple stated equivalence between two outer end chain become increasingly at+the+same+time argue that use weak alternative such as rdf grammar%23NPR reduce utility relation express intended semantics seem that different linktypes require different circumstances
rather that truth+value equality assertion depend on context generate
illustrate problem semantics grammar%23NPR example from scientific domain perform analysis multiple dataset field grammar%23NPR grammar%23NPR study dynamics scientific idea field depend on large variety heterogeneous data
study success scientific organization grammar%23NPR researcher need align such organization dataset describe organisation various country e.g. grammar%23NPR grammar%23NPR 3%2C000%2C000 corporation large multinational organisation substantial patent portfolio occur both dataset grid distinguish between national 3%2C000%2C000 branch six country while grammar%23NPR only refer single 3%2C000%2C000 entity entity designate as same dataset depend study that aim compare organization at global level should comparison country should not
issue arise other area biochemistry user not uniformly decide context two entry about molecule different dataset be same pharmacology should drug consider equal if have same brand+name same chemical structure some+other criteria humanities should two imprint same manuscript consider equal same grammar%23NPR be reason that grammar%23NPR match relation be not transitive
https%3A%2F%2Fgrid.ac grammar%23NPR www.orgref.org%2Fweb%2Fdownload.htmtext same author different different printing edition all case choice depend upon application at+hand
core question address this paper be how facilitate user construct share use context sensitive equality relationship between semantic+web dataset extend earlier work on linksets link script scientific lens by make explicit both link method construct link as+well+as justification
then manipulate such explicit representation query combine validate this way construct different equality relation depend on context
approach consist two step first generate multiple set candidate correspondence linksets explicitly represent such candidate set including reason why generate as candidate how generate by when etc. then use this rich metadata select candidate link combine candidate set into context specific lenticular lens then serve as context specific equality relation view over integrated data this paper be structured as+follows discuss related work section give formal definition our approach section provide rdf model implementation section section introduce use case from grammar%23NPR domain evaluate how lenticular lens help user answer research question question rely on alternative mapping dataset section conclude
grammar%23NPR our discussion related work concern three aspect method identify generate equality relation between entity dataset method express correspondence their metadata metadata allow select right link on+demand such way not lead inconsistency
terminology briefly clarify terminology use then discuss how related work compare approach present this paper relation between two entity originate from different dataset either call rdf link more specifically correspondence triple when link express equality relation alignment between two dataset call linkset when correspondence triple that make alignment all use same link predicate this predicate call linktype linkset e.g. owl grammar%23NPR skos exactmatch
generate correspondence theory propose owl grammar%23NPR semantics semantics base equality explicitly on commitment set identity criteria property value pair that resource have common set be varied pending on context our approach generalise as allow other method construct context sensitive equality relation beyond simple intersection equal propertyvalue pair furthermore our approach allow establish different equality criteria between different pair dataset while theory assume single universe equality criteria apply globally detail section
find equality relation between resource make use several tool fall into two category operating as black be array magnify lens design so that when view from slightly different angle different image aremagnified from en.wikipedia.org%2Fwiki%2Flenticular_lens
e.g. use machine+learning other heuristics rely on user guide mapping process example former be grammar%23NPR oto other string graph based similarity algorithm system intend be generic sense not take context domain specific consideration into account make less suitable our purpose as argue above appropriateness linkset most often determine by contextual factor
user guided type seem more promise generate mapping triple base on user define rule rule serve as explicit representation context specific identity criteria for+example use request express grammar%23NPR language discover variety relation between resource one more dataset similarly silk drive by link specification language silk-lsl use express context specific user-defined condition two resource link silk-lsl support syntactic semantic hybrid similarity metrics silk workbench ui allow user inspect confidence value link discover
tool not record justification their context specific mapping declarative form e.g. as rdf metadata as result be hard decide if particular context specific mapping re-use another purpose only justification mapping from tool be implicit encoding context form mapping rule
be interactive tool primarily focus on alignment produce large provenance record record capture how correspondence create use combination grammar%23NPR vocabulary re%C3%AFfied rdf triple statement however such re%C3%AFfied encoding be not suitable representation on+demand context specific selection candidate linksets
pragmatic encoding take provide access integrated biomedical linked data because user have different requirement sameness this access be through so call enable disable specific linksets grammar%23NPR linksets represent as name graph contain pair resource that link use single predicate facilitate selection lens express as rdf use extension void vocabulary grammar%23NPR include reference justification linkset i.e. property on basis two resource link
grammar%23NPR approach improve over silk have declarative representation improve over grammar%23NPR by have practically useful encoding therefore choose build on grammar%23NPR approach
grammar%23NPR scientific lens limit three respect first model only allow express metadata such as confidence at level lens linkset as+a+whole named graph secondly lens only combine multiple linksets through take union such linksets overcome first limitation our approach express metadata at graph at triple level
allow fine-grained reuse candidate link from grammar%23NPR semanticweb.cs.vu.nl%2Famalgame
www.w3.org%2Ftr%2Fprov-o grammar%23NPR openphacts.org
https%3A%2F%2Fwww.w3.org%2Ftr%2Fvoid iilinkset secondly define more complex operation over lens allow union intersection difference composition see section third limitation scientific lens grammar%23NPR be that correspondence express use linking predicate e.g. owl grammar%23NPR directly as result multiple competing lens potentially introduce inconsistency into knowledge+base avoid introduce unique linking predicate individual correspondence
correspondence ensure that decorate each individual correspondence so that select re-use ensure that semantics alternative linksets not interact suggest use n-ary relations express correspondence when multiple alternative match exist at+the+same+time essentially this pattern be similar rdf re%C3%AFfication introduce new resource type rdf statement three predicate identify rdf subject rdf predicate rdf object re%C3%AFfied triple idea be then associate metadata newly added statement unfortunately rdf semantics not allow infer existence re%C3%AFfied triple from existence rdf statement furthermore both reification n-ary relations not only introduce overhead at+least four new triple but also query become more difficult
use named graph each triple however have same effect on query complexity have additional negative effect because triple store indices typically optimize fewer named graph one way address this shortcoming be use in-line syntax express re%C3%AFfied triple data query see listing however require rdf grammar%23NPR
grammar%23NPR grammar%23NPR owl grammar%23NPR dct creator http example.com%2Fagdistis dct create grammar%23NPR z xsd grammar%23NPR
in-line reification statement level metadata grammar%23NPR et+al. propose singleton properties where predicate every individual triple e.g. linkset be uniquely identifiable be rdf grammar%23NPR more generic property allow assign metadata each candidate link singleton property while at+the+same+time semantics link not interact linksets as re%C3%AFfication query complexity performance affect but compare other approach experiment suggest that less affect
section discuss entity match tool use linksets lens allow different view over data way assign metadata named graph individual triple conclude that be no integrated framework that allow user-defined method generate correspondence b allow context specific selection correspondence c allow rich operation over linksets lens
defining lenticular lens this section define notion contextualise equality supporting notion linkset lens before present next section rdf model implementation
semantics owl construction equality owl grammar%23NPR owl grammar%23NPR adhere grammar%23NPH law on identity indiscernibles quasi- rdf notation read p x y x p v y p v where denote triple this bi implication capture two separate principle be too strong useful deployment on semantics direction capture indiscernibility identicals identical object share their property even simple example from our introduction make clear that be too strong due quantification over all possible properties p whereas in+practice set predicate use determine equality differ between context be problem context independence direction capture identity indiscernibles
be too strong use on semantic+web because+of open world assumption again repair by restrict set predicate p finite+set such as predicate occur some knowledge graph some ontology
context sensitive indiscernibility unsuitability grammar%23NPH principle semantic+web also note by
define context as set predicate be necessary sufficient determine indiscernibility hence equality p x y x p y p because+of restriction predicate be now context sensitive be limited closed world
be unclear about treatment properties p
property propagate over equality ie
p x y x p y p not simple example show that answer again differ context grammar%23NPR publish data about more than drug over properties drug among be chemical structure drug biological target brand+name sell pharmaceutical set two drug be equal if have same chemical structure hence context defining property be structure but two properties both not behave differently with+respect+to two drug same structure address same target hence apply but two drug same structure not necessarily have same brand+name hence not apply brand
reason propose rich definition context recognise that both structure be important pharmacological context as oppose pharmacologically irrelevant property brand but that play different epistemological role structure determine indiscernibility equivalence+class inside class value propagate but not determine indiscernibility
context context define by two set property indiscernibility propagation p x y x p v y p v p x y x p v y p v p value p remain unchanged where be definition contextualise equality define contextualise propagation allow drug be indiscernible pharmacological context structure structure still have different value properties outside such as brand
multiple dataset above assume that all properties p entity x value v live single namespace in+practice owl grammar%23NPR link often use create link between multiple dataset different namespace so simply state x p y p be not enough if both side live different namespace extend definition above take into account use symbol indicate alignment between two term from different dataset choose approximate symbol because such alignment be often indeed approximate stringmatching e.g. string matching new+york grammar%23NPR but also be more elaborate e.g. dictionary based matching the+hague the+hague linguistic e.g. translate pneumonia longontsteking now context consist not only set predicate indiscernability propagation but also alignment procedure i.e. context be now then definition contextualise equality become x y grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR x grammar%23NPR grammar%23NPR y grammar%23NPR grammar%23NPR similar definition contextualise propagation
now have place define our central notion linkset lenticular lens give two dataset grammar%23NPR context linkset l be set all pair x y from be indiscernible that context l x y x y in+other+words linkset be set all context specific correspondence between two dataset
grammar%23NPR lenticular lens be also set context specific correspondence between two dataset but construct from linksets through union intersection difference composition x y li x y li x y li x y li x y la lb x y la x y lb x y la o lb z x z la z y lb rdf model implementation this section present rdf model lenticular lens allow generate select combine alignment contextspecific manner aim maximise potential future reuse achieve combine metadata about individual correspondence use singleton properties metadata about entire linkset singleton properties organise property hierarchy allow capture context specific notion equality at different level move hierarchically from linktypes very specific task linktypes more generic task vice+versa find common shared notion indiscernibility whenever different linksets lens combine specific task
illustrate our model see figure simple example from sti domain have three linksets r b y that each use different identity criteria specify correspondence between two dataset grammar%23NPR grid generate r entity from both dataset research organisation align use their resource identifier b apply edit distance algorithm over organisation name only include pair similarity measure finally y refine b by further demand that organisation have same country as+well+as
each correspondence linkset link through singleton property triple specific linktype e.g. two correspondence link through my my eter grammar%23NPR my grid grid grammar%23NPR
eter grammar%23NPR my grid grid listing singleton properties linkset b represent metadata individual correspondence separate named this graph specify evidence strength each singleton property relate shared taskspecific property my generic named graph this later graph serve as basis reproducibility
my rdf grammar%23NPR my ll strength ll grammar%23NPR compare
rdf grammar%23NPR my ll strength ll grammar%23NPR compare
singleton their task specific parent task specific property my define as rdf grammar%23NPR domain specific linktype correspond domain knowledge use determine correspondence grammar%23NPR r+and+b plus grammar%23NPR y. this case domain knowledge my express by relate property algorithm define elsewhere domain specific linktypes define as ll grammar%23NPR link predicate commonly use grammar%23NPR such as owl grammar%23NPR
additionally specify predicate be sub property ll grammar%23NPR figure illustrate this hierarchy
rdf grammar%23NPR grammar%23NPR rdf comment grammar%23NPR use organisation name threshold ll algorithm
task domain specific linktypes
define as graph use extension void grammar%23NPR vocabulary as provide three additional predicate ll mechanism predicate relate omit trig notation metadata graph
meta- model correspondence
domain specific predicate ll ll refine bdb grammar%23NPR bdb grammar%23NPR indicate set property use determine correspondence in+other+words following make explicit identity criteria our correspondence rdf type void void grammar%23NPR my void triple void grammar%23NPR eter void grammar%23NPR grid ll mechanism grammar%23NPR ll ll grid name bdb grammar%23NPR
rdf comment similarity measure interval
defining metadata linkset b using lens use above representation linksets construct lenticular lens lens reflect use case specific identity criteria
lens definition use look+up linksets enable specific grammar%23NPR query be named graph s that scope query our approach differ two important way
enable multiple linksets query make correspondence true that query in+other+words lens only express union linksets secondly our linksets express use singleton properties rather than actual equality relation thus need define how lens combine linksets more expressive way how lens make equality relation visible
consider scenario where user only want take+into+account correspondence correspondence have similar name have same resource identifier be same country list specify such lens as intersection linksets r y from example above express use ll operator property
rdf type bdb lens ll grammar%23NPR void target void triple ll grammar%23NPR ll grammar%23NPR ll operator intersection bdb grammar%23NPR
ll sparql
defining metadata grammar%23NPR
singleton properties not carry semantics by need define how use when answer query over dataset apply be where linktype hierarchy come+into+play merge linksets where linktypes normalize shared ancestor property hierarchy use specification list generate grammar%23NPR query listing do just where pair resource from linksets r y be same insert equality relation use most specific common ancestor respective singleton properties query be in+essence template apply any combination two linksets that merge use intersection resulting lens now use as
that only fixed resource be uri linkset metadata graph
http insert graph ancestor where graph grammar%23NPR graph grammar%23NPR graph grammar%23NPR rdf grammar%23NPR graph grammar%23NPR rdf grammar%23NPR graph grammar%23NPR get all common grammar%23NPR rdf grammar%23NPR ll grammar%23NPR ancestor
rdf grammar%23NPR ll grammar%23NPR ancestor
exclude common ancestor ancestor have common child ancestor filter not exist rdf grammar%23NPR ll grammar%23NPR grammar%23NPR
rdf grammar%23NPR ll grammar%23NPR grammar%23NPR
grammar%23NPR rdf grammar%23NPR ll grammar%23NPR ancestor
listing grammar%23NPR query generate grammar%23NPR without metadata by implement intersection over linksets
lens describe listing be very simple if be need combine multiple alignment find common shared linktype become more challenge provide formal definition supported operator union intersection difference composition section
complex operation over lens be combination linksets and%2For other lens use setlike operator such as union intersection difference composition
operator implement grammar%23NPR not use off-the-shelf our framework three reason first equality relation be symmetric direction triple be irrelevant indiscernability relationship grammar%23NPR r iff grammar%23NPR r where r be linktype second proposed approach identity represent through unique singleton property two instance same identity between grammar%23NPR grammar%23NPR look+like grammar%23NPR r grammar%23NPR r respectively r different from r third new singleton properties generate resulting lens technicality operator behave as their set theoretic counterpart issue address three steps
algorithm generate lens use union operator
alignment compose three graph main correspondence graph generic generic metadata graph specific singleton graph
set alignment g size n triple output lens complexity o n n assume efficient search use note n be total number triple while k much small than n be number duplicate each iteration n
do generic metadata about lens g do o n g x p y grammar%23NPR do o n triple x q y x q y grammar%23NPR search o n grammar%23NPR grammar%23NPR triple preds q x q y triple r grammar%23NPR preds if x y then x r y else y r x x q y triple do o k r prov grammar%23NPR q return grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR first abstract from individual singleton properties their direction by collect all unordered pair grammar%23NPR grammar%23NPR grammar%23NPR r l some singleton property r. second create lens new singleton properties ri each pair satisfy condition construction operator intersection etc. finally choose consistent direction each pair grammar%23NPR ri base on alphabetical order name dataset describe
summary generate lens first ignore unique predicate their direction then set operation perform reconstruct unique linking predicate resulting triple algorithm illustrate use union operator where efficiency such procedure come+down use fast search+algorithm
qualitative evaluation as+usual methodological proposal be not obvious how perform quantitative evaluation proposed model therefore proceed qualitative evaluation through series increasingly complex case+study take from research project sti field show that framework have both necessary sufficient functionality fulfil needs case+study
first present two illustrative use case subsection subsection present more complicated realistic case show how model lenticular lens fulfil requirement such complicated yet realistic use case also discuss time space complexity subsection
visualise execution use case implement prototype user+interface lenticular-lens.risis.eu ease user interaction over lenticular lens+system realise theory framework describe above address data+integration as process where user connect data small steps enable analyse refine combine linking result at each step their context specific integration furthermore ever growing library linksets lens system allow user similar interests reuse modify existing alignment
case+study research question grammar%23NPR selection first two input system be context dataset interest since use case take from observe support social+scientist system associate data linking task context specific research question case study introduce section share same goal align grid grammar%23NPR but have different motivating research question what be innovation strategy multinational firm how canadian industrial structure compare us structure respectively consequently case+study need align firm use company+name independently their geographic location while case+study align on basis matching their name their location
grammar%23NPR creation first look+at case+study figure show that context define scope research question by select dataset grid grammar%23NPR entity type interest institution organisation properties use matching name grid name grammar%23NPR i.e. set matching method approximate string matching i.e. procedure figure grammar%23NPR creation four input together result linkset potential correspondence
current implementation support six matching mechanism exact approximate string uri identity embed link present source dataset intermediate use intermediate data+source similarity geo- similarity but set be intentionally open-ended easily extend
case+study redefine linkset from+scratch not need as linkset from case+study can re-use intersect additional matching mechanism geo- similarity apply property country country grid grammar%23NPR respectively such explicit re-use previously alignment including full provenance trail be essential feature our approach distinguish from literature
andapproving correspondence create linksets user manually refine illustrate this step filter linkset study based on correspondence correspondence have name similarity great than token 3%2C000%2C000 in+the+name+of subject object resource six resource from grid link single occurrence 3%2C000%2C000 grammar%23NPR user now evaluate each correspondence by enrich more annotation in+particular by annotate as accept reject note correspondence mark as reject not remove but remain linkset allow linkset reuse provide full provenance trail again crucial feature feature set our approach apart+from literature
final integration as final step user create final lens by select set correspondence use study view select all resource properties show as part tabular representation present user our case study lens select all correspondence correspondence not label as reject
case+study third case+study illustrate need combine multiple lens show how proposed model be rich enough deal grammar%23NPR simplification our current implementation on linkset always assume
youtu.be%2Fccffblcbf54%3Flist%3Dplo4ybuarfsnwj9xjvp6rlimsaw_rfkt9c viwith this complex case aim be investigate whether university rank grammar%23NPR dataset predict from properties university environment other dataset
need align university five dataset grammar%23NPR provide characteristic european higher+education institution such as third+party fund total academic staff total expenditure student enrol etc grid worldwide collection academic research+institute grammar%23NPR performance metrics over major university worldwide grammar%23NPR last two dataset be subset grid grammar%23NPR enrich geographic boundary from grammar%23NPR count on number worldwide organisation european university grammar%23NPR dotted+line fig. show complex mapping between dataset that need answer research question because+of this complexity this use case be suitable test strength our approach
creation list give textual representation show user+interface figure research question dataset type entity match properties align matching mechanism use grammar%23NPR directly align three different dataset use different type properties mechanism result grammar%23NPR through grammar%23NPR
can predict cwt score from characteristic university mapping dataset grammar%23NPR grammar%23NPR university grid institution grammar%23NPR institution grammar%23NPR university grammar%23NPR university grammar%23NPR specification grammar%23NPR specification dataset grammar%23NPR grid grammar%23NPR grid grammar%23NPR university institution university institution grammar%23NPR name name mechanism approximate similarity approximate similarity grammar%23NPR specification grammar%23NPR specification dataset grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR university university university university grammar%23NPR university university mechanism approximate similarity approximate similarity grammar%23NPR specification grammar%23NPR specification dataset grammar%23NPR grammar%23NPR grid grammar%23NPR grammar%23NPR university university institution institution grammar%23NPR mechanism identifier match identifier match listing case+study lens creation from linksets generate previous step generate lens grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR align grammar%23NPR grid grammar%23NPR also capture by solid arrow figure
as explain above construct view be final step express user contextspecific integration perspective over data see figure system allow construction complex combination lens
show specification generate final view require use case where grammar%23NPR represent final contextual grammar%23NPR www.gadm.org figure integration model case+study integration grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR specify different filter filter refine condition on correspondence be include for+example grammar%23NPR be filter include only correspondence from grammar%23NPR have strength have threshold greater equal evaluate as accepted list also show list property that use construct view table be variable data study
grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR cr f1 grammar%23NPR filter strength threshold grammar%23NPR cr grammar%23NPR filter strength threshold grammar%23NPR grammar%23NPR grid
name level level university country grammar%23NPR total country number r%26d orgs city field grammar%23NPR type listing specification generate final lens view outcome this view be table be readily usable by researcher answer research question this case extent variable indeed predict ranking be answer but correlation between number r%26d organization ranking performance score dutch university only be strong correlation suggest indeed that further analysis be useful as indicate potential causality show that realistic alignment process such as case+study require complex combination operation over individual separate alignment require additional functionality our lenticular lens+system over other system literature justify rich data+model over alignment introduce above
complexity time complexity give that create linkset require two dataset x y time complexity depend on alignment algorithm use size dataset for+example use naive stringbased match algorithm each entry source dataset compare target dataset take o x y time
use blocking solution implement framework take o x z where z y z y furthermore cost grammar%23NPR about this study be available at sms.risis.eu%2Fusecase_universities_environemnt_ decorate generated correspondence be low insertion cost o each correspondence other performance parameter be strong triple store dependent compute exact similarity between resource fully implement grammar%23NPR in+practice take less+than minute create linkset between grid instance grammar%23NPR instance on commodity hardware
complexity space complexity determine by number match found between dataset size l linkset l x y however in+practice l max x y decoration add only fixed+number n triple correspondence result output n times big than decorate linkset
conclusion this paper present system construct context specific link between data+set lenticular lens context specific approach need because whether not resource different dataset be consider indiscernible strongly depend on purpose task dataset be use not propose any new disambiguation method instead our system allow use existing alignment method construct linkset potential mapping while annotate thesemappings rich meta-data use proposed meta- model such meta-data enable linked data user combine variety matching tool obtain multiple context sensitive alignment through simple grammar%23NPR query over correspondence their annotation
owl grammar%23NPR proposed context grammar%23NPR provide generic share metadata alignment reproducibility specific correspondence metadata context specific re- usability validation
three different case+study from social+science discipline science technology study show that our approach provide necessary functionality fulfil requirements complex realistic alignment problem maintain rich metamodel allow user select candidate alignment on variety property such as tool origin alignment strength user approval type resource properties use etc declarative representation linksets lens allow construction new lens by re-use previously construct alignment reusability also facilitate reproducibility share metadata allow complex operation over alignment not only union operator from existing system but also intersection difference composition maintain full declarative provenance trail record correspondence lens originate from linkset each linkset by tool generate among support mixed initiative approach where candidate correspondence generate by algorithm vet reject through user intervention while reject correspondence remain part linkset albeit label as reject in+order+to maintain full provenance trail re- usability our three realistic case study from social e science show that feature indeed require develop environment construction manipulation context sensitive link between dataset
singleton properties not easily use system allow+for convert enriched alignment into usual flat format also allow+for import flat alignment generate by other tool although alignment come without annotation be still possible document correspondence e.g. validation
serve as justification select valid correspondence as future work plan integrate state+of+the+art alignment algorithm such as automate conversion other re%C3%AFfication model into singleton properties vice+versa
important feature include be detection chain equality predicate dataset there respective contextual information furthermore reference resource evolve through time leave need investigate impact over network correspondence lenticular lens+system including change statement due new dataset version additionally plan enable linking dataset construction external dataset finally beside deploy validate production version user+interface plan investigate ontology lenticular lens ll vocabulary
grammar%23NPH grammar%23NPH c grammar%23NPR grammar%23NPR reasoning infer sameas fact linked data 22nd conference on artificial+intelligence grammar%23NPR page 698+706 press
describe link dataset void vocabulary technical report url https%3A%2F%2Fwww.w3.org%2Ftr%2Fvoid
et+al. scientific lens support multiple view over linked chemistry data grammar%23ENT page 98+113 springer
semantics owl sameas page 405+419 springer
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH scientific lens over linked data approach support task specific view vision proceeding 2nd international workshop on linked science
oto match system multi strategy approach instance matching grammar%23NPR page 286+300 springer
ontology matching springer grammar%23NPR grammar%23NPR 2nd edition
grammar%23NPH handbook science technology study
grammar%23NPH grammar%23NPH
owl sameas be not analysis identity linked data grammar%23ENT page 305+320 springer
foundation alternative approach reification grammar%23NPR preprint grammar%23NPR
grammar%23NPH grammar%23NPH framework semantic link discovery over relational data 18th association+for+computing+machinery conference on information knowledge+management page association+for+computing+machinery
grammar%23NPH grammar%23NPH grammar%23NPH
query writer proceeding vldb endowment grammar%23ENT grammar%23ENT
grammar%23NPH query wikidata comparing sparql relational graph database page 88+103

grammar%23NPR logic based scalable ontology matching grammar%23ENT page 273+288 springer
v. grammar%23NPR not like rdf reification make statement about statement use singleton property proceedings 23rd international conference on page 759+770 association+for+computing+machinery
grammar%23NPH grammar%23NPH c grammar%23NPH grammar%23NPH
graph based disambiguation named entity using linked data grammar%23ENT page 457+471 springer
v. grammar%23NPR boer interactive vocabulary alignment international conference on theory practice digital+library page 296+307 springer
discover maintain link on data page 650+665 springer
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR knowledgebase drug drug action drug target nucleic+acid research database-issue 901+906
zooming wiens%40cs.uni bonn.de grammar%23NPH grammar%23NPR st. grammar%23NPR steffen.lohmann%40iais.fraunhofer.de soeren.auer%40tib.eu abstract visualization ontology particular graph visualization form node link diagram often use support ontology development exploration verification sensemaking growing size complexity their represented information tend become hard comprehend due visual clutter information+overload present new approach semantic zooming that abstract simplify underlying graph structure separate information into three layer discrete level detail approach apply force directed graph layout use grammar%23NPR notation mental+map preserve by use smart expanding ordering element layout navigation sensemaking support by local global exploration method halo visualization smooth zooming result user study confirm increase readability visual clarity information clarity enhance our semantic zooming approach
concept human-centered computing information system owl keyword semantic zooming graph grammar%23NPR navigation support smart expanding grammar%23NPR association+for+computing+machinery reference format grammar%23NPH semantic zooming grammar%23NPR grammar%23ENT grammar%23NPR grammar%23ENT knowledge capture conference grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery new+york new+york united+states+of+america https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148015 introduction fundamental aspect semantic+web be create communicate conceptualization information data certain domain taxonomy vocabulary ontology serve this purpose by provide formal machine-readable representation domain knowledge large more interlinked such vocabulary ontology become more challenging be permission make digital hard+copy part this work personal classroom use grant without fee provide that copy not make distribute profit commercial advantage that copy bear this notice full citation on first page copyright component this work own by than association+for+computing+machinery honor abstract credit permit copy otherwise republish post on server redistribute lists require prior specific permission and%2For fee request permission from permissions%40acm.org
grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america association+for+computing+machinery
grammar%23ENT grammar%23ENT https%3A%2F%2Fdoi.org%2F10.1145%2F3148011.3148015 humans explore comprehend visualization ontology offer good starting+point exploration support sensemaking provided information in+particular graph visualization form node link diagram often use depict structure ontology node link diagram represent concept ontology by grammar%23NPR n l where class typically comprise set node n their interrelation describe by set link l use term ontology
cognitive+science show that human capacity be limited aspect store process information ontology graph visualization tend become hard read growing size complexity induce by visual clutter cross edge occlusion render primitive additionally every rendering primitive e.g. circle rectangle text link arrow even color represent information information+overload be natural consequence when amount rendering primitive exceed cognitive perceptive capacity user comprehend visual representation ontology conceptual element ontology visualize by multiple rendering primitive because for+example depiction class require at+least two primitive name shape indicate that amount rendering primitive grow fast than amount node link ontology graph visualization leading quickly information+overload thus readability information clarity visual clarity represented graph structure not only affect by number class property ontology but more general by number render primitive
paper propose semantic zooming approach ontology graph visualization that depict as node link diagram grammar%23NPR zoom be form level detail visualization our approach information comprise ontology partition organize by three different information layer topological layer aggregation layer visual appearance layer simplification abstraction each information layer realize form discrete level detail allow user focus on certain region ontology graph visualization explore structure additional local exploration method enable focused investigation class relation attribute
topological layer describe simplification topological graph structure discrete topological level detail assign class level allow user adjust visualization specified level detail additionally number presented node control use local exploration method
aggregation layer be property orient describe aggregation class attribute comprise datatype property object property different aggregation method define multiple properties between distinct class datatype property reflexive properties properties reflexive properties correspond one class thus organize on grammar%23NPR grammar%23ENT united+states+of+america grammar%23NPH cooperation different information layer discrete level detail their influence on graphical+representation ontology friend friend image c show incremental detail visual appearance layer image c topological level detail indicate by colored circle red indicate low green intermediate blue high topological detail level image c e show incremental level detail aggregation layer
detail enforce their collapse expand operation perform simultaneously implementation semantic zooming approach ontology graph visualization
comprise removal render primitive from visualization e.g. when class name become small read style manipulation i.e. line thickness link size arrow element thus visual representation depicted graph adapt required level detail formative expert interview guide definition discrete level detail with+respect+to visual appearance render primitive
review related work section introduce approach semantic zooming ontology graph visualization section apply on force directed layout use grammar%23NPR notation section summative user study section show advantage new approach through comparison existing ontology graph visualization framework grammar%23NPR extension semantic zooming approach already small medium size ontology exhibit visual clutter occlusion render primitive information+overload while this paper focus on sophisticated method size ontology future work as describe section aim extend semantic zooming large very large ontology graph visualization visualization introduce further challenge with+respect+to responsiveness framework sensemaking information
grammar%23NPR a+number+of ontology visualization technique propose last couple year available approach visualize ontology as graph use node link diagram
node be ontology class link represent property relation between as describe above while some approach visualize only particular type property relation such as class hierarchy e.g. grammar%23NPR aim at visualize large set property relation e.g. grammar%23NPR
available approach use different notation representation node link on+the+one+hand popular notation reuse adapt ontology such as uml class diagram concept map on+the+other+hand new notation propose that specifically design visual representation ontology such as grammar%23NPR grammar%23NPR independently chosen visualization strategy notation approach run into problem cross link primitive occlusion increasing size complexity graph structures
approach apply technique technique help reduce occlusion render primitive decrease number cross link for+instance implementation slovenska+obverscevalno-varnostna+agencija grammar%23NPR allow filter certain property class type on+demand in+order+to reduce number node edge graph visualization work apply hierarchical edge bundle group property link apply force directed graph layout reduce number cross link grammar%23NPR use adjacency matrices visualize dense part graph structure underlie ontology ontology increase visual scalability but not show structural aspect dense graph part any+longer while technique improve readability ontology graph visualization not prevent that visual clutter occlusion information+overload still become problem at+some+point i.e. when number render element be beyond cognitive capacity human perception
this paper present approach approach address this problem by let user dynamically control number element amount information show visualization information comprise ontology partition organize by different information layer discrete level detail abstraction simplification achieve use aggregation mechanism semantic zooming technique be not aware any other work apply technique visualization ontology graph while basic form semantic zooming use ontology visualization grammar%23NPR jambalaya be very limited our approach simply let user switch between different view window graphical+user+interface but not apply semantic zooming ontology graph thus be first approach present sophisticated method semantic zooming ontology graph visualization semantic zooming ontology graph grammar%23ENT united+states+of+america b c d e f g cost grammar%23ENT d+2 e+1 grammar%23ENT g+2 b c figure assignment global topological level detail input graph b minimum span tree organization c path matrix largest connected component input graph indicate computation cost exploration
approach semantic zooming approach ontology graph visualization present this paper partition organize terminological box ontology into three information layer each layer discrete level detail control amount visualized primitive thus also control amount information present user following grammar%23NPR visual information seek mantra overview first zoom filter then detail on-demand three information layer allow user focus on certain region visualization explore structure ontology on different detail level
topology simplification topology simplification apply on simplified undirected graph g n l contrast grammar%23NPR n l reflect full ontology graph structure set node n represent only class ontology no datatypes set link l represent only object properties connect two different class additionally multiple+connection between two class reduce single undirected link excluded datatype property object properties reintroduce visualization propertyoriented aggregation layer
breadth first search determine set connected component c grammar%23NPR cn simplified graph g base on topology connected component topological detail level d assign each node graphical+representation adjust specified level detail by reduce enrich visualization accordingly detail level class node n our approach differ between global grammar%23NPR local grammar%23NPR topological detail level global detail level use global exploration whole ontology graph including removal connected component at low topological detail level as indicate figure grammar%23NPR local detail level use exploration particular connected component allow focused investigation certain region graph using col lapsing topological branch expansion adjacent node
local topological detail level grammar%23NPR ci individual connected component ci c determine by identify pairwise relational distance between class distance compute by investigate object properties ontology connect class that connected component distance each class store path grammar%23NPR entry matrix mi j k describe number steps that require node nj ci reach another node nk ci row wise summation relational distance reflect cost value v nj k mi j k exploration connected component start node nj example cost value show cost column table figure grammar%23ENT row r j path matrix mi minimal cost value v nj select determine local topology tree similar work address centrality measure graph compute pairwise relational distance between node contrast centrality measure node obtain directly topological detail level through relational distance row r j
optimal pathsmay exist this grammar%23NPR nq v nr nq nr ci nq nr each optimal path correspond row minimal cost value v. our approach assign topological detail level base on minimal relational distance correspond path thus low topological detail level not include single but several root node span tree organization
computation span tree determine local topological detail level grammar%23NPR connected component additionally achieve exploration minimal amount expansion operation path matrix largest connected component synthetic input graph figure grammar%23NPR illustrate table figure grammar%23ENT local expansion operation class node add directly connect class node visualization
local collapse operation identify additionally indirectly connect class by compute hiding chain chain recursively traverse accumulate directly connect class node high local topological level detail start from node operation enable user collapse topological branch reduce visualization such that focus direct on particular region corresponding graph
global topological detail level grammar%23NPR determine by align trees such way that their high local topological detail level be equal span tree correspond connected component ci c have tree depth ti ti determine by high local topological detail level grammar%23NPR grammar%23ENT united+states+of+america grammar%23NPH self- loop figure definition discrete level detail visual appearance layer define by formative expert interview different geometric zoom level
node this connected component base on span tree depth tmax max ti total number global topological detail level set tmax trees insert into grammar%23NPR offset h tmax ti ti add local topological detail level connected component
alignment trees enable removal complete connected component from visualization additionally large connected component most likely provide more relevant information visualize first while smaller connected component add visualization increase topological detail level as show figure figure grammar%23NPR
implementation approach additionally assign each leaf node high level tmax global topological detail level enforce their introduction visualization on high detail level justification this design decision be for+example grammar%23NPR notation multiply class owl grammar%23NPR connect different class ontology as show figure additionally expansion leaf node on high global detail level provide visual indication user when topological layer reach maximal level detail
aggregation aggregation layer address class attribute comprise datatype properties reflexive properties multiple properties between different class open world assumption ontology domain allow unlimited amount attribute thus generate cluttered graph representation without provide more insight into underlying structure ontology
link enrich simplified connection l l between two node by introduce directed link visualize multiple+connection that describe by different object properties
properties reflexive properties assign individual class thus enforce our approach that two have same detail level as represent attribute correspond particular class our approach aggregation layer consist following level detail aggregation all attribute expansion all multi link expansion all attribute
first level reduce visualization by remove all aggregated attribute allow user obtain overview graph structure second level introduce more detail by expand all multi link thus provide information about different relation between class third level add remaining attribute properties reflexive properties visualization thus provide full details about attribute different class three discrete detail level show figure grammar%23NPR d e e illustrate aggregation layer use grammar%23NPR notation additionally local exploration method enable user collapse expand attribute individual class node multi link allow focus on certain exploration task
visual appearance third layer address how visual appearance render primitive modify observation observation on zoomed-out visualization graph class name additional symbol become small read raise specification this particular information layer no additional insight obtain when user not clearly see read provided render primitive instead small primitive introduce noise distraction visualization thus unreadable rendering primitive result much clearer visualization additionally boost performance underlying framework as few primitive stress rendering pipeline
obtain definition discrete detail level this information layer from expert interview five participant participant have at+least five years experience ontology domain
find most appropriate number discrete level detail still remain open research question at this point interview conduct framework four graphical view different geometric zoom level high geometric zoom level choose in+order+to fit largest graph questionnaire on screen question design such+that different geometric zoom level visual appearance class object properties properties reflexive properties multiple+connection between class identify assign one level detail ontology expert choose each geometric zoom level from set option modify visual appearance primitive respectively
on expert decision majority vote define individual primitive on geometric zoom level appear
link thickness arrow size determine by average value respectively base on outcome expert interview defined level illustrate figure
different screen+resolution affect outcome formative expert interview all interview perform on 24%22 display resolution 2%2C560+pixels evaluation expert interview show that geometric zoom level have same visualization base on result implement only three level detail visual appearance layer summative user study despite fact that this information layer describe visual appearance render primitive on certain geometric zoom level not enforce manipulation rendering primitive perform together+with geometric zoom operation but rather let grammar%23NPR experiment be available online at vowl.visualdataweb.org%2Fsemzoom%2Fconfig%2Fsemantic zooming ontology graph grammar%23ENT united+states+of+america user set discrete level level fit underlying task nevertheless toggle option implementation approach allow user perform manipulation render primitive together+with geometric zoom similar as geographical map application like
implementation apply approach semantic zooming ontology graph visualization node link diagram force directed layout use grammar%23NPR notation in+particular extend implementation provide by grammar%23NPR extended implementation approach exploit restrict different aspect ontology graph visualization with+respect+to grammar%23NPR notation disjoint property owl grammar%23NPR remove from set link l that use first information layer identification local topological detail level class node observation real-world dataset indicate that tend generate cluttered visualization lot cross edge without comprise high information value additionally implementation approach exploit grammar%23NPR notation datatypes properties due fact each datatype property corresponding datatype node generate graphical+representation connect one particular class by domain property thus not exhibit multiple+connection
allow true conceptual separation aggregation layer between multiple object properties connect two class properties connect class node datatype node
usually distribute textual format not carry any spatial information in+particular position node graphical+representation be not part definition ontology force directed layout use random node position initialization if no spatial information provide optimization process iteratively realign position node generate visual appeal representation ontology force between node optimize also with+respect+to constrains provide by their links due random position initialization optimization process graphical+representation exhibit different layout layout interfere user mental+map from previous visualization same ontology
approach introduce collapse expand operation on topological layer on aggregation layer different expansion operation add new element force directed layout implementation position new element be either random not consistent current layout graph
optimization process this optimal position introduce strong+force layout lead significant change position already present element visualization
mental+map user distort by this significant modification ontology graph visualization additionally as indicate figure grammar%23NPR result optimization processmay not always generate visual appeal representation ontology because base on find local optimum implementation our approach mental+map preserve by use call smart expanding method method consider order position element
multi- link expansion grammar%23NPR application representation multiple properties connect two different class introduce primitive that use force directed layout optimization expansion operation consider relative+position b two participating node determine position corresponding collective render primitive relative distance direction between two node describe by vector v b orthogonal vectorw v describe direction expansion rendering primitive initial position rendering primitive stack along vectorw additionally center stacked primitive align center link connect two class node symmetric smooth fanning out animation generate by optimization process due new initial position be near optimal layout configuration from previous computation this technique not only reduce computation time optimization process but also achieve stable deterministic expansion rendering primitive visual appeal fanning-out animation
single expansion operation datatype properties reflexive properties determine their parent class node ni two disjoint+sets element t ni e ni current topological detail level user interaction expand collapse operation on topological layer affect set class node be present current layout graph base on current configuration topological layer set class node t ni represent directly connected neighbor node ni set element e ni comprise datatypes properties reflexive properties that add visualization by expansion operation
position node comprise set t ni use determine angle on unit circle with+respect+to position node ni angle sort full grammar%23ENT representation their adjacent difference compute pairwise difference indicate free angular space expansion operation by provide start end angle s e largest free angular space be good candidate position expanded element exhibit maximal free+space graph layout additionally newly added element interfere less already present class node force directed layout optimization process element comprise set e ni equally distribute largest free angular space angular offset h s e e ni class node ni empty set neighboring node t ni use full grammar%23NPR free angular space class node only one neighboring node t ni use free angular space grammar%23NPR where center free angular space align direction from node ni single node t ni on opposite unit circle base on angle initial position element e ni set relative position node ni fixed distance reflect half-length original link smooth expanding animation achieve by this reduced relative distance expanded element node ni initial position provide already good but not optimal starting+point force directed layout optimization cap grammar%23ENT united+states+of+america grammar%23NPH order be crucial multiple interconnected node
correct ordering node initialization resulting planar layout b initialization wrong ordering node result optimal layout graphwith cross edge reduced readability because optimization process end localminimum
topological expansion expansion operation aggregation layer exploit datatypes self loop fact that not interconnect between each grammar%23NPR notation expand element have only one property property connect parent node ni thus ordering element expansion be not importance order expanded node become crucial when have multiple interconnection as illustrate for+example by node label figure grammar%23ENT in+order+to achieve visualization without cross edge if set expanded class node exhibit planar+graph layout organization this expansion operation order set expanded node e ni initialize their position same fashion as describe section
interrelation set expanded node e ni identify pair connected class node build base on their id graph layout sequence id determine by concatenate reorder pair such that only matching id use concatenation implement domino tile like behavior correspond chain id entry comprise one chain remove provide ordering corresponding class node final ordering expanded class node achieve by concatenation all duplicate free chain
node expanded set e ni not participate any chain add last chain so not interfere already correctly organized class node base on ordering expanded class node add into free angular space when connection between expanded node be more complex than open closed+chain id this technique not generate crossing free layout neglect order interconnected node result cross edge decrease readability graph as illustrate figure grammar%23NPR
navigation graph visualization form node link diagram require search localization functionality support navigation exploration information implement search+bar search+bar generate dictionary dictionary base on term ontology search+query analyze good matching entry dictionary suggest user element correspond user selection visually highlight ontology graph visualization element element be inside viewport highlight by outer border rendering figure halo node interest leave screenshot approach implement halo visualization right artificially generate image halo indicate viewport
i.e. circle class node rectangle property datatype node in+case+of grammar%23NPR element element be outside viewport visually indicate user use halo halo be circle have dynamic radius as illustrate figure radius halo adjust such+that part circle render at border viewport visible arc halo enable user virtually extend screen space estimate location element by complete arc mentally circle determine center rectangular highlight element transfer into circle when locate outside viewport thus their position off-screen also indicate by arc halo element
localization functionality translate viewport such+that center center element interest align additionally localization request preserve overview context graph by smooth geometric zooming panning if multiple element highlight by selected search+query e.g.
representation thing as figure localization operation alternate between multiple element
evaluation user study conduct in+order+to evaluate benefit ontology graph visualization enhance semantic zooming
version visualization framework use force directed layout grammar%23NPR notation directly compare with+respect+to extended feature our semantic zooming approach user study include participant participant comprise computer+science student employee institute age participant be range years median years selfestimation participant skill average experience graph visualization ontology show table
study design study consist two consecutive experiment each participant first experiment perform on framework without semantic zooming approach version second experiment perform on framework ontology graph visualization enhance semantic zooming halo smooth zooming as+well+as search locate functionality version b both grammar%23NPR zooming ontology graph grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america expertise level novice skilled expert avg year graph visualization ontology table expertise level study participant their average years experience graph visualization ontology respectively
task task identify main component ofthe ontology graph find object property r
name key class maincomponent be domain range this property how many properties link theclasses x y. press button reset relocate property r
how many class z. freely explore ontology graph
perform task where x y z r refer variable class property name corresponding ontology
this study be available all experiment perform on 24%22 display resolution 1%2C920+pixels implementation execute in+full screen mode additionally keyboard mouse use interaction ontology graph visualization
participant equally divide into two group ga united+kingdom start ontology friend grammar%23NPR grammar%23NPR proceed second experiment ontology perform same experiment but order ontology reverse both experiment as indicate table participant search class identify different relations count number link datatypes respectively task design familiarize participant functionality two implemented version
use each version participant rate along various dimensions rating on each dimension use scale range from five where map very low five map very high additionally participant ask list positive negative aspect corresponding version framework
result result user study reveal that application ontology graph visualization enhance semantic zooming outperform without reflect high rating as show figure readability visual clarity information clarity navigation support layout stability
version participant like ontology graph visualization form force directed node link diagram use grammar%23NPR notation intuitive interaction visualization framework navigation via pan zoom use mouse positively acknowledge by most participant
i available at vowl.visualdataweb.org%2Fsemzoom%2Fversiona version b at vowl.visualdataweb.org%2Fsemzoom%2Fversionb
star plot visualization result obtain from user study compare two version
version b implement presented approach participant positively acknowledge control information use different information layer their discrete level detail smart expanding also positively remark comment optimal organization expanded node deterministic behavior expansion method search locate halo visualization appreciate by participant all participant give positive+feedback implementation approach however user wish tooltip functionality similar different information layer additional visual indicator different level detail majority participant mention that control mechanism need first learn fully understand in+order+to obtain full potential semantic zooming ontology graph visualization same context majority describe learning+curve as steep control mechanism as easy learn
user study only consider preliminary evaluation indicate some benefit presented semantic zooming approach small selective group participant simple evaluation setup limit validity this user study
visualization related functionality search locate implement version b introduce bias evaluation result
conclusion discussion this paper present semantic zooming approach ontology graph visualization growing size complexity corresponding graph visualization exhibit visual clutter information+overload occlusion problem problem make underlying structure ontology difficult comprehend our approach partition organize information ontology into three information layer discrete level detail allow user control amount presented information
implementation new semantic zooming approach evaluate on force directed layout use grammar%23NPR notation
element different exploration method allow user focused investigation by adjust visualization their needs examine part ontology as suit required task understanding validate ontology navigation ontology graph visualization support by search locate functionality additionally halo enable user estimate position search off screen element localization grammar%23NPR grammar%23ENT grammar%23ENT grammar%23ENT united+states+of+america grammar%23NPH be smooth animation transition element interest
overview context user preserve by geometric zooming operation during transition process
work include extension new semantic zooming approach large very large ontology graph visualization visualization large ontology integration semantic zooming require more investigation in+order+to achieve intuitive sophisticated exploration navigation their graphical+representation for+example current implementation approach collapsed element neither visually highlight by halo element localization operation perform on only element element be present current layout graph visually highlight locate
ontology graph exhibit additional challenge only when full ontology transfer into graph structure connected component analysis perform be basis topological detail level determination in+order+to overcome this limitation plan apply some kind key concept analysis on ontology key concepts provide additional information about topology graph structure in+particular assigned topological detail level contrast current approach base merely on topology corresponding graph not comprise any information about importance class do integrate semantics relation between class e.g. rdf grammar%23NPR rdf grammar%23NPR etc. additionally key concepts plan use ordering connected component their class base on importance connected component their class order enable streaming process underlying graph structure ontology
large ontology process block global local topological level detail step-by-step generation detail level topological layer aggregation layer enable user explore ontology already on low detail level while background additional information process add ontology graph visualization note stream process also introduce new challenge synchronization global local expansion operation
plan investigate method method improve performance responsiveness framework large ontology graph visualization amount rendering primitive affect performance every visualization framework dynamic static through different level detail reduction rendering primitive achieve already good performance because less element stress rendering system nevertheless performance be when framework show full detail ontology graph visualization improve performance play key role visualization very large ontology
discussion address only portion method advance visualization large ontology apart+from hope that semantic zooming approach ontology graph visualization be useful other researcher developer with+respect+to sensemaking exploration validation ontology
this work support by european+union horizon grammar%23ENT research innovation programme graceful project grant agreement no.
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
hybrid visualization populated ontology 20th int. conf. on companion volume association+for+computing+machinery 177+180
grammar%23NPH grammar%23NPH halo technique visualize offscreen location human factor chi 418+488
grammar%23NPH grammar%23NPH grammar%23NPH
ontology visualization 2nd int. conf. on information+technology grammar%23NPR

faster algorithm betweenness centrality journal mathematical sociology grammar%23ENT 163+177
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH visual modeling owl dl ontology use uml 3rd international semantic+web conference grammar%23NPR springer 198+213
visualize semantic+web xml base internet information visualization springer
grammar%23NPH magical mystery four current direction psychological+science grammar%23ENT
grammar%23NPH grammar%23NPR navigating ontology visualization landscape 19th int. conf. on knowledge engineering knowledge+management grammar%23NPR springer 137+152
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH
model owl ontology grammar%23NPR semantic+web grammar%23NPR grammar%23ENT satellite event springer 320+325
grammar%23NPH grammar%23NPR protegewiki.stanford.edu%2Fwiki%2Fontograf
grammar%23ENT
grammar%23NPH set measure centrality base on betweenness
grammar%23ENT
grammar%23NPH grammar%23NPH graph drawing by forcedirected placement software practice experience grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH towards ontology modeling tool validation software+engineering scenario application grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH collaborative knowledge capture ontology 3rd int. conf. on knowledge capture grammar%23NPR association+for+computing+machinery 99+106
grammar%23NPH grammar%23NPH grammar%23NPH
hierarchical edge bundle visualize complex ontology glow 27th annual association+for+computing+machinery symposium on applied computing chinese+second+artillery+force association+for+computing+machinery 304+311
grammar%23NPH grammar%23NPR protegewiki.stanford.edu%2Fwiki grammar%23NPR grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH ontology visualization method survey association+for+computing+machinery computer survey grammar%23ENT
grammar%23NPH grammar%23NPH grammar%23NPH visualization ontology tool int. conf. on complex grammar%23NPR grammar%23NPR institute+of+electrical+and+electronics+engineers 705+711
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR web-based visualization ontology grammar%23NPR grammar%23ENT satellite event
154+158
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH visualize ontology grammar%23NPR semantic+web grammar%23ENT 399+419
grammar%23NPH grammar%23NPH grammar%23NPH identify key concept ontology through integration cognitive principle statistical topological measure 3rd asian semantic+web conference grammar%23NPR

grammar%23NPH eyes have task by data+type taxonomy information institute+of+electrical+and+electronics+engineers symposium on visual language vl

grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH jambalaya interactive environment explore ontology 7th int. conf. on intelligent user+interface grammar%23NPR association+for+computing+machinery 239+239
grammar%23NPH smooth efficient zooming panning 9th institute+of+electrical+and+electronics+engineers symposium on information visualization grammar%23NPR
lutz grammar%23NPR grammar%23NPR protegewiki.stanford.edu%2Fwiki grammar%23NPR grammar%23ENT
grammar%23NPH building grammar%23NPR
institute+of+electrical+and+electronics+engineers symposium on information visualization grammar%23NPR institute+of+electrical+and+electronics+engineers 159+166 user friendly automatic construction background+knowledge mode construction from grammar%23NPR at alexander.hayes%40utdallas.edu at mayukh.das1%40utdallas.edu phodom%40indiana.edu at natarajan%40utdallas.edu abstract key advantage system be ability domain expert provide background+knowledge as mode allow efficient search through space hypothesis however be inherent assumption that this expert also be expert provide effective mode relax this assumption by design graphical+user+interface graphical+user+interface allow domain expert interact system use diagram interaction use construct mode learning system evaluate our algorithm on probabilistic logic learning system where demonstrate that user be able construct effective background+knowledge on par expert encoded knowledge on five data+set
feature selection logical relational learning model interaction paradigm introduction recently there be increase development algorithm model combine expressiveness first-order logic ability probability+theory model uncertainty
call model method become popular learning presence multi relational noisy data while effective learn model remain computationally intensive task be due fact that learner search hypothesis at multiple level abstraction
method search strategy inspire from introduce make learning more efficient method demonstrate arguably good result several benchmark real data+set while effective key issue method be that require domain expert also be expert thus provide right set directive learn target concept additional directive typically calledmodes restrict search space such that learning probabilistic clause be efficient many real user system especially fail learn good model algorithm may not able select correct mode guide search consequence be that learning procedure get+stuck local minimum get time out result sub- optimal model
way that this problem address literature be by employ database underneath learner improve search speed while system certainly improve search recent work by grammar%23NPH et+al. clearly demonstrate need mode achieve effective learning even when use database their work show order+of+magnitude improvement over standard state-of-the-art grammar%23NPR learning system however their work also require mode be specified attain this efficiency
by their success propose method specify mode from database perspective specifically propose employ use diagram as graphical tool base on user specify mode key intuition be that mode specify how search conduct through space hypothesis when view from relational perspective see as specify part relational graph be relevant target concept provide interface interface allow user guide grammar%23NPR learner use diagram our interface automatically convert user input on diagram mode definition then later employ guide search
work also inspire by work walker et+al. where ui design provide advice grammar%23NPR learner while their ui be domain specific our contribution be generalized approach utilize any diagram automatically construct background+knowledge logic based learner our work be also related other work walker et+al. where mode construction be automated use layered approach approach rely on successively broaden search space until relevant model find
their work be effective due layering scale their work large task be inefficient be more restricted approach allow domain expert specify mode use diagram
make following contribution propose approach make grammar%23NPR system more usable by domain expert by create graphical+user+interface demonstrate how effective background+knowledge encode use diagram provide algorithm translation from ui input mode specification file show empirically effectiveness our learning approach standard grammar%23NPR united+states+of+america grammar%23NPH diagram illustrate entity professor student course their attribute oval relationship diamonds among
first provide necessary background+knowledge diagram system then outline procedure convert input mode definition finally conclude paper by demonstrate effectiveness on standard grammar%23NPR domain outline direction future research
background as our approach use diagram as way construct background+knowledge through mode discuss both diagram as+well+as how mode typically use
grammar%23NPR model relationship model allow+for express structure semantics database at abstract level as object class object entity entity class attribute such entity relationship relationship exist between such entity class
represent as rectangle attribute as circle relationship as diamonds while relational logic as use be equally expressive model represent as graphical structures diagram make more intuitive interpretable
modeling be insufficient express operation on data but our problem set that have no impact figure illustrate diagram example domain
diagram commonly utilize by both database+designer domain expert conceptualize structural characteristic given domain relational schema be alternate abstract representation structure relational data consist structural definition relational table attribute foreign key constraint etc. conceptually knowledge convey by relational schema use abstract background mode grammar%23NPR learning task but limitation lie ambiguity introduce for+instance relation node ta figure express relational schema either as foreign key constraint from one entity another as table column have unique identifier connected entity course student choice typically depend on design system use database diagram avoid such ambiguity via consistent syntax
approach motivate from intuitive connection between constrained logical clause search sql structured+query+language query augmentation logical clause be equivalent relational query since fundamentally sql statement be manifestation entity set define via relational+calculus several grammar%23NPR learning framework successfully utilize this concept similarly mode clause learning interpret as constraint on relational query construction query evaluation hint relational query be special symbolic tool guide query evaluation engine prioritize some database operation over enhance efficiency thus be akin soft directive constraint mode on search space
background+knowledge background+knowledge serve two purpose system describe underlying structure data constrain space model over algorithm explore thus background+knowledge set via mode be key component get relational learning algorithm work effectively mode describe way instantiate predicate clause define hypothesis mode predicate pred n argument define as pred grammar%23NPR grammar%23NPR typen each type describe domain object appear as that argument as+well+as whether instantiate input variable output variable constant input variable previously define model output variable be free variable not define
learner search through space model different way aleph generate clause bottom-up by construct most specific explanation example then generalize while tilde construct clause top-down our mode construction approach be capable generate background+knowledge variety different system validate our approach make use state-of-the-art system call learn set relational regression tree top-down manner relational regression tree contain relational logic inner node regression values leaf each iteration learn tree k push model direction current error error current model grammar%23NPR compute over each training example grammar%23NPR xi p pa xi where be indicator function whether xi be positive example p represent current predicted probability final model be sum over trees m m more detail refer grammar%23NPH et+al.
human naive approach mode construction allow exhaustive search enable learner find good solution at cost time intensive search process other approach allow one free variable each atom consider restrict search space but ignore fact that not all area search space be equally important given target
consider guided construction mode gmc where human assume be domain expert not expert expert provide structure grammar%23NPR friendly automatic construction background+knowledge mode construction from grammar%23NPR grammar%23NPR grammar%23ENT united+states+of+america algorithm procedure gmc expert e max depth d target t related attribute entity interface e grammar%23NPR do path grammar%23NPR t d p path do m p end end grammar%23NPR end procedure procedure grammar%23NPR target t related attribute entity u find shortest path grammar%23NPR max depth d solution grammar%23NPR grammar%23NPR t while grammar%23NPR grammar%23NPR peek d do n grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR xk grammar%23NPR r grammar%23NPR do entity y xk appear relation r do if n r y grammar%23NPR then continue end if if y u u ay then solution append n r y if grammar%23NPR then return solution end if end if grammar%23NPR n r y end end end while return solution end procedure procedure path p grammar%23NPR xi ri xi p do term tj ri do if tj ei then tj ej else if tj then tj ej else tj ej end if end m append ri grammar%23NPR grammar%23NPR tn end grammar%23NPR end procedure graphical+user+interface allow expert interact entity-relationship diagram target entity about model learn mark expert be responsible mark attribute be relevant target
find path through entity relation be able connect target feature related grammar%23NPR their attribute as describe more detail later path be basis construct mode
illustrative example consider set data involve professor student course associate attribute relationship between each
show such grammar%23NPR where grade mark red identify by expert as be important attribute predict tenure mark blue gmc first connect target concept related concepts by find path from diagram figure b show two path path connect tenure grade
path establish variable set as be open close ground base on order entity variable appear since tenure be target concept learn with+relation+to conversion process begin tenure professor
add allow learner search along path
show each step one path corresponding mode mode generate table note that entity attribute highlight different color show argument have same type first time type student course introduce along path mode set allow free variable introduce during search subsequently appearance type have mode that set force previous variable use during search as grade be attribute as oppose entity ground use mode
table give example clause generate by system specified mode english interpretation this rule be that tenure depend on grade student student advise by professor
algorithm goal gmc algorithm be guide learner by construct background+knowledge base on input from human user this background+knowledge consist set mode define search space learner enable find reasonable hypothesis efficiently create user+interface user+interface allow human domain expert provide relevant attribute entity given target line gmc construct mode mode allow relevant attribute entity appear model thus two key step gmc be find path diagram grammar%23NPR generate mode from path now discuss steps
grammar%23NPR give target t relevant attribute entity u find path between diagram path include set entity relationship together relate t u. each path p t rt grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR grammar%23NPR xk consist attribute entity xi relation r j explore set all path breadth first manner start from t. at each step select from among shortest path expand assume grammar%23NPR represent entity set class type not actual instance entity be precise however since context our approach never deal instance use term entity denote entity class grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH target tenure informative important grade select by user b equidistant shortest path between tenure grade
illustrative example show knowledge guide walk on grammar%23NPR give figure mode construction
step step step step step path tenure pro f essor advises student take grade mode tenure pro f advises pro f stud take stud course grade clause tenure p advises p s take s c table each step path corresponding mode generate by gmc
current end selected path denote grammar%23NPR as set relation entity xk appear path p then extend each relation r grammar%23NPR by create path each entity appear r
find path when reach u if u be entity when reach entity y u be attribute u ay be two potential setting correspond number path found if grammar%23NPR set true find shortest path otherwise find all path up particular depth d. our hypothesis be that find all path yield background+knowledge background+knowledge encompass good model while find shortest path yield most efficient+set mode still allow learner find acceptable model note that shortest path consider most simple way relate t u such simple knowledge be basis learning algorithm
give single path p found by grammar%23NPR now create set mode guide search as describe previously mode be specified particular predicate each argument mode specify attribute type define by structure diagram as+well+as how new variable constant introduce each relationship path p define new mode as mention earlier number type argument define by structure diagram also assume that argument correspond attribute value e.g. value blood+pressure grade course consider as constant thus only need describe select between input%2Foutput variable
each pair relation path connect through entity ri xi ri p generate modemri ri denote r x j k be argument relation rk associate type x j. set argument rxi grammar%23NPR as input variable all other argument set as output variable ri ryi note that be multiple argument same type if be more+than one generate mode each argument as input variable treat all as output variable
set mode generate by gmc m use directly search as gmc allow domain expert only provide input on diagram no expertise mode construction require now describe our interface more detail
interface primary objective our approach be facilitate domain expert have limited understanding create suitable mode as given problem necessitate intuitive user-friendly interface develop gui figure provide user have basic understanding entity relation tool build diagram from+scratch load existing annotate knowledge about target informative attribute entity interface design allow user drag shapes arrow construct node edge diagram as+well+as select any node by double clicking on set properties from drop-down+menu left pane properties include whether relation attribute node be target whether attribute relation be important informative predictive if attribute be multi-valued binary note if data store relational+database diagram construct automatically some degree fidelity but most case sanity quality grammar%23NPR be subject database+designer choice user friendly automatic construction background+knowledge mode construction from grammar%23NPR grammar%23NPR grammar%23ENT united+states+of+america figure interface provide drag-and-drop console drop down annotate grammar%23NPR as mention earlier rectangle diamonds ellipse circle represent entity relation attribute respectively here rating be as target popularity attribute be as important
experiment pose following question evaluate effectiveness efficiency our approach background+knowledge construction
facilitate learner optimally explore hypothesis space performance grammar%23NPR gmc enhance efficiency via sufficiently constrain search space grammar%23NPR simple shortest path mode generate robust model grammar%23NPR what be importance human guidance discuss our result base on two scenario search all path from our target our predicate explore shortest path compare our approach against two baseline mode encode by grammar%23NPR grammar%23NPR b mode construction base on depth restricted random path
that achieves similar performance walk from target every feature domain at low average training b inspire by success random+walk algorithm be capable solve challenge task
system have two component platform+independent gui component creation annotation revolutionary+army diagram b mode construction from diagram design be compatible any revolutionary+army diagram tool give common intermediate representation
use state-of-the-art grammar%23NPR structure parameter learning framework relational functional gradient as test+bed evaluate quality automatically mode
domain use four standard grammar%23NPR grammar%23NPR dataset namely grammar%23NPR grammar%23NPR grammar%23NPR empirical evaluation our automatic mode construction system fact refer evidence all relations grammar%23NPR on manual mode construction be beyond scope this paper
different object be true given domain example refer total number positive negative true false target relation attribute feature each crossvalidation fold
dataset create information extraction citation matching have 121%2C891 fact 116%2C679 example split four+cross validation fold each corresponding different topic our goal be predict field title paper correspond fourteen other predicate base on token their relative+position document
be consolidated dataset link among departmental web+page from four university washington+dc group into four+cross validation fold have fact example where target be predict faculty base on several predicate grammar%23NPR grammar%23NPR project grammar%23NPR
like grammar%23NPR be about citation matching key difference be type relations that capture dataset consist 6%2C541 fact 62%2C715 example split into five crossvalidation fold target be predict if citation have same author grammar%23NPR
represent relation between movie people people work on as+well+as several attribute such movie people people either be actor director mutually exclusive goal be predict whether actor work certain director grammar%23NPR total be fact example
be anonymized representation staff student five computer+science department distribute five crossvalidation fold consist fact 94%2C000 example goal be predict advise advisedby
experimental setup our algorithm have two setting walk all path walk all path on graph from user specified target select feature find only shortest path from target select feature
perform on server twenty clock at grammar%23ENT no other processes on server server interfere training time compare performance each method report mean standard+deviation training time autodefensas+unidas+de+colombia+%28auc%29 roc grammar%23NPR pr cross validation fold independent run every dataset number feature setting namely negative positive ratio literal at each tree node underlying grammar%23NPR learner grammar%23NPR keep consistent all approach trees learn each case
attribute relation expert annotate as important informative arrange order select experimental result figure x-axis represent order respective value each point represent performance horizontal slice all predicate up including that point show how each additional predicate influence performance training united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPR avg. training time b grammar%23NPR avg. autodefensas+unidas+de+colombia+%28auc%29 roc c grammar%23NPR avg. autodefensas+unidas+de+colombia+%28auc%29 pr d grammar%23NPR avg. training time e f grammar%23NPR avg. autodefensas+unidas+de+colombia+%28auc%29 pr figure result grammar%23NPR grammar%23NPR dataset top row grammar%23NPR bottom row grammar%23NPR leave efficiency training time low be good middle right performance average autodefensas+unidas+de+colombia+%28auc%29 roc autodefensas+unidas+de+colombia+%28auc%29 pr respectively high be good
experimental result from figures observe that both setting our algorithm outperform random+walk dataset grammar%23NPR grammar%23NPR autodefensas+unidas+de+colombia+%28auc%29 roc grammar%23NPR grammar%23NPR uw autodefensas+unidas+de+colombia+%28auc%29 pr difference be more pronounced earlier learning+curve when fewer path find as expect when number path increase performance random+walk often approach gmc our approach be capable match performance grammar%23NPR expert mode often very few informative predicate mark except case grammar%23NPR require additional marked predicate thus our gmc method generate mode mode facilitate effective grammar%23NPR search grammar%23NPR
our gmc approach generate high performance also constrain search space allow efficient model learn training time random+walk vary be low than our approach three dataset grammar%23NPR grammar%23NPR high grammar%23NPR uw even+though grammar%23NPR be more efficient be less effective grammar%23NPR than our approach
compare grammar%23NPR expert mode our gmc approach be significantly more efficient all domain except grammar%23NPR all path perform similarly grammar%23NPR expert mode overall our gmc approach be capable learning more efficient model than baseline while also achieve high performance grammar%23NPR
both variant our algorithm walk all path compare favorably other baseline now discuss their difference intuitively have efficiency advantage grammar%23NPR all path demonstrate two domain grammar%23NPR where achieve similar performance grammar%23NPR all path while have significantly low training time all other domain both variant perform similarly suggest that short explanation be often sufficient allow robust efficient search grammar%23NPR
better comprehend role human guidance grammar%23NPR let consider two not necessarily distinct aspect primarily human guidance act as search space constraint grammar%23NPR learner efficiently search model hence careful encoding mode be necessary achieve comparable at+times good performance than super- exponential exhaustive search random+walk manage reduce search space by work+with randomly region however as result illustrate figure e f etc. not result robust model other aspect be knowledge about most important feature node be automatic mode construction empirical result illustrate that all dataset all empirical measurements there exist grammar%23NPR friendly automatic construction background+knowledge mode construction from revolutionary+army diagram grammar%23NPR grammar%23ENT united+states+of+america avg. training time b avg. autodefensas+unidas+de+colombia+%28auc%29 roc c avg. autodefensas+unidas+de+colombia+%28auc%29 pr d grammar%23NPR avg. training time e grammar%23NPR avg. autodefensas+unidas+de+colombia+%28auc%29 roc f grammar%23NPR avg. autodefensas+unidas+de+colombia+%28auc%29 pr figure result grammar%23NPR dataset top row bottom row grammar%23NPR leave efficiency training time low be good middle right performance average autodefensas+unidas+de+colombia+%28auc%29 roc autodefensas+unidas+de+colombia+%28auc%29 pr respectively high be good
b uw avg. autodefensas+unidas+de+colombia+%28auc%29 roc c uw avg. autodefensas+unidas+de+colombia+%28auc%29 pr figure uw result leave efficiency training time low be good middle right performance average autodefensas+unidas+de+colombia+%28auc%29 roc autodefensas+unidas+de+colombia+%28auc%29 pr respectively high be good
where including additional guidance annotation important feature no+longer lead better performance grammar%23NPR require all four predicate take+into+account but grammar%23NPR grammar%23NPR performance no+longer improve predicate grammar%23ENT united+states+of+america grammar%23NPH grammar%23NPH grammar%23NPH respectively all case except training time continue increase slightly while overall performance stabilize domain expert be essential provide initial revolutionary+army model as+well+as annotate most important feature node be
conclusion consider problem capture domain expert knowledge context learning first-order probabilistic model develop solution base on entity relationship diagram diagram allow domain expert provide relevant knowledge effectively make search process efficient our solution inspire by observation that most probabilistic logic model see as learn probabilistic model over relational graph line probabilistic relational+model probabilistic entityrelational model give this observation domain expert identify relevant node revolutionary+army diagram translate provide appropriate mode clause learning system our experiment on standard grammar%23NPR domain demonstrate effectiveness our proposed approach extend this system actively solicit advice as+needed be possible future direction allow incomplete noisy even competing advice be another direction
extend interface allow knowledge capture other learning framework such as sequential decision-making relational+model relational deep network other relational+model remain interesting direction future research
grammar%23NPH gratefully acknowledge support grammar%23ENT us advanced+research+projects+agency advanced+research+projects+agency grammar%23NPH grammar%23NPH acknowledge support grant number grammar%23ENT young investigator program
top-down induction first order logical decision+tree ai grammar%23NPR
flexible query hint relational+database grammar%23ENT us patent 8%2C190%2C595
s unified view data
transactions on tod
grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH grammar%23NPH et+al. search system use search subdomain hint subdomain search+query statement result on basis grammar%23ENT us patent 7%2C499%2C914
second edition

learning probabilistic relational+model relational data+mining grammar%23NPR
introduction statistical relational learning
probabilistic model relational data
grammar%23NPR grammar%23ENT
relational retrieval use combination pathconstrained random+walk machine+learning
grammar%23NPH relational+database query optimization perform query evaluation plan prune base on partition property grammar%23ENT us patent 6%2C092%2C062
grammar%23NPH grammar%23NPH inductive logic+programming meet relational+database application statistical relational learning
grammar%23NPR
bottom-up learning markov logic network+structure grammar%23NPR page 625+632
grammar%23NPH boost statistical relational learner from benchmark data driven medicine springer
grammar%23NPH gradient based statistical relational learning case grammar%23NPR
grammar%23NPH boost relational dependency network grammar%23NPR
grammar%23NPR scaling up statistical+inference logic network use rdbms grammar%23NPR abs
grammar%23NPH grammar%23NPH actively interact expert probabilistic logic approach ecml
joint inference information extraction grammar%23NPR page 913+918
grammar%23NPH grammar%23NPH statistical relational artificial+intelligence logic probability computation grammar%23NPH grammar%23NPR publisher
aum+shinrikyo manual
grammar%23NPH integrating knowledge capture supervised learning through human computer interface grammar%23NPR
grammar%23NPH c. grammar%23NPH
grammar%23NPR setup task converting user advice about specific example into general background+knowledge on inductive logic+programming page 253+268 springer
grammar%23NPR scalable inductive logic+programming
vldb endowment
